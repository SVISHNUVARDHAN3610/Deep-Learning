{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1054,"status":"ok","timestamp":1650013623866,"user":{"displayName":"ref learn","userId":"08629560787204524666"},"user_tz":-330},"id":"b65eWEDDwtkL","outputId":"56e9f10e-c869-4586-e4a1-e0cdee7b9a5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'snake-ai-pytorch'...\n","remote: Enumerating objects: 38, done.\u001b[K\n","remote: Counting objects: 100% (38/38), done.\u001b[K\n","remote: Compressing objects: 100% (33/33), done.\u001b[K\n","remote: Total 38 (delta 15), reused 24 (delta 4), pack-reused 0\u001b[K\n","Unpacking objects: 100% (38/38), done.\n"]}],"source":["!git clone https://github.com/python-engineer/snake-ai-pytorch.git"]},{"cell_type":"code","source":["pip install pygame==2.0.0.dev20\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7p_olI4elp-","executionInfo":{"status":"ok","timestamp":1650013726233,"user_tz":-330,"elapsed":6535,"user":{"displayName":"ref learn","userId":"08629560787204524666"}},"outputId":"dccfa258-1cb1-4704-9127-35e778d4050b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pygame==2.0.0.dev20\n","  Downloading pygame-2.0.0.dev20-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n","\u001b[K     |████████████████████████████████| 11.6 MB 4.1 MB/s \n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-2.0.0.dev20\n"]}]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":580,"status":"ok","timestamp":1650014033584,"user":{"displayName":"ref learn","userId":"08629560787204524666"},"user_tz":-330},"id":"PFztW8wivpgH"},"outputs":[],"source":["import pygame\n","import random\n","from enum import Enum\n","from collections import namedtuple\n","import numpy as np\n","\n","pygame.init()\n","font = pygame.font.Font('/content/snake-ai-pytorch/arial.ttf', 25)\n","#font = pygame.font.SysFont('arial', 25)\n","\n","class Direction(Enum):\n","    RIGHT = 1\n","    LEFT = 2\n","    UP = 3\n","    DOWN = 4\n","\n","Point = namedtuple('Point', 'x, y')\n","\n","# rgb colors\n","WHITE = (255, 255, 255)\n","RED = (200,0,0)\n","BLUE1 = (0, 0, 255)\n","BLUE2 = (0, 100, 255)\n","BLACK = (0,0,0)\n","\n","BLOCK_SIZE = 20\n","SPEED = 40\n","\n","class SnakeGameAI:\n","\n","    def __init__(self, w=640, h=480):\n","        self.w = w\n","        self.h = h\n","        # init display\n","        self.display = pygame.display.set_mode((self.w, self.h))\n","        pygame.display.set_caption('Snake')\n","        self.clock = pygame.time.Clock()\n","        self.reset()\n","\n","\n","    def reset(self):\n","        # init game state\n","        self.direction = Direction.RIGHT\n","\n","        self.head = Point(self.w/2, self.h/2)\n","        self.snake = [self.head,\n","                      Point(self.head.x-BLOCK_SIZE, self.head.y),\n","                      Point(self.head.x-(2*BLOCK_SIZE), self.head.y)]\n","\n","        self.score = 0\n","        self.food = None\n","        self._place_food()\n","        self.frame_iteration = 0\n","\n","\n","    def _place_food(self):\n","        x = random.randint(0, (self.w-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n","        y = random.randint(0, (self.h-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n","        self.food = Point(x, y)\n","        if self.food in self.snake:\n","            self._place_food()\n","\n","\n","    def play_step(self, action):\n","        self.frame_iteration += 1\n","        # 1. collect user input\n","        for event in pygame.event.get():\n","            if event.type == pygame.QUIT:\n","                pygame.quit()\n","                quit()\n","        \n","        # 2. move\n","        self._move(action) # update the head\n","        self.snake.insert(0, self.head)\n","        \n","        # 3. check if game over\n","        reward = 0\n","        game_over = False\n","        if self.is_collision() or self.frame_iteration > 100*len(self.snake):\n","            game_over = True\n","            reward = -10\n","            return reward, game_over, self.score\n","\n","        # 4. place new food or just move\n","        if self.head == self.food:\n","            self.score += 1\n","            reward = 10\n","            self._place_food()\n","        else:\n","            self.snake.pop()\n","        \n","        # 5. update ui and clock\n","        self._update_ui()\n","        self.clock.tick(SPEED)\n","        # 6. return game over and score\n","        return reward, game_over, self.score\n","\n","\n","    def is_collision(self, pt=None):\n","        if pt is None:\n","            pt = self.head\n","        # hits boundary\n","        if pt.x > self.w - BLOCK_SIZE or pt.x < 0 or pt.y > self.h - BLOCK_SIZE or pt.y < 0:\n","            return True\n","        # hits itself\n","        if pt in self.snake[1:]:\n","            return True\n","\n","        return False\n","\n","\n","    def _update_ui(self):\n","        self.display.fill(BLACK)\n","\n","        for pt in self.snake:\n","            pygame.draw.rect(self.display, BLUE1, pygame.Rect(pt.x, pt.y, BLOCK_SIZE, BLOCK_SIZE))\n","            pygame.draw.rect(self.display, BLUE2, pygame.Rect(pt.x+4, pt.y+4, 12, 12))\n","\n","        pygame.draw.rect(self.display, RED, pygame.Rect(self.food.x, self.food.y, BLOCK_SIZE, BLOCK_SIZE))\n","\n","        text = font.render(\"Score: \" + str(self.score), True, WHITE)\n","        self.display.blit(text, [0, 0])\n","        pygame.display.flip()\n","\n","\n","    def _move(self, action):\n","        # [straight, right, left]\n","\n","        clock_wise = [Direction.RIGHT, Direction.DOWN, Direction.LEFT, Direction.UP]\n","        idx = clock_wise.index(self.direction)\n","\n","        if np.array_equal(action, [1, 0, 0]):\n","            new_dir = clock_wise[idx] # no change\n","        elif np.array_equal(action, [0, 1, 0]):\n","            next_idx = (idx + 1) % 4\n","            new_dir = clock_wise[next_idx] # right turn r -> d -> l -> u\n","        else: # [0, 0, 1]\n","            next_idx = (idx - 1) % 4\n","            new_dir = clock_wise[next_idx] # left turn r -> u -> l -> d\n","\n","        self.direction = new_dir\n","\n","        x = self.head.x\n","        y = self.head.y\n","        if self.direction == Direction.RIGHT:\n","            x += BLOCK_SIZE\n","        elif self.direction == Direction.LEFT:\n","            x -= BLOCK_SIZE\n","        elif self.direction == Direction.DOWN:\n","            y += BLOCK_SIZE\n","        elif self.direction == Direction.UP:\n","            y -= BLOCK_SIZE\n","\n","        self.head = Point(x, y)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1650014034082,"user":{"displayName":"ref learn","userId":"08629560787204524666"},"user_tz":-330},"id":"7KNshy4hwiaS"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import os\n","\n","class Linear_QNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super().__init__()\n","        self.linear1 = nn.Linear(input_size, hidden_size)\n","        self.linear2 = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        x = F.relu(self.linear1(x))\n","        x = self.linear2(x)\n","        return x\n","\n","    def save(self, file_name='model.pth'):\n","        model_folder_path = './model'\n","        if not os.path.exists(model_folder_path):\n","            os.makedirs(model_folder_path)\n","\n","        file_name = os.path.join(model_folder_path, file_name)\n","        torch.save(self.state_dict(), file_name)\n","\n","\n","class QTrainer:\n","    def __init__(self, model, lr, gamma):\n","        self.lr = lr\n","        self.gamma = gamma\n","        self.model = model\n","        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n","        self.criterion = nn.MSELoss()\n","\n","    def train_step(self, state, action, reward, next_state, done):\n","        state = torch.tensor(state, dtype=torch.float)\n","        next_state = torch.tensor(next_state, dtype=torch.float)\n","        action = torch.tensor(action, dtype=torch.long)\n","        reward = torch.tensor(reward, dtype=torch.float)\n","        # (n, x)\n","\n","        if len(state.shape) == 1:\n","            # (1, x)\n","            state = torch.unsqueeze(state, 0)\n","            next_state = torch.unsqueeze(next_state, 0)\n","            action = torch.unsqueeze(action, 0)\n","            reward = torch.unsqueeze(reward, 0)\n","            done = (done, )\n","\n","        # 1: predicted Q values with current state\n","        pred = self.model(state)\n","\n","        target = pred.clone()\n","        for idx in range(len(done)):\n","            Q_new = reward[idx]\n","            if not done[idx]:\n","                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n","\n","            target[idx][torch.argmax(action[idx]).item()] = Q_new\n","    \n","        # 2: Q_new = r + y * max(next_predicted Q value) -> only do this if not done\n","        # pred.clone()\n","        # preds[argmax(action)] = Q_new\n","        self.optimizer.zero_grad()\n","        loss = self.criterion(target, pred)\n","        loss.backward()\n","\n","        self.optimizer.step()"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1650014034704,"user":{"displayName":"ref learn","userId":"08629560787204524666"},"user_tz":-330},"id":"WNlQwEudw-37"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from IPython import display\n","\n","plt.ion()\n","\n","def plot(scores, mean_scores):\n","    display.clear_output(wait=True)\n","    display.display(plt.gcf())\n","    plt.clf()\n","    plt.title('Training...')\n","    plt.xlabel('Number of Games')\n","    plt.ylabel('Score')\n","    plt.plot(scores)\n","    plt.plot(mean_scores)\n","    plt.ylim(ymin=0)\n","    plt.text(len(scores)-1, scores[-1], str(scores[-1]))\n","    plt.text(len(mean_scores)-1, mean_scores[-1], str(mean_scores[-1]))\n","    plt.show(block=False)\n","    plt.pause(.1)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1650014035874,"user":{"displayName":"ref learn","userId":"08629560787204524666"},"user_tz":-330},"id":"LLV7z1vyxPZW"},"outputs":[],"source":["import os\n","os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4LWjRbsxCkR"},"outputs":[],"source":["import torch\n","import random\n","import numpy as np\n","from collections import deque\n","\n","\n","MAX_MEMORY = 100_000\n","BATCH_SIZE = 1000\n","LR = 0.001\n","\n","class Agent:\n","\n","    def __init__(self):\n","        self.n_games = 0\n","        self.epsilon = 0 # randomness\n","        self.gamma = 0.9 # discount rate\n","        self.memory = deque(maxlen=MAX_MEMORY) # popleft()\n","        self.model = Linear_QNet(11, 256, 3)\n","        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)\n","\n","\n","    def get_state(self, game):\n","        head = game.snake[0]\n","        point_l = Point(head.x - 20, head.y)\n","        point_r = Point(head.x + 20, head.y)\n","        point_u = Point(head.x, head.y - 20)\n","        point_d = Point(head.x, head.y + 20)\n","        \n","        dir_l = game.direction == Direction.LEFT\n","        dir_r = game.direction == Direction.RIGHT\n","        dir_u = game.direction == Direction.UP\n","        dir_d = game.direction == Direction.DOWN\n","\n","        state = [\n","            # Danger straight\n","            (dir_r and game.is_collision(point_r)) or \n","            (dir_l and game.is_collision(point_l)) or \n","            (dir_u and game.is_collision(point_u)) or \n","            (dir_d and game.is_collision(point_d)),\n","\n","            # Danger right\n","            (dir_u and game.is_collision(point_r)) or \n","            (dir_d and game.is_collision(point_l)) or \n","            (dir_l and game.is_collision(point_u)) or \n","            (dir_r and game.is_collision(point_d)),\n","\n","            # Danger left\n","            (dir_d and game.is_collision(point_r)) or \n","            (dir_u and game.is_collision(point_l)) or \n","            (dir_r and game.is_collision(point_u)) or \n","            (dir_l and game.is_collision(point_d)),\n","            \n","            # Move direction\n","            dir_l,\n","            dir_r,\n","            dir_u,\n","            dir_d,\n","            \n","            # Food location \n","            game.food.x < game.head.x,  # food left\n","            game.food.x > game.head.x,  # food right\n","            game.food.y < game.head.y,  # food up\n","            game.food.y > game.head.y  # food down\n","            ]\n","\n","        return np.array(state, dtype=int)\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done)) # popleft if MAX_MEMORY is reached\n","\n","    def train_long_memory(self):\n","        if len(self.memory) > BATCH_SIZE:\n","            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n","        else:\n","            mini_sample = self.memory\n","\n","        states, actions, rewards, next_states, dones = zip(*mini_sample)\n","        self.trainer.train_step(states, actions, rewards, next_states, dones)\n","        #for state, action, reward, nexrt_state, done in mini_sample:\n","        #    self.trainer.train_step(state, action, reward, next_state, done)\n","\n","    def train_short_memory(self, state, action, reward, next_state, done):\n","        self.trainer.train_step(state, action, reward, next_state, done)\n","\n","    def get_action(self, state):\n","        # random moves: tradeoff exploration / exploitation\n","        self.epsilon = 80 - self.n_games\n","        final_move = [0,0,0]\n","        if random.randint(0, 200) < self.epsilon:\n","            move = random.randint(0, 2)\n","            final_move[move] = 1\n","        else:\n","            state0 = torch.tensor(state, dtype=torch.float)\n","            prediction = self.model(state0)\n","            move = torch.argmax(prediction).item()\n","            final_move[move] = 1\n","\n","        return final_move\n","\n","\n","def train():\n","    plot_scores = []\n","    plot_mean_scores = []\n","    total_score = 0\n","    record = 0\n","    agent = Agent()\n","    game = SnakeGameAI()\n","    while True:\n","        # get old state\n","        state_old = agent.get_state(game)\n","\n","        # get move\n","        final_move = agent.get_action(state_old)\n","\n","        # perform move and get new state\n","        reward, done, score = game.play_step(final_move)\n","        state_new = agent.get_state(game)\n","\n","        # train short memory\n","        agent.train_short_memory(state_old, final_move, reward, state_new, done)\n","\n","        # remember\n","        agent.remember(state_old, final_move, reward, state_new, done)\n","\n","        if done:\n","            # train long memory, plot result\n","            game.reset()\n","            agent.n_games += 1\n","            agent.train_long_memory()\n","\n","            if score > record:\n","                record = score\n","                agent.model.save()\n","\n","            print('Game', agent.n_games, 'Score', score, 'Record:', record,\"final_move\",final_move)\n","\n","            plot_scores.append(score)\n","            total_score += score\n","            mean_score = total_score / agent.n_games\n","            plot_mean_scores.append(mean_score)\n","            plot(plot_scores, plot_mean_scores)\n","\n","\n","if __name__ == '__main__':\n","    train()"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"DpzufAM_xIu-","executionInfo":{"status":"ok","timestamp":1650014009862,"user_tz":-330,"elapsed":2,"user":{"displayName":"ref learn","userId":"08629560787204524666"}}},"outputs":[],"source":["def get_state(game):\n","    head = game.snake[0]\n","    point_l = Point(head.x - 20, head.y)\n","    point_r = Point(head.x + 20, head.y)\n","    point_u = Point(head.x, head.y - 20)\n","    point_d = Point(head.x, head.y + 20)\n","    \n","    dir_l = game.direction == Direction.LEFT\n","    dir_r = game.direction == Direction.RIGHT\n","    dir_u = game.direction == Direction.UP\n","    dir_d = game.direction == Direction.DOWN\n","\n","    state = [\n","        # Danger straight\n","        (dir_r and game.is_collision(point_r)) or \n","        (dir_l and game.is_collision(point_l)) or \n","        (dir_u and game.is_collision(point_u)) or \n","        (dir_d and game.is_collision(point_d)),\n","\n","        # Danger right\n","        (dir_u and game.is_collision(point_r)) or \n","        (dir_d and game.is_collision(point_l)) or \n","        (dir_l and game.is_collision(point_u)) or \n","        (dir_r and game.is_collision(point_d)),\n","\n","        # Danger left\n","        (dir_d and game.is_collision(point_r)) or \n","        (dir_u and game.is_collision(point_l)) or \n","        (dir_r and game.is_collision(point_u)) or \n","        (dir_l and game.is_collision(point_d)),\n","        \n","        # Move direction\n","        dir_l,\n","        dir_r,\n","        dir_u,\n","        dir_d,\n","        \n","        # Food location \n","        game.food.x < game.head.x,  # food left\n","        game.food.x > game.head.x,  # food right\n","        game.food.y < game.head.y,  # food up\n","        game.food.y > game.head.y  # food down\n","        ]\n","\n","    return np.array(state, dtype=int)\n","def get_action(self, state):\n","    # random moves: tradeoff exploration / exploitation\n","    self.epsilon = 80 - self.n_games\n","    final_move = [0,0,0]\n","    if random.randint(0, 200) < self.epsilon:\n","        move = random.randint(0, 2)\n","        final_move[move] = 1\n","    else:\n","        state0 = torch.tensor(state, dtype=torch.float)\n","        prediction = self.model(state0)\n","        move = torch.argmax(prediction).item()\n","        final_move[move] = 1\n","\n","    return final_move\n"]},{"cell_type":"code","source":["game = SnakeGameAI()\n","state = get_state(game)\n","state.shape\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","s1 =  torch.tensor(state,dtype = torch.float)\n","s1.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kq6FaM7Cf7I1","executionInfo":{"status":"ok","timestamp":1650015022141,"user_tz":-330,"elapsed":611,"user":{"displayName":"ref learn","userId":"08629560787204524666"}},"outputId":"061900db-ccc7-4e2a-a579-1fd4cf47ef0f"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([11])"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["class Net(nn.Module):\n","  def __init__(self):\n","    super(Net ,self).__init__()\n","    self.mi = nn.Sequential(\n","        nn.Linear(11,32),\n","        nn.ReLU(),\n","        nn.Linear(32,128),\n","        nn.ReLU(),\n","        nn.Linear(128,512),\n","        nn.ReLU(),\n","        nn.Linear(512,512),\n","        nn.ReLU(),\n","        nn.Linear(512,256),\n","        nn.ReLU(),\n","        nn.Linear(256,128),\n","        nn.ReLU(),\n","        nn.Linear(128,64),\n","        nn.ReLU(),\n","        nn.Linear(64,3),\n","        nn.ReLU()\n","\n","    )\n","  def forward(self,x):\n","    x = self.mi(x)\n","    return x    "],"metadata":{"id":"8gxULEgJgJnt","executionInfo":{"status":"ok","timestamp":1650015024302,"user_tz":-330,"elapsed":2,"user":{"displayName":"ref learn","userId":"08629560787204524666"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mz474zh5i0k8","executionInfo":{"status":"ok","timestamp":1650015025044,"user_tz":-330,"elapsed":2,"user":{"displayName":"ref learn","userId":"08629560787204524666"}},"outputId":"332b5eeb-9236-45a8-ec57-bd2f631f80c5"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (mi): Sequential(\n","    (0): Linear(in_features=11, out_features=32, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=32, out_features=128, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=128, out_features=512, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=512, out_features=512, bias=True)\n","    (7): ReLU()\n","    (8): Linear(in_features=512, out_features=256, bias=True)\n","    (9): ReLU()\n","    (10): Linear(in_features=256, out_features=128, bias=True)\n","    (11): ReLU()\n","    (12): Linear(in_features=128, out_features=64, bias=True)\n","    (13): ReLU()\n","    (14): Linear(in_features=64, out_features=3, bias=True)\n","    (15): ReLU()\n","  )\n",")"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["s = model(s1)\n","s"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2rf4R0Ei7IC","executionInfo":{"status":"ok","timestamp":1650015031040,"user_tz":-330,"elapsed":519,"user":{"displayName":"ref learn","userId":"08629560787204524666"}},"outputId":"2ab3de5d-0080-486b-d828-4f1bd32b83c7"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0908, 0.0562, 0.1070], grad_fn=<ReluBackward0>)"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":[""],"metadata":{"id":"Q7nN-08rjLCc","executionInfo":{"status":"ok","timestamp":1650015009314,"user_tz":-330,"elapsed":2,"user":{"displayName":"ref learn","userId":"08629560787204524666"}}},"execution_count":44,"outputs":[]}],"metadata":{"colab":{"name":"snake ai.ipynb","provenance":[],"authorship_tag":"ABX9TyMU+nKXYb0awahQCU4M3cjm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}