{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN7CZ83VtK1iILmmIoUgWX+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install gym==0.10.5\n","!git clone https://github.com/openai/multiagent-particle-envs.git"],"metadata":{"id":"KPjeHsT2IvHo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662878750424,"user_tz":-330,"elapsed":8760,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"ad0d26bc-7023-4786-d245-2d6df1ff70ba"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gym==0.10.5\n","  Downloading gym-0.10.5.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 35.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.21.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.15.0)\n","Collecting pyglet>=1.2.0\n","  Downloading pyglet-1.5.26-py3-none-any.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 53.3 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (1.24.3)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.10.5-py3-none-any.whl size=1581307 sha256=e6bff3bcb00fb2b0beacbcdbdb66aafd74be39c2af579850501724ed68a97267\n","  Stored in directory: /root/.cache/pip/wheels/7a/2c/df/a05b548a40fae16ca400ecbeda0067e1a296499c1fbd7e0c9a\n","Successfully built gym\n","Installing collected packages: pyglet, gym\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","Successfully installed gym-0.10.5 pyglet-1.5.26\n","Cloning into 'multiagent-particle-envs'...\n","remote: Enumerating objects: 242, done.\u001b[K\n","remote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Compressing objects: 100% (5/5), done.\u001b[K\n","remote: Total 242 (delta 0), reused 3 (delta 0), pack-reused 237\u001b[K\n","Receiving objects: 100% (242/242), 107.24 KiB | 477.00 KiB/s, done.\n","Resolving deltas: 100% (127/127), done.\n"]}]},{"cell_type":"code","source":["cd multiagent-particle-envs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DW2a1V2wIy2X","executionInfo":{"status":"ok","timestamp":1662878750424,"user_tz":-330,"elapsed":6,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"c2a7b382-0cd0-43e7-b152-ec96f8df442e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/multiagent-particle-envs\n"]}]},{"cell_type":"code","source":["pip install -e ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzAklXgjI5ks","executionInfo":{"status":"ok","timestamp":1662878756156,"user_tz":-330,"elapsed":5736,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"8c789f9b-70a0-439a-f019-a853100b0568"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/multiagent-particle-envs\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from multiagent==0.0.1) (0.10.5)\n","Collecting numpy-stl\n","  Downloading numpy_stl-2.17.1-py3-none-any.whl (18 kB)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.5.26)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.21.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.15.0)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (2022.6.15)\n","Requirement already satisfied: python-utils>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from numpy-stl->multiagent==0.0.1) (3.3.3)\n","Installing collected packages: numpy-stl, multiagent\n","  Running setup.py develop for multiagent\n","Successfully installed multiagent-0.0.1 numpy-stl-2.17.1\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","import numpy as np\n","from torch.autograd import variable as v\n","from make_env import make_env\n","env = make_env(\"simple_reference\")"],"metadata":{"id":"U3Vu3PezUoO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1XX1uOdgb90U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Actor(nn.Module):\n","  def __init__(self,state_size,action_size):\n","    super(Actor,self).__init__()\n","    self.state_size  = state_size\n","    self.action_size = action_size\n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.fc3 = nn.Linear(64,128).to(self.device)\n","    self.fc4 = nn.Linear(128,256).to(self.device)\n","    self.fc5 = nn.Linear(256,512).to(self.device)\n","    self.fc6 = nn.Linear(512,256).to(self.device)\n","    self.fc7 = nn.Linear(256,128).to(self.device)\n","    self.fc8 = nn.Linear(128,64).to(self.device)\n","    self.fc9 = nn.Linear(64,32).to(self.device)\n","    self.fc10= nn.Linear(32,self.action_size).to(self.device)\n","    \n","  def forward(self,state,message):\n","    message   = torch.tensor([message.item(),0]).to(self.device)\n","    self.fs1 = nn.Linear(state.shape[0],32).to(self.device)\n","    self.fm1 = nn.Linear(message.shape[0],32).to(self.device)\n","    state    = torch.tensor(state).to(self.device)\n","    s1       = self.fs1(state).to(self.device)\n","    m1       = self.fm1(message)\n","    cat      = torch.cat([s1.view(32,-1),m1.view(32,-1)],0)\n","    cat      = torch.reshape(cat,(-1,)).to(self.device)\n","    self.fc2 = nn.Linear(cat.shape[0],64).to(self.device)\n","    x        = self.fc5(f.relu(self.fc4(f.relu(self.fc3(f.relu(self.fc2(cat)))))))\n","    x        = self.fc9(f.relu(self.fc8(f.relu(self.fc7(f.relu(self.fc6(f.relu(x))))))))\n","    x        = f.softmax(self.fc10(f.relu(x)))\n","    return x"],"metadata":{"id":"VLbarRKSGUL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class Critic(nn.Module):\n","  def __init__(self,state_size,action_size,message_size):\n","    super(Critic,self).__init__()\n","    self.state_size  = state_size\n","    self.action_size = action_size\n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.fc3 = nn.Linear(64,128).to(self.device)\n","    self.fc4 = nn.Linear(128,256).to(self.device)\n","    self.fc5 = nn.Linear(256,512).to(self.device)\n","    self.fc6 = nn.Linear(512,256).to(self.device)\n","    self.fc7 = nn.Linear(256,128).to(self.device)\n","    self.fc8 = nn.Linear(128,64).to(self.device)\n","    self.fc9 = nn.Linear(64,32).to(self.device)\n","    self.fc10= nn.Linear(32,1).to(self.device)\n","  def forward(self,state,action,reward):\n","    self.fs1 = nn.Linear(state.shape[0],32).to(self.device)\n","    self.fa1 = nn.Linear(action.shape[0],32).to(self.device)\n","    self.fr1 = nn.Linear(2,32).to(self.device)\n","    s1       = self.fs1(state)\n","    a1       = self.fa1(action)\n","    r1       = self.fr1(reward)\n","    cat      = torch.cat([s1.view(32,-1),r1.view(32,-1),a1.view(32,-1)],0)\n","    cat      = torch.reshape(cat,(-1,))\n","    self.fc2 = nn.Linear(cat.shape[0],64).to(self.device)\n","    x        = self.fc5(f.relu(self.fc4(f.relu(self.fc3(f.relu(self.fc2(cat)))))))\n","    x        = self.fc9(f.relu(self.fc8(f.relu(self.fc7(f.relu(self.fc6(f.relu(x))))))))\n","    x        = self.fc10(f.relu(x))\n","    return x"],"metadata":{"id":"F3M30PvxJbvg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class  MGN(nn.Module):\n","  def __init__(self,state_size,action_size,message_size):\n","    super(MGN,self).__init__()\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.message_size = message_size\n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.fo3 = nn.Linear(64,128).to(self.device)\n","    self.fo4 = nn.Linear(128,256).to(self.device)\n","    self.fo5 = nn.Linear(256,512).to(self.device)\n","    self.fo6 = nn.Linear(512,256).to(self.device)\n","    self.fo7 = nn.Linear(256,128).to(self.device)\n","    self.fo8 = nn.Linear(128,64).to(self.device)\n","    self.fo9 = nn.Linear(64,32).to(self.device)\n","    self.f10 = nn.Linear(32,21).to(self.device)\n","  def action_predictor(self,observation):\n","    self.fa1 = nn.Linear(observation.shape[0],32).to(self.device)\n","    self.fa2 = nn.Linear(32,64).to(self.device)\n","    self.fa3 = nn.Linear(64,128).to(self.device)\n","    self.fa4 = nn.Linear(128,256).to(self.device)\n","    self.fa5 = nn.Linear(256,512).to(self.device)\n","    self.fa6 = nn.Linear(512,256).to(self.device)\n","    self.fa7 = nn.Linear(256,128).to(self.device)\n","    self.fa8 = nn.Linear(128,64).to(self.device)\n","    self.fa9 = nn.Linear(64,32).to(self.device)\n","    self.fa10= nn.Linear(32,self.action_size).to(self.device)\n","    obs = self.fa1(observation)\n","    x   = self.fa5(f.relu(self.fa4(f.relu(self.fa3(f.relu(self.fa2(f.relu(obs))))))))\n","    x   = self.fa9(f.relu(self.fa8(f.relu(self.fa7(f.relu(self.fa6(f.relu(x))))))))\n","    x   = f.softmax(self.fa10(f.relu(x)))\n","    return x\n","  def observation_predictor(self,prsent_action,other_action,observation):\n","    self.fo1 = nn.Linear(observation.shape[0],32).to(self.device)\n","    self.fa1 = nn.Linear(prsent_action.shape[0],32).to(self.device)\n","    self.fa2 = nn.Linear(other_action.shape[0],32).to(self.device)\n","    o        = self.fo1(observation)\n","    at       = self.fa1(prsent_action)\n","    at1      = self.fa2(other_action)\n","    cat      = torch.cat([o.view(32,-1),at.view(32,-1),at1.view(32,-1)],0)\n","    cat      = torch.reshape(cat,(-1,))\n","    self.fo2 = nn.Linear(cat.shape[0],64).to(self.device)\n","    x        = self.fo5(f.relu(self.fo4(f.relu(self.fo3(f.relu(self.fo2(cat)))))))\n","    x        = self.fo9(f.relu(self.fo8(f.relu(self.fo7(f.relu(self.fo6(x)))))))\n","    x        = f.relu(self.f10(f.relu(x)))\n","    return x\n","  def policy(self,message,observation):\n","    self.fm1  = nn.Linear(message.shape[0],32).to(self.device)\n","    self.fs1  = nn.Linear(observation.shape[0],32).to(self.device)\n","    m         = self.fm1(message)\n","    o         = self.fs1(observation)\n","    cat       = torch.cat([m.view(32,-1),o.view(32,-1)],0)\n","    cat       = torch.reshape(cat,(-1,))\n","    self.fc1  = nn.Linear(cat.shape[0],64).to(self.device)\n","    self.fc2  = nn.Linear(64,128).to(self.device)\n","    self.fc3  = nn.Linear(128,256).to(self.device)\n","    self.fc4  = nn.Linear(256,512).to(self.device)\n","    self.fc5  = nn.Linear(512,256).to(self.device)\n","    self.fc6  = nn.Linear(256,128).to(self.device)\n","    self.fc7  = nn.Linear(128,64).to(self.device)\n","    self.fc8  = nn.Linear(64,32).to(self.device)\n","    self.fc9  = nn.Linear(32,self.action_size).to(self.device)\n","    x         = self.fc5(f.relu(self.fc4(f.relu(self.fc3(f.relu(self.fc2(f.relu(self.fc1(cat)))))))))\n","    x         = self.fc8(f.relu(self.fc7(f.relu(self.fc6(f.relu(x))))))\n","    x         = f.softmax(self.fc9(f.relu(x)))\n","    return x\n","  def IGTM(self,message,observation,action):\n","    other_action   = self.action_predictor(observation)\n","    observation_t1 = self.observation_predictor(action,other_action,observation)\n","    policy         = self.policy(message,observation_t1)\n","    return message,observation_t1,policy\n","  def attention(self,message,observation,policy):\n","    trajectory    = torch.cat([observation,policy],0)\n","    trajectory    = torch.reshape(trajectory , (-1,))\n","    self.wv       = nn.Linear(trajectory.shape[0],128).to(self.device)\n","    self.wk       = nn.Linear(trajectory.shape[0],128).to(self.device)\n","    self.wq       = nn.Linear(message.shape[0],128).to(self.device)\n","    v             = self.wv(trajectory)\n","    k             = self.wk(trajectory)\n","    q             = self.wq(message)\n","    x1            = torch.matmul(q,k)\n","    x2            = f.softmax(x1)\n","    x1            = torch.tensor([x1.item(),0])\n","    x2            = torch.tensor([x2.item(),0])\n","    x             = torch.matmul(x1,x2)\n","    return x #message\n","  def forward(self,observation,action,message):\n","    message                     = torch.tensor([message.item(),0]).to(self.device)\n","    msg,observationt2,policy    =  self.IGTM(message,observation,action)\n","    msg                         =  self.attention(msg,observationt2,policy)\n","    return msg"],"metadata":{"id":"057LCjXoKBJd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Buffer:\n","  def __init__(self):\n","    self.agent1_reward    = []\n","    self.agent2_reward    = []\n","    self.agent1_returns   = []\n","    self.agent2_returns   = []\n","    self.agent1_q_value   = []\n","    self.agent2_q_value   = []\n","    self.agent1_n_q_value = []\n","    self.agent2_n_q_value = []\n","    self.agent1_loss      = []\n","    self.agent2_loss      = []\n","    self.agent1_mgn_loss  = []\n","    self.agent2_mgn_loss  = []\n","    self.agent1_msg       = []\n","    self.agent2_msg       = []\n","    self.agent1_action    = []\n","    self.agent2_action    = []\n","    self.agent1_prob      = []\n","    self.agent2_prob      = []\n","    self.agent1_next_prob = []\n","    self.agent2_next_prob = []\n","    self.agent1_actor_loss= []\n","    self.agent2_actor_loss= []\n","    self.agent1_critic_loss=[]\n","    self.agent2_critic_loss=[]\n","    self.agent1_mean_reward=[]\n","    self.agent2_mean_reward=[]\n","    self.agent1_mean_critic=[]\n","    self.agent2_mean_critic=[]\n","    self.agent1_mean_loss = []\n","    self.agent2_mean_loss = []\n","    self.agent1_mean_mgn  = []\n","    self.agent2_mean_mgn  = []\n","    self.agent1_mean_actor= []\n","    self.agent2_mean_actor= [] \n","    self.agent1_mean_value= []\n","    self.agent2_mean_value= []\n","    self.agent1_meannvalue= []\n","    self.agent2_meannvalue= []\n","    self.agent1_meanreturn= []\n","    self.agent2_meanreturn= []\n","    self.agent1_meanlog   = []\n","    self.agent2_meanlog   = []\n","    self.agent1_nextlog   = []\n","    self.agent2_nextlog   = []\n","    self.episodes         = []\n","    self.loss             = []\n","    self.mean_loss        = []\n","    self.ag1r             = []\n","    self.ag2r             = []"],"metadata":{"id":"arZZ40tj2zPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","import numpy as np\n","from torch.autograd import variable as v\n","import sys\n","sys.path.append('./')\n","from Networks.Actor import Actor\n","from Networks.Critic import Critic\n","from Networks.mgn import MGN\n","\n","class Agent1:\n","  def __init__(self,state_size,action_size,message_size,buffer):\n","    self.state_size = state_size\n","    self.action_size  = action_size\n","    self.message_size = message_size\n","    self.gamma        = 0.99\n","    self.lamda        = 0.95\n","    self.lr1          = 0.0000009\n","    self.lr2          = 0.0000007\n","    self.buffer       = buffer\n","    self.device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.actor        = Actor(self.state_size,self.action_size).to(self.device)\n","    self.critic       = Critic(self.state_size,self.action_size,self.message_size).to(self.device)\n","    self.mgn          = MGN(self.state_size,self.action_size,self.message_size).to(self.device)\n","    self.actor_optim  = optim.Adam(self.actor.parameters() ,lr = self.lr1)\n","    self.critic_optim = optim.Adam(self.critic.parameters() ,lr = self.lr2)\n","    self.mgn_optim    = optim.Adam(self.mgn.parameters(),lr = self.lr2)\n","  def choose_action(self,observation,message):\n","    act               = self.actor(observation,message).to(self.device)\n","    return act\n","  def q_value(self,observation,action,reward):\n","    reward            = torch.tensor([reward.item(),0],dtype = torch.float32).to(self.device)\n","    value             = self.critic(observation,action,reward).to(self.device)\n","    return value\n","  def message(self,observation,action,message):\n","    message           = self.mgn(observation,action,message).to(self.device)\n","    return message\n","  def ppo_iter(self,reward,value,next_value,done):\n","    returns   = []\n","    gae       = 0\n","    for i in range(5):\n","      delta   = reward + self.gamma *(1-done)* next_value - value\n","      gae     = delta + self.gamma*self.lamda*gae*(1-done)\n","      returns.insert(0,gae + value+next_value)\n","    return returns\n","  def appending(self,loss,actor_loss,critic_loss,mgn,value,next_value,log_prob,returns,next_log_prob,action,msg1):\n","    self.buffer.agent1_mgn_loss.append(mgn)\n","    self.buffer.agent1_actor_loss.append(actor_loss)\n","    self.buffer.agent1_critic_loss.append(critic_loss)\n","    self.buffer.agent1_loss.append(loss)\n","    self.buffer.agent1_q_value.append(value)\n","    self.buffer.agent1_n_q_value.append(next_value)\n","    self.buffer.agent1_prob.append(log_prob.mean())\n","    self.buffer.agent1_returns.append(returns)\n","    self.buffer.agent1_msg.append(msg1.cpu())\n","    self.buffer.agent1_next_prob.append(next_log_prob.mean())\n","    self.buffer.agent1_action.append(action)\n","  def learn(self,state,next_state,reward,done,next_value,message):\n","    next_state        = torch.from_numpy(next_state).float().to(self.device)\n","    state             = torch.from_numpy(state).float().to(self.device)\n","    reward            = torch.tensor(reward , dtype = torch.float32).to(self.device)\n","    done              = torch.tensor(done   , dtype = torch.float32).to(self.device)\n","    action            = self.choose_action(state,message)\n","    next_action       = self.choose_action(next_state,message)\n","    message           = self.message(state,action,message)\n","    value             = self.q_value(state,action,reward)\n","    returns           = self.ppo_iter(reward.item(),value,next_value,done)\n","    returns           = torch.tensor(returns ,dtype = torch.float32).to(self.device)\n","    advantage         = returns - value -next_value\n","    log_prob          = torch.log(action).to(self.device)\n","    next_log          = torch.log(next_action).to(self.device)\n","    ratio             = (next_log - log_prob).exp()\n","    s1                = ratio * advantage\n","    s2                = torch.clamp(ratio,0.8,1.2)\n","    actor_loss        = torch.min(s1,s2).mean()\n","    critic_loss       = (returns - value)**2\n","    critic_loss       = torch.mean(critic_loss)\n","    actor_loss        = actor_loss\n","    loss              = actor_loss + 0.5*critic_loss\n","    mgn_loss          = message * log_prob * value\n","    mgn_loss          = torch.mean(mgn_loss)\n","    mgn_loss          = torch.tensor(mgn_loss,requires_grad=True).to(self.device)\n","    torch.save(self.actor.state_dict(),\"memory/Agent1/actor.pth\")\n","    torch.save(self.critic.state_dict(),\"memory/Agent1/critic.pth\")\n","    torch.save(self.mgn.state_dict(),\"memory/Agent1/mgn.pth\")\n","    self.appending(loss,actor_loss,critic_loss,mgn_loss,value,next_value,log_prob,returns,next_log,action,message)\n","    self.actor_optim.zero_grad()\n","    self.critic_optim.zero_grad()\n","    loss.backward()\n","    self.actor_optim.step()\n","    self.critic_optim.step()\n","    self.mgn_optim.zero_grad()\n","    mgn_loss.backward()\n","    self.mgn_optim.step()"],"metadata":{"id":"4ExTzz1gFlau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","import numpy as np\n","from torch.autograd import variable as v\n","import sys\n","sys.path.append('./')\n","from Networks.Actor import Actor\n","from Networks.Critic import Critic\n","from Networks.mgn import MGN\n","\n","class Agent2:\n","  def __init__(self,state_size,action_size,message_size,buffer):\n","    self.state_size = state_size\n","    self.action_size  = action_size\n","    self.message_size = message_size\n","    self.gamma        = 0.99\n","    self.lamda        = 0.95\n","    self.lr1          = 0.0000009\n","    self.lr2          = 0.0000007\n","    self.buffer       = buffer\n","    self.device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.actor        = Actor(self.state_size,self.action_size).to(self.device)\n","    self.critic       = Critic(self.state_size,self.action_size,self.message_size).to(self.device)\n","    self.mgn          = MGN(self.state_size,self.action_size,self.message_size).to(self.device)\n","    self.actor_optim  = optim.Adam(self.actor.parameters() ,lr = self.lr1)\n","    self.critic_optim = optim.Adam(self.critic.parameters() ,lr = self.lr2)\n","    self.mgn_optim    = optim.Adam(self.mgn.parameters(),lr = self.lr2)\n","  def choose_action(self,observation,message):\n","    act               = self.actor(observation,message).to(self.device)\n","    return act\n","  def q_value(self,observation,action,reward):\n","    reward            = torch.tensor([reward.item(),0],dtype = torch.float32).to(self.device)\n","    value             = self.critic(observation,action,reward).to(self.device)\n","    return value\n","  def message(self,observation,action,message):\n","    message           = self.mgn(observation,action,message).to(self.device)\n","    return message\n","  def ppo_iter(self,reward,value,next_value,done):\n","    returns   = []\n","    gae       = 0\n","    for i in range(5):\n","      delta   = reward + self.gamma *(1-done)*   next_value - value\n","      gae     = delta + self.gamma*self.lamda*gae*(1-done)\n","      returns.insert(0,gae + value+next_value)\n","    return returns\n","  def appending(self,loss,actor_loss,critic_loss,mgn,value,next_value,log_prob,returns,next_log_prob,action,msg1):\n","    self.buffer.agent2_mgn_loss.append(mgn)\n","    self.buffer.agent2_actor_loss.append(actor_loss)\n","    self.buffer.agent2_critic_loss.append(critic_loss)\n","    self.buffer.agent2_loss.append(loss)\n","    self.buffer.agent2_q_value.append(value)\n","    self.buffer.agent2_n_q_value.append(next_value)\n","    self.buffer.agent2_prob.append(log_prob.mean())\n","    self.buffer.agent2_returns.append(returns)\n","    self.buffer.agent2_msg.append(msg1.cpu())\n","    self.buffer.agent2_next_prob.append(next_log_prob.mean())\n","    self.buffer.agent2_action.append(action)\n","  def learn(self,state,next_state,reward,done,next_value,message):\n","    next_state        = torch.from_numpy(next_state).float().to(self.device)\n","    state             = torch.from_numpy(state).float().to(self.device)\n","    reward            = torch.tensor(reward , dtype = torch.float32).to(self.device)\n","    done              = torch.tensor(done   , dtype = torch.float32).to(self.device)\n","    action            = self.choose_action(state,message)\n","    next_action       = self.choose_action(next_state,message)\n","    message           = self.message(state,action,message)\n","    value             = self.q_value(state,action,reward)\n","    returns           = self.ppo_iter(reward.item(),value,next_value,done)\n","    returns           = torch.tensor(returns ,dtype = torch.float32).to(self.device)\n","    advantage         = returns - value -next_value\n","    log_prob          = torch.log(action).to(self.device)\n","    next_log          = torch.log(next_action).to(self.device)\n","    ratio             = (next_log - log_prob).exp()\n","    s1                = ratio * advantage\n","    s2                = torch.clamp(ratio,0.8,1.2)\n","    actor_loss        = torch.min(s1,s2).mean()\n","    critic_loss       = (returns - value)**2\n","    critic_loss       = torch.mean(critic_loss)\n","    actor_loss        = actor_loss\n","    loss              = actor_loss + 0.5*critic_loss\n","    mgn_loss          = message * log_prob * value\n","    mgn_loss          = torch.mean(mgn_loss)\n","    #mgn_loss          = v(mgn_loss,requires_grad=True).to(self.device)\n","    mgn_loss          = torch.tensor(mgn_loss,requires_grad=True).to(self.device)\n","    torch.save(self.actor.state_dict(),\"memory/Agent2/actor.pth\")\n","    torch.save(self.critic.state_dict(),\"memory/Agent2/critic.pth\")\n","    torch.save(self.mgn.state_dict(),\"memory/Agent2/mgn.pth\")\n","    self.appending(loss,actor_loss,critic_loss,mgn_loss,value,next_value,log_prob,returns,next_log,action,message)\n","    self.actor_optim.zero_grad()\n","    self.critic_optim.zero_grad()\n","    loss.backward()\n","    self.actor_optim.step()\n","    self.critic_optim.step()\n","    self.mgn_optim.zero_grad()\n","    mgn_loss.backward()\n","    self.mgn_optim.step()"],"metadata":{"id":"J4wRaOSDtlUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import matplotlib.pyplot as plt\n","class Ploting:\n","  def __init__(self,buffer):\n","    self.buffer = buffer\n","    self.agent1_loss()\n","    self.agent1_networks()\n","    self.agent1_value()\n","    self.agent1_policy()\n","    self.agent2_loss()\n","    self.agent2_networks()\n","    self.agent2_value()\n","    self.agent2_policy()\n","    self.agent1_reward()\n","    self.agent2_reward()\n","  def agent1_loss(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent1_mean_loss)\n","    plt.xlabel(\"episodes\")\n","    plt.ylabel(\"loss\")\n","    plt.title(\"loss vs episodes\")\n","    plt.savefig(\"memory/ploting/agent1_loss.png\")\n","    plt.close()\n","  def agent1_reward(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent1_mean_reward)\n","    plt.xlabel(\"episodes\")\n","    plt.ylabel(\"loss\")\n","    plt.title(\"loss vs reward\")\n","    plt.savefig(\"memory/ploting/agent1_reward.png\")\n","    plt.close()\n","  def agent2_reward(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent2_mean_reward)\n","    plt.xlabel(\"episodes\")\n","    plt.ylabel(\"loss\")\n","    plt.title(\"loss vs reward\")\n","    plt.savefig(\"memory/ploting/agent2_reward.png\")\n","    plt.close()\n","  def agent1_networks(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent1_mean_actor,label = \"actor_loss\")\n","    plt.plot(self.buffer.episodes,self.buffer.agent1_mean_critic,label = \"Critic_loss\")\n","    plt.plot(self.buffer.episodes,self.buffer.agent1_mean_loss,label = \"loss\")\n","    plt.xlabel(\"episode\")\n","    plt.ylabel(\"loss\")\n","    plt.title(\"episode_vs_agent1_loss\")\n","    plt.legend()\n","    plt.savefig(\"memory/ploting/agent1_network_loss.png\")\n","    plt.close()\n","  def agent1_value(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent1_mean_value,label = \"q_value\")\n","    plt.plot(self.buffer.episodes,self.buffer.agent1_meannvalue,label = \"next_q_value\")\n","    plt.xlabel(\"episode\")\n","    plt.ylabel(\"value\")\n","    plt.title(\"episode_vs_value\")\n","    plt.legend()\n","    plt.savefig(\"memory/ploting/agent1_values.png\")\n","    plt.close()\n","  def agent1_policy(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent1_meanlog,label = \"policy\")\n","    plt.plot(self.buffer.episodes,self.buffer.agent1_nextlog,label = \"next_policy\")\n","    plt.xlabel(\"episode\")\n","    plt.ylabel(\"policy\")\n","    plt.title(\"episode_vs_policy\")\n","    plt.legend()\n","    plt.savefig(\"memory/ploting/agent1_policy.png\")\n","    plt.close()\n","  def agent2_loss(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent2_mean_loss)\n","    plt.xlabel(\"episodes\")\n","    plt.ylabel(\"loss\")\n","    plt.title(\"loss vs episodes\")\n","    plt.savefig(\"memory/ploting/agent2_loss.png\")\n","    plt.close()\n","  def agent2_networks(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent2_mean_actor,label = \"actor_loss\")\n","    plt.plot(self.buffer.episodes,self.buffer.agent2_mean_critic,label = \"Critic_loss\")\n","    plt.plot(self.buffer.episodes,self.buffer.agent2_mean_loss,label = \"loss\")\n","    plt.xlabel(\"episode\")\n","    plt.ylabel(\"loss\")\n","    plt.title(\"episode_vs_agent2_loss\")\n","    plt.legend()\n","    plt.savefig(\"memory/ploting/agent2_network_loss.png\")\n","    plt.close()\n","  def agent2_value(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent2_mean_value,label = \"q_value\")\n","    plt.plot(self.buffer.episodes,self.buffer.agent2_meannvalue,label = \"next_q_value\")\n","    plt.xlabel(\"episode\")\n","    plt.ylabel(\"value\")\n","    plt.title(\"episode_vs_value\")\n","    plt.legend()\n","    plt.savefig(\"memory/ploting/agent2_values.png\")\n","    plt.close()\n","  def agent2_policy(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent2_meanlog,label = \"policy\")\n","    plt.plot(self.buffer.episodes,self.buffer.agent2_nextlog,label = \"next_policy\")\n","    plt.xlabel(\"episode\")\n","    plt.ylabel(\"policy\")\n","    plt.title(\"episode_vs_policy\")\n","    plt.legend()\n","    plt.savefig(\"memory/ploting/agent2_policy.png\")\n","    plt.close()\n","  def agent_returns(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent1_meanreturn,label = \"agent1_returns\")\n","    plt.plot(self.buffer.episodes,self.buffer.agent2_meanreturn,label = \"agent2_returns\")\n","    plt.xlabel(\"episode\")\n","    plt.ylabel(\"returns\")\n","    plt.title(\"episode_vs_returns\")\n","    plt.legend()\n","    plt.savefig(\"memory/ploting/agent1_returns.png\") \n","    plt.close()\n","  def agent_mgn(self):\n","    plt.plot(self.buffer.episodes,self.buffer.agent1_mean_mgn,label = \"agent1_mgn_loss\")\n","    plt.plot(self.buffer.episodes,self.buffer.agent2_mean_mgn,label = \"agent2_mgn_loss\")\n","    plt.xlabel(\"episode\")\n","    plt.ylabel(\"loss\")\n","    plt.title(\"episode_vs_agent_mgn_loss\")\n","    plt.legend()\n","    plt.savefig(\"memory/ploting/agent1_mgn_loss.png\")\n","    plt.close()\n","  def loss(self):\n","    plt.plot(self.buffer.episodes,self.buffer.mean_loss)\n","    plt.xlabel(\"episodes\")\n","    plt.ylabel(\"loss\")\n","    plt.title(\"loss vs episodes\")\n","    plt.savefig(\"memory/ploting/loss_vs_episodes.png\")\n","    plt.close()\n"],"metadata":{"id":"nTPeMzdiNzYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","import numpy as np\n","from torch.autograd import variable as v\n","import sys\n","sys.path.append('./')\n","from buffer.ploting import Ploting\n","from Agents.Agent1 import Agent1\n","from Agents.Agent2 import Agent2\n","from Networks.Critic import Critic\n","from make_env import make_env\n","env = make_env(\"simple_reference\")\n","\n","class Main:\n","  def __init__(self,state_size,action_size,message_size,buffer,n_agents,n_games,steps):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.message_size = message_size\n","    self.n_agents    = n_agents\n","    self.n_games     = n_games\n","    self.steps       = steps\n","    self.gamma  = 0.99\n","    self.lr     = 0.0000007\n","    self.buffer = buffer\n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.agent1 = Agent1(self.state_size,self.action_size,self.message_size,self.buffer)\n","    self.agent2 = Agent2(self.state_size,self.action_size,self.message_size,self.buffer)\n","    self.central= Critic(self.state_size,self.action_size,self.message_size).to(self.device)\n","    self.optim  = optim.Adam(self.central.parameters() , lr = self.lr)\n","    self.msg1   = []\n","    self.msg2   = []\n","  def choose_action(self,state,msg1,msg2):\n","    act  = []\n","    obs1 = torch.from_numpy(state[0]).float().to(self.device)\n","    obs2 = torch.from_numpy(state[1]).float().to(self.device)\n","    msg1 = torch.tensor(msg1,dtype = torch.float32).to(self.device)\n","    msg2 = torch.tensor(msg2,dtype = torch.float32).to(self.device)\n","    act1 = self.agent1.choose_action(obs1,msg2).cpu()\n","    act2 = self.agent2.choose_action(obs2,msg1).cpu()\n","    act.append(act1.detach().numpy())\n","    act.append(act2.detach().numpy())\n","    return act\n","  def next_value(self,next_state,action,r1,r2):\n","    obs1 = torch.from_numpy(next_state[0]).float().to(self.device)\n","    obs2 = torch.from_numpy(next_state[1]).float().to(self.device)\n","    r1   = torch.tensor([r1,0],dtype = torch.float32).to(self.device)\n","    r2   = torch.tensor([r2,0],dtype = torch.float32).to(self.device)\n","    nv1  = self.central(obs1,action[0],r1)\n","    nv2  = self.central(obs2,action[1],r2)\n","    return nv1,nv2\n","  def message(self,state,action,msg1,msg2):\n","    obs1 = torch.from_numpy(state[0]).float().to(self.device)\n","    obs2 = torch.from_numpy(state[1]).float().to(self.device)\n","    act1 = torch.tensor(action[0],dtype = torch.float32).to(self.device)\n","    act2 = torch.tensor(action[1],dtype = torch.float32).to(self.device)\n","    msg2 = torch.tensor(msg2,dtype = torch.float32).to(self.device)\n","    msg1 = self.agent1.message(obs1,act1,msg2)\n","    msg2 = self.agent2.message(obs2,act2,msg1)\n","    return msg1,msg2\n","  def update(self,state,next_state,reward,done,msg1,msg2):\n","    action     = self.choose_action(state,msg1,msg2)\n","    action     = torch.tensor(action,dtype = torch.float32).to(self.device)\n","    msg1,msg2  = self.message(state,action,msg1,msg2)\n","    agnv1,agnv2= self.next_value(next_state,action,reward[0],reward[1])\n","    agv1       = self.agent1.q_value(torch.from_numpy(state[0]).float().to(self.device),action[0],torch.tensor(reward[0],dtype = torch.float32))\n","    agv2       = self.agent2.q_value(torch.from_numpy(state[1]).float().to(self.device),action[1],torch.tensor(reward[1],dtype = torch.float32))\n","    agr1       = torch.tensor(self.agent1.ppo_iter(torch.tensor(reward[0],dtype = torch.float32).to(self.device),agv1,agnv1,done[0]))\n","    agr2       = torch.tensor(self.agent2.ppo_iter(torch.tensor(reward[1],dtype = torch.float32).to(self.device),agv2,agnv2,done[1]))\n","    returns    = torch.tensor(agr1 + agr2).to(self.device)\n","    loss       = returns - agv1-agv2-agnv1-agnv2\n","    loss       = torch.tensor(loss[0],requires_grad = True).to(self.device)\n","    self.buffer.loss.append(loss)\n","    torch.save(self.central.state_dict() , \"memory/centeral.pth\")\n","    self.agent1.learn(state[0],next_state[0],reward[0],done[0],agnv1,msg2)\n","    self.agent2.learn(state[1],next_state[1],reward[1],done[1],agnv2,msg1)\n","    self.optim.zero_grad()\n","    loss.backward()\n","    self.optim.step()\n","  def mean(self,episode):\n","    if episode==0:\n","      self.buffer.mean_loss.append(sum(self.buffer.loss))\n","      self.buffer.agent1_mean_loss.append(sum(self.buffer.agent1_loss))\n","      self.buffer.agent1_mean_actor.append(sum(self.buffer.agent1_actor_loss))\n","      self.buffer.agent1_mean_critic.append(sum(self.buffer.agent1_critic_loss))\n","      self.buffer.agent1_mean_mgn.append(sum(self.buffer.agent1_mean_loss))    \n","      self.buffer.agent1_meanlog.append(sum(self.buffer.agent1_prob))\n","      self.buffer.agent1_nextlog.append(sum(self.buffer.agent1_next_prob))\n","      self.buffer.agent1_mean_value.append(sum(self.buffer.agent1_q_value))\n","      self.buffer.agent1_meanreturn.append(sum(self.buffer.agent1_returns))\n","      self.buffer.agent1_meannvalue.append(sum(self.buffer.agent1_n_q_value))\n","      self.buffer.agent1_mean_reward.append(sum(self.buffer.agent1_reward))\n","      self.buffer.agent2_mean_actor.append(sum(self.buffer.agent2_actor_loss))\n","      self.buffer.agent2_mean_critic.append(sum(self.buffer.agent2_critic_loss))\n","      self.buffer.agent2_mean_mgn.append(sum(self.buffer.agent2_mean_loss))    \n","      self.buffer.agent2_meanlog.append(sum(self.buffer.agent2_prob))\n","      self.buffer.agent2_nextlog.append(sum(self.buffer.agent2_next_prob))\n","      self.buffer.agent2_mean_value.append(sum(self.buffer.agent2_q_value))\n","      self.buffer.agent2_meanreturn.append(sum(self.buffer.agent2_returns))\n","      self.buffer.agent2_meannvalue.append(sum(self.buffer.agent2_n_q_value))\n","      self.buffer.agent2_mean_reward.append(sum(self.buffer.agent2_reward))\n","      self.buffer.agent2_mean_loss.append(sum(self.buffer.agent2_loss))\n","    else:\n","      self.buffer.mean_loss.append(sum(self.buffer.loss)/len(self.buffer.loss))\n","      self.buffer.agent1_mean_loss.append(torch.tensor(sum(self.buffer.agent1_loss)/len(self.buffer.agent1_loss)).cpu())\n","      self.buffer.agent2_mean_loss.append(torch.tensor(sum(self.buffer.agent2_loss)/len(self.buffer.agent2_loss)).cpu())\n","      self.buffer.agent1_mean_actor.append(torch.tensor(sum(self.buffer.agent1_actor_loss)/len(self.buffer.agent1_actor_loss)).cpu())\n","      self.buffer.agent1_mean_critic.append(torch.tensor(sum(self.buffer.agent1_critic_loss)/len(self.buffer.agent1_critic_loss)).cpu())\n","      self.buffer.agent1_mean_mgn.append(sum(self.buffer.agent1_mgn_loss)/len(self.buffer.agent1_mgn_loss))    \n","      self.buffer.agent1_meanlog.append(torch.tensor(sum(self.buffer.agent1_prob)/len(self.buffer.agent1_prob)).cpu())\n","      self.buffer.agent1_nextlog.append(torch.tensor(sum(self.buffer.agent1_next_prob)/len(self.buffer.agent1_next_prob)).cpu())\n","      self.buffer.agent1_mean_value.append(torch.tensor(sum(self.buffer.agent1_q_value)/len(self.buffer.agent1_q_value)).cpu())\n","      self.buffer.agent1_meanreturn.append(sum(self.buffer.agent1_returns)/len(self.buffer.agent1_returns))\n","      self.buffer.agent1_meannvalue.append(torch.tensor(sum(self.buffer.agent1_n_q_value)/len(self.buffer.agent1_n_q_value)).cpu())\n","      self.buffer.agent1_mean_reward.append(torch.tensor(sum(self.buffer.agent1_reward)/len(self.buffer.agent1_reward)).cpu())\n","      self.buffer.agent2_mean_actor.append(torch.tensor(sum(self.buffer.agent2_actor_loss)/len(self.buffer.agent2_actor_loss)).cpu())\n","      self.buffer.agent2_mean_critic.append(torch.tensor(sum(self.buffer.agent2_critic_loss)/len(self.buffer.agent2_critic_loss)).cpu())\n","      self.buffer.agent2_mean_mgn.append(sum(self.buffer.agent2_mgn_loss)/len(self.buffer.agent2_mgn_loss))    \n","      self.buffer.agent2_meanlog.append(torch.tensor(sum(self.buffer.agent2_prob)/len(self.buffer.agent2_prob)).cpu())\n","      self.buffer.agent2_nextlog.append(torch.tensor(sum(self.buffer.agent2_next_prob)/len(self.buffer.agent2_next_prob)).cpu())\n","      self.buffer.agent2_mean_value.append(torch.tensor(sum(self.buffer.agent2_q_value)/len(self.buffer.agent2_q_value)).cpu())\n","      self.buffer.agent2_meanreturn.append(sum(self.buffer.agent2_returns)/len(self.buffer.agent2_returns))\n","      self.buffer.agent2_meannvalue.append(torch.tensor(sum(self.buffer.agent2_n_q_value)/len(self.buffer.agent2_n_q_value)).cpu())\n","      self.buffer.agent2_mean_reward.append(torch.tensor(sum(self.buffer.agent2_reward)/len(self.buffer.agent2_reward)).cpu())\n","  def clear(self):\n","    self.buffer.loss = []\n","    self.buffer.agent2_actor_loss = []\n","    self.buffer.agent2_critic_loss = []\n","    self.buffer.agent2_prob = []\n","    self.buffer.agent2_next_prob= []\n","    self.buffer.agent2_q_value =[]\n","    self.buffer.agent2_returns  = []\n","    self.buffer.agent2_n_q_value = []\n","    self.buffer.agent2_reward = []\n","    self.buffer.agent1_actor_loss = []\n","    self.buffer.agent1_critic_loss = []\n","    self.buffer.agent1_prob = []\n","    self.buffer.agent1_next_prob= []\n","    self.buffer.agent1_q_value =[]\n","    self.buffer.agent1_returns  = []\n","    self.buffer.agent1_n_q_value = []\n","    self.buffer.agent1_reward = []\n","  def run(self):\n","    for i in range(self.n_games):\n","      state = env.reset()\n","      score = [0,0]\n","      done  = [False ,False]\n","      self.buffer.episodes.append(i)\n","      self.mean(i)\n","      self.clear()\n","      plts = Ploting(self.buffer)\n","      print(\"episode:\",i,\",\",\"agent1_reward:\",self.buffer.agent1_mean_reward[i],\",\",\"agent2_reward:\",self.buffer.agent1_mean_reward[i])\n","      for step in range(self.steps):\n","        if i==0 & step==0:\n","          msg1 = torch.zeros(1)\n","          msg2 = torch.zeros(1)\n","          action  = self.choose_action(state,msg1,msg2)\n","          msg1,msg2 = self.message(state,action,msg1,msg2)\n","          self.msg1.append(msg1)\n","          self.msg2.append(msg2)\n","        else:\n","          msg1 = self.msg1[0]\n","          msg2 = self.msg2[0]\n","          action= self.choose_action(state,msg1,msg2)\n","          self.msg1 = []\n","          self.msg2 = []\n","          self.msg1.append(msg1)\n","          self.msg2.append(msg2)\n","        next_state,reward,done,info = env.step(action)\n","        self.buffer.agent1_reward.append(reward[0])\n","        self.buffer.agent2_reward.append(reward[1]) \n","        self.update(state,next_state,reward,done,msg1,msg2)\n","        if done:\n","          state = next_state\n","          score += reward\n","        else:\n","          self.update(state,next_state,reward,done,msg1,msg2)\n","          state = next_state\n","          score += reward\n","          print(\"completed\")"],"metadata":{"id":"3KgneDX-tm4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","import numpy as np\n","from torch.autograd import variable as v\n","import sys\n","sys.path.append('./')\n","from main import Main\n","from buffer.buffer import Buffer\n","if __name__ ==\"__main__\":\n","  buffer = Buffer()\n","  main = Main(21,5,5,buffer,2,100,100)\n","  main.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nD8HRDBfTVa5","outputId":"7d4fa98f-6338-4468-8cec-c19baf36e70c"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["episode: 0 , agent1_reward: 0 , agent2_reward: 0\n","-1.553622198589335\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["-1.5552437553048892\n","-1.5574720664116666\n","-1.560243133334449\n","-1.563486442280751\n","-1.5671650870975342\n","-1.5712548350425593\n","-1.575708614987663\n","-1.5805063831971078\n","-1.585637538772731\n","-1.5910754824536282\n","-1.5968225094950492\n","-1.6028706094843563\n","-1.6092023937597704\n","-1.615810686905632\n","-1.6226832369141393\n","-1.629808164875258\n","-1.6371783612907491\n","-1.6447677525701798\n","-1.6525690407020137\n","-1.660574120979126\n","-1.668774261819734\n","-1.6771602222564292\n","-1.6857404116049477\n","-1.6945011060138149\n","-1.7034422902449267\n","-1.7125555445389677\n","-1.7218239864205553\n","-1.7312551031531074\n","-1.7408526679834986\n","-1.750617167457727\n","-1.7605535838405626\n","-1.7706662732265088\n","-1.7809529498255987\n","-1.7914103271914932\n","-1.8020406591143983\n","-1.8128699037974938\n","-1.8238899721539599\n","-1.8351069742334845\n","-1.8465308806247736\n","-1.8581455384084538\n","-1.8699656762224794\n","-1.8820087371382062\n","-1.8942739829058544\n","-1.9067918028331705\n","-1.919574235517134\n","-1.9326094276551957\n","-1.9459036871133235\n","-1.9594679342609187\n","-1.9732840374206293\n","-1.9873582803965744\n","-2.001658713329144\n","-2.016191864344732\n","-2.0309499289778645\n","-2.045914283922212\n","-2.061084256411175\n","-2.0764599989431023\n","-2.0920238048066455\n","-2.107771193848999\n","-2.1237011389211697\n","-2.1398166135408534\n","-2.1561188176988777\n","-2.1725966616645818\n","-2.189252002545092\n","-2.206065725046156\n","-2.223049500901687\n","-2.2401871926602075\n","-2.257507387563286\n","-2.2749987361821793\n","-2.292646336322058\n","-2.310434631629893\n","-2.328373270750945\n","-2.346457863237706\n","-2.364688737775993\n","-2.3830956610981753\n","-2.40162025223339\n","-2.4202691923957333\n","-2.4390361472052264\n","-2.4579288457136794\n","-2.4769211817162704\n","-2.4960321778301915\n","-2.5152445495582016\n","-2.534604238922116\n","-2.5540904815160816\n","-2.573695812043552\n","-2.5934348462823515\n","-2.6132572044389257\n","-2.633212615691633\n","-2.6532789549356037\n","-2.6734807676481434\n","-2.6938474605771585\n","-2.7143523226407913\n","-2.7349931597210473\n","-2.7558096183183762\n","-2.776800134376981\n","-2.7979284592080242\n","-2.8191938676064057\n","-2.8406302586705943\n","-2.862316596423238\n","-2.8841833690598575\n","-2.9062607913078584\n","-2.92851658659204\n","-2.9509322777433393\n","-2.973563513916855\n","-2.9964148947116427\n","-3.0194611198535024\n","-3.0427190677137896\n","-3.0662608818630246\n","-3.090034750872421\n","-3.1141197952828286\n","-3.138493217769377\n","-3.1631642470611983\n","-3.1880740789655895\n","-3.2132461028065094\n","-3.238715475338256\n","-3.2644756710854685\n","-3.290525098998423\n","-3.316786459239717\n","-3.343331003622178\n","-3.3701332131526103\n","-3.3972203770885505\n","-3.424596727503081\n","-3.4522534609504167\n","-3.480174659610121\n","-3.508381130466244\n","-3.536873703400037\n","-3.565675449394478\n","-3.594785001575409\n","-3.624218026324447\n","-3.653964746735163\n","-3.6840746893388037\n","-3.7145085134109523\n","-3.7453042579348628\n","-3.7764689449896633\n","-3.808011860947645\n","-3.8398995272031513\n","-3.872118152847226\n","-3.904659737818487\n","-3.9375186564672324\n","-3.9707011124212888\n","-4.004196802987746\n","-4.0380119456303625\n","-4.072085868232863\n","-4.106409673875594\n","-4.140989036944868\n","-4.175764748084434\n","-4.2107777101048605\n","-4.245987373996057\n","-4.281349388676429\n","-4.3169409960179355\n","-4.352668193508473\n","-4.388556739577109\n","-4.424594448016752\n","-4.4607092513213455\n","-4.497005676638953\n","-4.533400977228424\n","-4.569918350374104\n","-4.606507173377335\n","-4.643261626703784\n","-4.6801835339554705\n","-4.717206625223449\n","-4.754352681194729\n","-4.791597135855167\n","-4.828922425696895\n","-4.866363080456498\n","-4.903937583677793\n","-4.941689694107541\n","-4.979623269785673\n","-5.017819418481073\n","-5.0562681338737505\n","-5.094843045145902\n","-5.1336015811498115\n","-5.172533807824968\n","-5.211751756275792\n","-5.251178754313885\n","-5.290768825624291\n","-5.330642569023066\n","-5.370700380481154\n","-5.410977845070058\n","-5.45148641450951\n","-5.4921995781416335\n","-5.53315549807626\n","-5.574262566895824\n","-5.615574774942321\n","-5.656984272734317\n","-5.698486790799773\n","-5.74019190226557\n","-5.781989458523831\n","-5.823841747904177\n","-5.865827840821381\n","-5.907859019232145\n","-5.950081506802949\n","-5.992498073225369\n","-6.035183810303353\n","-6.078171030457215\n","-6.121436087425364\n","-6.16487818004696\n","-6.208429851801361\n","-6.252276563859655\n","-6.296391724711011\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["episode: 1 , agent1_reward: tensor(-3.2428, dtype=torch.float64) , agent2_reward: tensor(-3.2428, dtype=torch.float64)\n","-2.240327307567644\n","-2.239347663459869\n","-2.2381252890670824\n","-2.236792165254176\n","-2.2353826777659305\n","-2.233968045403971\n","-2.2326175461335596\n","-2.231376724489004\n","-2.2302927774605124\n","-2.229352862154497\n","-2.2285740104328893\n","-2.227960545743552\n","-2.2275485156331825\n","-2.227311666594101\n","-2.2273009227916734\n","-2.2274853933801024\n","-2.227878963567726\n","-2.228485923290767\n","-2.22932957990577\n","-2.230389520509738\n","-2.2316711694896694\n","-2.23317850045302\n","-2.23489886547968\n","-2.2368221368768806\n","-2.2388781474241584\n","-2.2411180846235665\n","-2.243588587520553\n","-2.246269313732267\n","-2.2491601117138678\n","-2.2521851158672725\n","-2.2554565033465703\n","-2.2589545013620134\n","-2.2626600248762845\n","-2.2665961018324863\n","-2.270712781061454\n","-2.2749414477817647\n","-2.279274956344052\n","-2.283788948011229\n","-2.2885539706782803\n","-2.2935262978805087\n","-2.2987679863220416\n","-2.304158558173291\n","-2.309759188226796\n","-2.3155749840329\n","-2.3216327768631193\n","-2.3278736826780024\n","-2.33436965157179\n","-2.3410994277104917\n","-2.3480083535088596\n","-2.355146011324234\n","-2.362499149580379\n","-2.370104800969747\n","-2.3779120863903094\n","-2.3859070363073434\n","-2.3940867894121487\n","-2.4024417359680603\n","-2.4110130615485144\n","-2.4197189829800485\n","-2.4286535802924245\n","-2.437698943948795\n","-2.4468761319533705\n","-2.456259893043079\n","-2.4658025233091942\n","-2.4755895661828564\n","-2.4856304344236366\n","-2.4958751800557697\n","-2.5063081108166196\n","-2.516956952134914\n","-2.527861460681191\n","-2.538908752047822\n","-2.5501539617118496\n","-2.5616600967739798\n","-2.5733589439926443\n","-2.5852330519841713\n","-2.597329457836743\n","-2.6096646882937904\n","-2.6222289079693795\n","-2.6350207016793674\n","-2.6480405784971097\n","-2.6612415569806043\n","-2.6746933727566793\n","-2.6883351771277915\n","-2.7021470900026854\n","-2.7160542933129976\n","-2.730155082244214\n","-2.744426829425019\n","-2.7588953840378907\n","-2.7735486461081345\n","-2.7883841598129964\n","-2.8034634691312297\n","-2.818703295676959\n","-2.83419133109496\n","-2.8499260782403293\n","-2.8658681241604436\n","-2.881933275482003\n","-2.8981118749190884\n","-2.914523678600748\n","-2.9311386832161395\n","-2.9479674763066037\n","-2.9650197965703056\n","-2.9822901387939256\n","-2.9997117903380675\n","-3.0173451720837976\n","-3.035145430074173\n","-3.0531406275309996\n","-3.0713570663802496\n","-3.0897103488196445\n","-3.108257485092814\n","-3.1270322367841112\n","-3.146026402664004\n","-3.16527604821622\n","-3.184701400216698\n","-3.204224453645197\n","-3.2239671725781047\n","-3.2438419491353536\n","-3.2639879216990275\n","-3.2842827791469302\n","-3.3047173563758268\n","-3.3252523374622025\n","-3.345756016129803\n","-3.366424482721369\n","-3.387186361275764\n","-3.4082042430417845\n","-3.429332989504294\n","-3.4506954317746534\n","-3.472149936968947\n","-3.4937492419795873\n","-3.5154578731590758\n","-3.5373630983968503\n","-3.5593918646439158\n","-3.581614869312528\n","-3.6040102388194004\n","-3.626594818561449\n","-3.649291388460259\n","-3.6721046734636227\n","-3.6950916580709086\n","-3.7183457575037417\n","-3.741903936625844\n","-3.7657074932985606\n","-3.7897302455393866\n","-3.813960239385473\n","-3.838253628987972\n","-3.8628789700914448\n","-3.8877409242385914\n","-3.9128563881428002\n","-3.9382457943163467\n","-3.9637298383139834\n","-3.989505567791104\n","-4.015500225907532\n","-4.041751891508628\n","-4.068217914447064\n","-4.09495212971115\n","-4.121953041225648\n","-4.149116767298395\n","-4.176493958432365\n","-4.204089167500917\n","-4.2319347789246615\n","-4.2600848817184405\n","-4.288543247431578\n","-4.317234613737448\n","-4.346246911601304\n","-4.375440460696057\n","-4.4048995516384295\n","-4.434689223112752\n","-4.464790431012522\n","-4.495147892072848\n","-4.525782209198684\n","-4.556698347786318\n","-4.58788070110021\n","-4.61937152737835\n","-4.651059207366433\n","-4.682894929151349\n","-4.714913009298293\n","-4.747201589255831\n","-4.779589444139054\n","-4.8120978025287355\n","-4.844745328422018\n","-4.877454972173787\n","-4.910306166595422\n","-4.943333590552527\n","-4.976520920504724\n","-5.009863683272136\n","-5.043250745735323\n","-5.076717803503454\n","-5.110223009175719\n","-5.14384723099436\n","-5.177510808838068\n","-5.211239811775404\n","-5.244989303612701\n","-5.278950986367664\n","-5.312993448846821\n","-5.347260434264676\n","-5.381611328457328\n","-5.416138809149831\n","-5.450878977862565\n","-5.485869733031554\n","-5.521132174217779\n","-5.55669129910297\n","-5.592506898484313\n","-5.628568005739401\n","episode: 2 , agent1_reward: tensor(-3.2941, dtype=torch.float64) , agent2_reward: tensor(-3.2941, dtype=torch.float64)\n","-1.4142242083906056\n","-1.4191064984712565\n","-1.425629404445783\n","-1.4334163952130776\n","-1.4421820397276837\n","-1.4517628533231606\n","-1.4620097336368318\n","-1.472784958705249\n","-1.4840314955858325\n","-1.4956686767662957\n","-1.5076635134210499\n","-1.5199598587020078\n","-1.5325118230257817\n","-1.5453243997608288\n","-1.558367345298609\n","-1.571641519101465\n","-1.5851171315067667\n","-1.5988042869844024\n","-1.6126939447065756\n","-1.6267997937037597\n","-1.6410757765618276\n","-1.6555546213007022\n","-1.6702302928821855\n","-1.6850930549742513\n","-1.7001663406791743\n","-1.7154392068074853\n","-1.7309091058419062\n","-1.7465871154229098\n","-1.7624722497945269\n","-1.7785325222599824\n","-1.794784226955781\n","-1.8112732330553836\n","-1.8279725031553866\n","-1.8448290968882382\n","-1.8619005214659905\n","-1.8791849146681572\n","-1.896674609199367\n","-1.9143530175987031\n","-1.932255919030935\n","-1.950371602197031\n","-1.9686481323446858\n","-1.9871599034229157\n","-2.0058572425162833\n","-2.024731307350023\n","-2.04381330125488\n","-2.0631282558580306\n","-2.08260261104608\n","-2.102255621617579\n","-2.1220859611119596\n","-2.1421661517995063\n","-2.162378163755695\n","-2.1828097024273254\n","-2.2034010097247485\n","-2.224218832643179\n","-2.245226285624277\n","-2.266445886758483\n","-2.287829919919695\n","-2.309369776279671\n","-2.331124607475923\n","-2.353038194734069\n","-2.3751283142922697\n","-2.3973699725160538\n","-2.419817850684043\n","-2.4425045119487376\n","-2.4653645331050282\n","-2.4883993695222077\n","-2.5115954893713015\n","-2.534992794435585\n","-2.5585629560825645\n","-2.582321454462207\n","-2.6063224898495285\n","-2.6305704716227822\n","-2.655060308421304\n","-2.6797324141435013\n","-2.7045408927613535\n","-2.7295599401952977\n","-2.7547984339883222\n","-2.7802806982399506\n","-2.805953922355868\n","-2.831827992122208\n","-2.857921822073898\n","-2.8842613396813324\n","-2.910854547637217\n","-2.9376181516016606\n","-2.964646309653168\n","-2.9919617572327044\n","-3.019519195265132\n","-3.0473338985923277\n","-3.0754486143533812\n","-3.1037332617459543\n","-3.1322628182367733\n","-3.1610085854763237\n","-3.1899204969959736\n","-3.219019236642146\n","-3.248380844071666\n","-3.2779663671365036\n","-3.307769526834317\n","-3.3378048864760945\n","-3.368083655593227\n","-3.398557323030915\n","-3.429240281369246\n","-3.4602020234734336\n","-3.491306792272347\n","-3.5226006283646942\n","-3.554118724572815\n","-3.58578797739389\n","-3.6176823458755822\n","-3.6497732825006217\n","-3.682097600698352\n","-3.7145872739864645\n","-3.7472424642267037\n","-3.7800937453648755\n","-3.8131074684660575\n","-3.8463117321751725\n","-3.879747078253449\n","-3.913399971579381\n","-3.947234516954068\n","-3.981230722142474\n","-4.015545557218126\n","-4.050111790430866\n","-4.084927568835035\n","-4.120038660982518\n","-4.155473711842614\n","-4.191180054221101\n","-4.227185285557063\n","-4.263400420083508\n","-4.299926502752447\n","-4.3367351963518175\n","-4.373833406730084\n","-4.411176393605989\n","-4.448762489755291\n","-4.4866389444132775\n","-4.524836621871626\n","-4.563289079057258\n","-4.602090637747619\n","-4.6412106455012925\n","-4.680639880439253\n","-4.72040846811478\n","-4.7604664855815635\n","-4.800760459737999\n","-4.8412914081881\n","-4.882115547471106\n","-4.923218593991322\n","-4.964572306176597\n","-5.006124113614869\n","-5.047998412680938\n","-5.090021150182442\n","-5.132261470185398\n","-5.174705717773817\n","-5.2173454289929735\n","-5.260149684119247\n","-5.3032700875391505\n","-5.346588525484044\n","-5.390093971771253\n","-5.433971270012289\n","-5.478143909627332\n","-5.5226149428771\n","-5.567367675953368\n","-5.612485523233108\n","-5.65790694148259\n","-5.703648407509284\n","-5.749660994318095\n","-5.795934001465204\n","-5.8424678225455615\n","-5.889263675494584\n","-5.936330853292128\n","-5.983685717606364\n","-6.031245310186359\n","-6.0791159062502205\n","-6.127258431663242\n","-6.175761940530895\n","-6.22446905709745\n","-6.273467433931698\n","-6.3227772496935\n","-6.372335215251677\n","-6.422244483952504\n","-6.472513066352871\n","-6.523090306800258\n","-6.573957110469516\n","-6.625190097856544\n","-6.676713372900913\n","-6.7285678500555015\n","-6.780758039913502\n","-6.833380944548505\n","-6.886340953486608\n","-6.93957791319653\n","-6.993072682728361\n","-7.046870161424014\n","-7.101029420951042\n","-7.155425080902936\n","-7.2099455210438865\n","-7.2647916669090264\n","-7.319900599253388\n","-7.375239541188144\n","-7.430797258942956\n","-7.486429879986094\n","-7.542147730561261\n","-7.597988659961185\n","-7.653927215593379\n","-7.709910814709272\n","episode: 3 , agent1_reward: tensor(-3.7927, dtype=torch.float64) , agent2_reward: tensor(-3.7927, dtype=torch.float64)\n","-2.9293689914548553\n","-2.913501653042987\n","-2.892630992697074\n","-2.8680937391631445\n","-2.8408892476687697\n","-2.811782662543501\n","-2.7812889179264904\n","-2.749860796014673\n","-2.7178109401105472\n","-2.685358655084921\n","-2.652632634154141\n","-2.619809127683275\n","-2.586990317196352\n","-2.5542422031900447\n","-2.521596485906855\n","-2.489103712443431\n","-2.4567881437806833\n","-2.424678871136761\n","-2.392758467591719\n","-2.3610463448612964\n","-2.3295454636428796\n","-2.2982607226038074\n","-2.2671932213312154\n","-2.236360529582511\n","-2.205751371981184\n","-2.1753558304421223\n","-2.1451642045404453\n","-2.115231207766272\n","-2.085538282661561\n","-2.056094508073973\n","-2.0268842826239117\n","-1.9979160839688097\n","-1.9691893401803213\n","-1.9406999742797018\n","-1.9124522176753787\n","-1.8844378831797366\n","-1.8566492114089774\n","-1.8290906707479064\n","-1.801736391818118\n","-1.7745986626682653\n","-1.7476810171084114\n","-1.720989589673204\n","-1.6945603815948895\n","-1.6683463160912075\n","-1.64237369817844\n","-1.6166273569204446\n","-1.5911263394572097\n","-1.5658406891006997\n","-1.5408025505621552\n","-1.5159794810315934\n","-1.4913617898615372\n","-1.4669535726968563\n","-1.4427960352293052\n","-1.4188648628945275\n","-1.3951686511977028\n","-1.3716825406486115\n","-1.3484199520176514\n","-1.3253697991626932\n","-1.3025476177290014\n","-1.2799642790218333\n","-1.2576037461197898\n","-1.235468412176408\n","-1.2135505053455427\n","-1.1918631456878004\n","-1.1703949057016216\n","-1.1491612358317698\n","-1.128150904953124\n","-1.1073643595390257\n","-1.0867991861151287\n","-1.0664569035461924\n","-1.0463404388553232\n","-1.0264479410371194\n","-1.0067962479926795\n","-0.9873675796963466\n","-0.9681534705434964\n","-0.9491601273905481\n","-0.9303952686162872\n","-0.911857349183285\n","-0.8935429899212495\n","-0.8754538364565323\n","-0.8575961089464206\n","-0.8399625565921673\n","-0.822551632213254\n","-0.8053561157387508\n","-0.7883882341736694\n","-0.7716387706676636\n","-0.7551194352200702\n","-0.738812289548144\n","-0.7227336270997076\n","-0.7068884994173091\n","-0.6912617589036567\n","-0.6758514290487297\n","-0.6606575257535989\n","-0.6456905187613006\n","-0.6309443139186518\n","-0.6164221132286833\n","-0.6021267317544514\n","-0.588060024282295\n","-0.5742109459912044\n","-0.5605898237737692\n","-0.5471931900010879\n","-0.5340222945912942\n","-0.5210912021545846\n","-0.5083674243144185\n","-0.49585854852286804\n","-0.48357689346398613\n","-0.47152207293110526\n","-0.45969380819435557\n","-0.44809003482072673\n","-0.43671194257880286\n","-0.42555786248790317\n","-0.4146359319519689\n","-0.40394290385767545\n","-0.3934719187787618\n","-0.3832262193406328\n","-0.3731945971931316\n","-0.36339285888736683\n","-0.3538205415233266\n","-0.3444713678760436\n","-0.33535011259218483\n","-0.32645639695500184\n","-0.31778728483428764\n","-0.309342822204819\n","-0.3011248799329612\n","-0.29312908703983426\n","-0.28535787091764586\n","-0.27780445346614957\n","-0.2704737941580647\n","-0.263372013468832\n","-0.25649715607391677\n","-0.24984869811529642\n","-0.24343197898434116\n","-0.23724811006522473\n","-0.23128691721796257\n","-0.22554835032998988\n","-0.22003545787174894\n","-0.21474366013212898\n","-0.20967775120290516\n","-0.20483764992415884\n","-0.20022521443691954\n","-0.19583911476109964\n","-0.19167920074017325\n","-0.18774406841746674\n","-0.18404236969255736\n","-0.1805673452681431\n","-0.17732102496044433\n","-0.17430108621469087\n","-0.1715064158274918\n","-0.16893733864967922\n","-0.16659713450038444\n","-0.16448075985150798\n","-0.1625940768903114\n","-0.1609333029892419\n","-0.1594998456936778\n","-0.1582945215014797\n","-0.15731563733429832\n","-0.15656235411545666\n","-0.1560346489634402\n","-0.15573256472864694\n","-0.15565515872637542\n","-0.15580530370660595\n","-0.1561835140911077\n","-0.15678933463271022\n","-0.15762363912862118\n","-0.1586851523516384\n","-0.15997424077468886\n","-0.16149243302842206\n","-0.16323694778033698\n","-0.1652075213496339\n","-0.16740242862948204\n","-0.16982388166238127\n","-0.17247579485191875\n","-0.17535293024735155\n","-0.17845744989322\n","-0.1817881516259989\n","-0.18534252721351738\n","-0.18912300348128397\n","-0.19313138882744033\n","-0.19736993659252394\n","-0.2018336653989694\n","-0.2065273809775864\n","-0.21144193553795587\n","-0.21658810925559147\n","-0.22196179010088626\n","-0.2275652963109564\n","-0.2333951592298189\n","-0.23945446733669307\n","-0.24573847492150563\n","-0.25224947979338364\n","-0.2589861111143537\n","-0.2659507003132545\n","-0.27314158640251146\n","-0.28055889555824304\n","-0.2882076335256189\n","-0.2960845512227624\n","-0.30418568325944884\n","-0.31251562912244657\n","-0.3210707642801308\n","-0.3298595630189574\n","-0.3388743320695071\n","episode: 4 , agent1_reward: tensor(-0.9278, dtype=torch.float64) , agent2_reward: tensor(-0.9278, dtype=torch.float64)\n","-3.648220059262624\n","-3.638395937806061\n","-3.6254906528779194\n","-3.6103151491084406\n","-3.593519126477249\n","-3.5755559625626905\n","-3.5567877459136152\n","-3.5374820895234222\n","-3.517847357537924\n","-3.498035320122076\n","-3.4781427029121565\n","-3.4582804455838385\n","-3.4384824774940244\n","-3.4187959102179764\n","-3.399244737931503\n","-3.379866377747544\n","-3.3606640907958107\n","-3.341646518303998\n","-3.322821113562651\n","-3.304201113038332\n","-3.2857941074627597\n","-3.2676172712456193\n","-3.249676602146627\n","-3.2319556262977147\n","-3.2144635490392868\n","-3.197196177707238\n","-3.180130828343101\n","-3.163301166295467\n","-3.1466921142332342\n","-3.1303207166182956\n","-3.114157108164627\n","-3.0982270955700564\n","-3.0825284140677076\n","-3.067049362347721\n","-3.0518120928854326\n","-3.036810008996415\n","-3.0220545683190156\n","-3.0075299140308784\n","-2.993254441006311\n","-2.9792102876685878\n","-2.9654177957639867\n","-2.9518592861864787\n","-2.938532302440489\n","-2.925431693258611\n","-2.912549963234741\n","-2.8998816144298467\n","-2.8874466368557403\n","-2.8752259805615914\n","-2.8632255095891255\n","-2.8514479174087692\n","-2.8398999639884064\n","-2.82857222651773\n","-2.817471711158586\n","-2.8065801576837046\n","-2.7959063004813602\n","-2.785456441701965\n","-2.7752052335989736\n","-2.7651960154379287\n","-2.7553931557605877\n","-2.745819461555171\n","-2.7364831218010126\n","-2.7273630040434504\n","-2.7184632553381465\n","-2.7098095499659522\n","-2.701390759010272\n","-2.6931725715056745\n","-2.68516499145812\n","-2.677411714055148\n","-2.6698933633480273\n","-2.662596966706034\n","-2.6555053245558935\n","-2.6486430302671726\n","-2.6420435298782774\n","-2.6356668236258045\n","-2.629513600613143\n","-2.623581148363444\n","-2.6178642783572554\n","-2.612346037879993\n","-2.6070424338595153\n","-2.6019583486109807\n","-2.5970687077997083\n","-2.5923752977925894\n","-2.5878807971334092\n","-2.5836049014440574\n","-2.57953124980735\n","-2.575690978192705\n","-2.5720454609177392\n","-2.5686035297535246\n","-2.565369434466782\n","-2.5623507535829906\n","-2.559566997810019\n","-2.55701185129261\n","-2.55467020048138\n","-2.5525461712164668\n","-2.550643602197298\n","-2.548983866784613\n","-2.5475441633364984\n","-2.546301439065141\n","-2.545280185843642\n","-2.5444640435198926\n","-2.543888994124036\n","-2.5435292349841805\n","-2.543394674766124\n","-2.5435005650308113\n","-2.5438180876959957\n","-2.544346912155045\n","-2.545097624956254\n","-2.5460993787299446\n","-2.5473390402014555\n","-2.5487890096270482\n","-2.550434547198229\n","-2.552307681247803\n","-2.5543860330297687\n","-2.556656207612856\n","-2.559129870040917\n","-2.5617972769465522\n","-2.5646714745579384\n","-2.5677231641379956\n","-2.570930313186754\n","-2.5742855948865344\n","-2.5778640036576115\n","-2.5815949933151128\n","-2.5855118769651457\n","-2.5896252829195943\n","-2.5939434995649897\n","-2.598386608591406\n","-2.603013707748326\n","-2.607791116345362\n","-2.6127791216201737\n","-2.617957912816999\n","-2.6232776721783693\n","-2.6287623799086055\n","-2.6344394886875597\n","-2.6403513485402286\n","-2.6464453064546722\n","-2.6528515743306498\n","-2.659541874691959\n","-2.6664582348987396\n","-2.6736725992489\n","-2.6811845058032064\n","-2.689008085293311\n","-2.6971182294061005\n","-2.7055459293616155\n","-2.7142924802206263\n","-2.7233713192304045\n","-2.7327985761102203\n","-2.7425435529570263\n","-2.75262637559842\n","-2.7630437575033095\n","-2.773826385132213\n","-2.784963081219451\n","-2.7964217550298125\n","-2.8082055932155248\n","-2.82027208814087\n","-2.8326347180115503\n","-2.8452911158736303\n","-2.858210526722819\n","-2.871445681897253\n","-2.8849860755432037\n","-2.8988185778568214\n","-2.912951804443144\n","-2.927338377237245\n","-2.9419958761850546\n","-2.956852720319631\n","-2.9719804674252583\n","-2.9873755249017515\n","-3.0029868528325707\n","-3.0188040578489335\n","-3.034917249272268\n","-3.0513669448981613\n","-3.0680800367197376\n","-3.0850009059716257\n","-3.1022273006554775\n","-3.1196016905743633\n","-3.137209755287916\n","-3.1550976199901886\n","-3.1731272630980616\n","-3.191247469496559\n","-3.2096498674476335\n","-3.2281462348852266\n","-3.246732777968559\n","-3.265324133487615\n","-3.284169062440209\n","-3.3030267671776876\n","-3.322049365666925\n","-3.3410815233798363\n","-3.360431156670756\n","-3.379751801453475\n","-3.399217720386508\n","-3.4190249494373552\n","-3.4388850083362703\n","-3.4590946283281383\n","-3.47951607151596\n","-3.5000706293144406\n","-3.521000562627973\n","-3.542392305394715\n","-3.564114493840864\n","-3.586019881700477\n","-3.6081261280229375\n","-3.6306364879678403\n","episode: 5 , agent1_reward: tensor(-2.9146, dtype=torch.float64) , agent2_reward: tensor(-2.9146, dtype=torch.float64)\n","-1.7693034791792577\n","-1.7692907894084773\n","-1.7693398103159959\n","-1.7696366022030134\n","-1.7701496285306662\n","-1.7707717914238004\n","-1.7715218944954427\n","-1.7724041161654207\n","-1.7735103168687811\n","-1.7748473513996545\n","-1.776450802831233\n","-1.77825631196055\n","-1.780245645557981\n","-1.7823782702857016\n","-1.7848317935086029\n","-1.7873820856132925\n","-1.7901952203031428\n","-1.793271994117193\n","-1.7965203866606132\n","-1.8000443918056015\n","-1.8037087415520563\n","-1.8075958980089288\n","-1.81178337299164\n","-1.8161271120557552\n","-1.8206261180101435\n","-1.8252446030766372\n","-1.8299646409140562\n","-1.835031562339866\n","-1.8403085010849922\n","-1.8457533899883272\n","-1.851381795028607\n","-1.857193262968657\n","-1.8632064377334483\n","-1.8694036548733464\n","-1.8757602928662633\n","-1.8823595068269112\n","-1.8891511785846324\n","-1.8961694680509047\n","-1.903380762310479\n","-1.9108220282555766\n","-1.918467569937196\n","-1.9263593430565598\n","-1.9344965919789225\n","-1.9428832346573417\n","-1.951510614889223\n","-1.960422552142553\n","-1.9695913365391904\n","-1.9790614103951356\n","-1.98878438239496\n","-1.9987750769690935\n","-2.009023111011115\n","-2.019538839177456\n","-2.030322501261888\n","-2.0413979619106395\n","-2.0526995910816352\n","-2.0642092621295625\n","-2.0759586515666912\n","-2.087941119086415\n","-2.1001532310901774\n","-2.112714671136442\n","-2.1255905078701227\n","-2.138736777366746\n","-2.152166994721361\n","-2.1657979500156754\n","-2.1797340737385738\n","-2.19388705706727\n","-2.208273541737067\n","-2.2228935387183073\n","-2.2377390343098305\n","-2.252678409836493\n","-2.267793276136\n","-2.283065235825976\n","-2.2984942544843454\n","-2.314018697307416\n","-2.3296959109112643\n","-2.3455302313100663\n","-2.361489816164638\n","-2.3776621129139155\n","-2.3940529888422777\n","-2.4106175629243562\n","-2.4273075893459373\n","-2.4441023895544576\n","-2.4610662574154647\n","-2.478102828523562\n","-2.4952554431123803\n","-2.512504412105358\n","-2.5297998729768674\n","-2.547134399098794\n","-2.564386545755882\n","-2.581620301243197\n","-2.5987832256190138\n","-2.616045679591098\n","-2.633394361565082\n","-2.6507768402144087\n","-2.668260911172548\n","-2.685899152897245\n","-2.7037442954429154\n","-2.721681008756068\n","-2.7398301294672662\n","-2.758236163724532\n","-2.7769630364477544\n","-2.7960817168521137\n","-2.8155275143906704\n","-2.8353677951038305\n","-2.8555911742789872\n","-2.876233509700382\n","-2.897326286582608\n","-2.9187788665039958\n","-2.9406540582631733\n","-2.9629387493963373\n","-2.9855804344772157\n","-3.008586188514374\n","-3.0320042943568284\n","-3.0558456428236447\n","-3.0801146492367177\n","-3.1047954040998578\n","-3.129893075692145\n","-3.1553748871862743\n","-3.181195753877004\n","-3.207365620205576\n","-3.2338568194142727\n","-3.2606552374172315\n","-3.28776703500049\n","-3.315171362329851\n","-3.3428958235898008\n","-3.370950648145956\n","-3.39934253321488\n","-3.428073776670231\n","-3.4571453885053893\n","-3.486621723567355\n","-3.5164735862780416\n","-3.5467299831409886\n","-3.577350411901851\n","-3.6083655517830273\n","-3.639767186783216\n","-3.6715677534220337\n","-3.7038849527393833\n","-3.736565948319398\n","-3.7695640490614384\n","-3.8029029755211834\n","-3.836615001975823\n","-3.8707660856269994\n","-3.9055308160143167\n","-3.9406624646528776\n","-3.9762962861113076\n","-4.012585893000089\n","-4.0495814762213875\n","-4.087098768203879\n","-4.12514307802987\n","-4.1636417966348205\n","-4.202533954596033\n","-4.242082854851467\n","-4.281977626560627\n","-4.322174280120231\n","-4.3626556791146065\n","-4.403204606466823\n","-4.4440520256836304\n","-4.484898517677264\n","-4.525847676560753\n","-4.567119510804866\n","-4.6088604832859685\n","-4.651250766111059\n","-4.69400305180949\n","-4.737331793967911\n","-4.780983572887097\n","-4.825191343507762\n","-4.870071024934473\n","-4.915614659257185\n","-4.961765029112865\n","-5.00883110040107\n","-5.056403194509813\n","-5.104246565260906\n","-5.153079698087872\n","-5.202649816615138\n","-5.252745830618828\n","-5.303533135784083\n","-5.354574766147339\n","-5.405934300462988\n","-5.457889374743276\n","-5.510801641906353\n","-5.563708229810928\n","-5.616859859262293\n","-5.669956336360846\n","-5.724598494339476\n","-5.779894820072073\n","-5.835670567482545\n","-5.891627619561768\n","-5.948769972721695\n","-6.006727895911321\n","-6.064881151159162\n","-6.123383601743157\n","-6.181575684063417\n","-6.240246939179052\n","-6.299000582689328\n","-6.358073701767998\n","-6.418680692556396\n","-6.47959343158411\n","-6.542617034953507\n","-6.60862212349621\n","-6.675714607827147\n","episode: 6 , agent1_reward: tensor(-3.2316, dtype=torch.float64) , agent2_reward: tensor(-3.2316, dtype=torch.float64)\n","-1.7154705804792831\n","-1.7239662404819482\n","-1.7352186388436017\n","-1.74826582518277\n","-1.7627558791212863\n","-1.7790908809621533\n","-1.7967569002518158\n","-1.8158650076307479\n","-1.8360230744556427\n","-1.85786940572459\n","-1.8823858546965762\n","-1.909050614469591\n","-1.9373645629083842\n","-1.96698932000162\n","-1.9979389934379874\n","-2.0301065790305826\n","-2.0633363852967714\n","-2.097541228895593\n","-2.1326786345724504\n","-2.168552971732038\n","-2.2050638897360173\n","-2.241816871302923\n","-2.2794003396720885\n","-2.3180542952216667\n","-2.3570809657617158\n","-2.3962679100072073\n","-2.4357770445745484\n","-2.4757052122720795\n","-2.5162309601271025\n","-2.5571394197892388\n","-2.5982986662543324\n","-2.640295894225403\n","-2.6824399165320236\n","-2.7248307876724973\n","-2.767973272005646\n","-2.811298300199912\n","-2.8566393694053853\n","-2.9033590886454803\n","-2.9497332157541716\n","-2.9957808848405\n","-3.0431101241402465\n","-3.0912202219950076\n","-3.1397765550619487\n","-3.189010149311401\n","-3.2383520083527304\n","-3.289053205544443\n","-3.341679596365478\n","-3.393610307761671\n","-3.4455394179755072\n","-3.496781418910645\n","-3.5468662191120806\n","-3.596200511878666\n","-3.6491521553157797\n","-3.705548424558899\n","-3.7643847460288598\n","-3.825854051178843\n","-3.8861685934045633\n","-3.9543870068212548\n","-4.025016242631285\n","-4.101074147260539\n","-4.180573729899368\n","-4.264149538648057\n","-4.3508104107997205\n","-4.433392476959272\n","-4.527938959249649\n","-4.624307364698725\n","-4.715338021801218\n","-4.819985849315623\n","-4.931433990870668\n","-5.029465832051576\n","-5.1400638195199555\n","-5.2766943893499025\n","-5.397469397186999\n","-5.523084939227837\n","-5.6288687522545775\n","-5.71921879329554\n","-5.7973307055401095\n","-5.865712071325133\n","-5.990040309062684\n","-6.1410793701453485\n","-6.304502717491861\n","-6.545194133949221\n","-6.833279296025463\n","-7.148710643227381\n","-7.544311857332533\n","-8.006132148535954\n","-8.50849409390722\n","-9.06797391695173\n","-9.687569273592283\n","-10.336920748335576\n","-11.031025673469468\n","-11.855373786908995\n","-12.800697142688856\n","-13.820175486072047\n","-14.984906893506142\n","-16.281800142201977\n","-17.620695424868533\n","-19.08581873750387\n","-20.66261663216107\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 7 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 8 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 9 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 10 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 11 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 12 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 13 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 14 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 15 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 16 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 17 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 18 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","episode: 19 , agent1_reward: tensor(nan, dtype=torch.float64) , agent2_reward: tensor(nan, dtype=torch.float64)\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n","nan\n"]}]},{"cell_type":"code","source":["buffer.agent2_mean_reward"],"metadata":{"id":"Bz8_s-0MM632"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["msg1 = buffer.agent1_msg\n","msg2 = buffer.agent2_msg"],"metadata":{"id":"EeJJHOlVeSFt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dict = {\"msg1\":msg1,\"msg2\":msg2}"],"metadata":{"id":"7sYO6dyIe8FF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd  \n","df = pd.DataFrame(dict) \n","df.to_csv('communication.csv')"],"metadata":{"id":"ZZBa0J_xfM50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def csvs(buffer):\n","  msg1 = buffer.agent1_msg\n","  msg2 = buffer.agent2_msg\n","  d1   = {\"msg1\":msg1,\"msg2\":msg2}\n","  d1 = pd.DataFrame(dict) \n","  d1.to_csv('communication.csv')\n"," "],"metadata":{"id":"Ep9PvEpsfPDr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d = torch.tensor(3.14)"],"metadata":{"id":"cpU-NUsWNIcQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["l = nn.Linear(0,32)"],"metadata":{"id":"b818SNrZNJ1v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["l(d)"],"metadata":{"id":"ctdzCc8JNNL3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-lpLKfQZNNsZ"},"execution_count":null,"outputs":[]}]}