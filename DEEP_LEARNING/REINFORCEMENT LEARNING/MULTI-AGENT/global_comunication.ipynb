{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNrL7J7TH92rq6eISwemDOJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!git clone https://github.com/openai/multiagent-particle-envs.git\n","!pip install gym==0.10.5"],"metadata":{"id":"ez1A8sazqawW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668137124554,"user_tz":-330,"elapsed":13952,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"fde51b85-fef7-4c37-9a84-bf5bc687bc92"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'multiagent-particle-envs'...\n","remote: Enumerating objects: 242, done.\u001b[K\n","remote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Compressing objects: 100% (5/5), done.\u001b[K\n","remote: Total 242 (delta 0), reused 3 (delta 0), pack-reused 237\u001b[K\n","Receiving objects: 100% (242/242), 107.24 KiB | 8.94 MiB/s, done.\n","Resolving deltas: 100% (127/127), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gym==0.10.5\n","  Downloading gym-0.10.5.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 7.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.21.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.15.0)\n","Collecting pyglet>=1.2.0\n","  Downloading pyglet-2.0.0-py3-none-any.whl (966 kB)\n","\u001b[K     |████████████████████████████████| 966 kB 12.7 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2.10)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.10.5-py3-none-any.whl size=1581308 sha256=3f937b9db30052b3fcc5bfb0f370d6e7edc1f77c4c9657989498043a36909643\n","  Stored in directory: /root/.cache/pip/wheels/7a/2c/df/a05b548a40fae16ca400ecbeda0067e1a296499c1fbd7e0c9a\n","Successfully built gym\n","Installing collected packages: pyglet, gym\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","Successfully installed gym-0.10.5 pyglet-2.0.0\n"]}]},{"cell_type":"code","source":["cd multiagent-particle-envs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KnNvImRtqgpR","executionInfo":{"status":"ok","timestamp":1668137124556,"user_tz":-330,"elapsed":22,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"98ca99e2-9512-4083-a8d9-5a51a247737d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/multiagent-particle-envs\n"]}]},{"cell_type":"code","source":["pip install -e."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_VJh2qphqspR","executionInfo":{"status":"ok","timestamp":1668137131201,"user_tz":-330,"elapsed":6657,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"95397a03-eeb2-4118-caf3-42e496659dd9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/multiagent-particle-envs\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from multiagent==0.0.1) (0.10.5)\n","Collecting numpy-stl\n","  Downloading numpy_stl-2.17.1-py3-none-any.whl (18 kB)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (2.23.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.21.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.15.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (2.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (2.10)\n","Requirement already satisfied: python-utils>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from numpy-stl->multiagent==0.0.1) (3.4.5)\n","Installing collected packages: numpy-stl, multiagent\n","  Running setup.py develop for multiagent\n","Successfully installed multiagent-0.0.1 numpy-stl-2.17.1\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Teb0vqntqEZw","executionInfo":{"status":"ok","timestamp":1668137134162,"user_tz":-330,"elapsed":2970,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","import numpy as np\n","from make_env import make_env\n","env = make_env(\"simple_tag\")\n","#agent 4 is different agent"]},{"cell_type":"code","source":["class Msg_net(nn.Module):\n","  def __init__(self):\n","    super(Msg_net,self).__init__()\n","    self.fc2= nn.Linear(32,64)\n","    self.fc3= nn.Linear(64,32)\n","    self.fc4= nn.Linear(32,2)\n","  def message_translate(self,state,values,messages,agents):\n","    w  = torch.cat([state[0],values[0],agents[0]],0)\n","    v1 = torch.cat([state[1],values[1],agents[1]],0)\n","    v2 = torch.cat([state[2],values[2],agents[2]],0)\n","    v  = torch.cat([v1,v2],0)\n","    v  = torch.rehsape(v,(v.shape[0],1))\n","    w  = torch.reshape(w,(w.shape[0],1))\n","    q  = messages\n","    t1 = torch.matmul(v,q)\n","    t2 = torch.matmul(w,q)\n","    final  = t1+t2\n","    return final  \n","  def forward(self,states,values,messages):\n","    final = self.message_translate(states,values,messages)\n","    fc1   = nn.Linear(final.shape[0],32)\n","    x     = f.relu(fc1(final))\n","    x     = f.relu(self.fc2(x))\n","    x     = f.relu(self.fc3(x))\n","    x     = f.relu(self.fc4(x))\n","    return x"],"metadata":{"id":"ZjDbTG1XqaER","executionInfo":{"status":"ok","timestamp":1668137134163,"user_tz":-330,"elapsed":21,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class Actor(nn.Module):\n","  def __init__(self,state_size,action_size,reward_size):\n","    super(Actor,self).__init__()\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.reward_size = reward_size\n","    self.fc1 = nn.Linear(17,32)\n","    self.fc2 = nn.Linear(32,64)\n","    self.fc3 = nn.Linear(64,128)\n","    self.fc4 = nn.Linear(128,64)\n","    self.fc5 = nn.Linear(64,32)\n","    self.fc6 = nn.Linear(32,5)\n","  def forward(self,state,message):\n","    x = torch.cat([state,message],0)\n","    x = f.relu(self.fc1(x))\n","    x = f.relu(self.fc2(x))\n","    x = f.relu(self.fc3(x))\n","    x = f.relu(self.fc4(x))\n","    x = f.relu(self.fc5(x))\n","    x = f.softmax(self.fc6(x))\n","    return x"],"metadata":{"id":"THnDw1ETsewX","executionInfo":{"status":"ok","timestamp":1668137134164,"user_tz":-330,"elapsed":20,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Critic(nn.Module):\n","  def __init__(self,state_size,action_size,reward_size):\n","    super(Critic,self).__init__()\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.reward_size = reward_size\n","    self.fc1 = nn.Linear(17,32)\n","    self.fc2 = nn.Linear(32,64)\n","    self.fc3 = nn.Linear(64,128)\n","    self.fc4 = nn.Linear(128,64)\n","    self.fc5 = nn.Linear(64,32)\n","    self.fc6 = nn.Linear(32,1)\n","  def forward(self,state):\n","    x = f.relu(self.fc1(x))\n","    x = f.relu(self.fc2(x))\n","    x = f.relu(self.fc3(x))\n","    x = f.relu(self.fc4(x))\n","    x = f.relu(self.fc5(x))\n","    x = f.relu(self.fc6(x))\n","    return x"],"metadata":{"id":"4ReEX3QUskUX","executionInfo":{"status":"ok","timestamp":1668137134165,"user_tz":-330,"elapsed":20,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class Actor_good(nn.Module):\n","  def __init__(self,state_size,action_size):\n","    super(Actor_good,self).__init__()\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.fc1 = nn.Linear(14,32)\n","    self.fc2 = nn.Linear(32,64)\n","    self.fc3 = nn.Linear(64,128)\n","    self.fc4 = nn.Linear(128,64)\n","    self.fc5 = nn.Linear(64,32)\n","    self.fc6 = nn.Linear(32,5)\n","  def forward(self,x):\n","    x = f.relu(self.fc1(x))\n","    x = f.relu(self.fc2(x))\n","    x = f.relu(self.fc3(x))\n","    x = f.relu(self.fc4(x))\n","    x = f.relu(self.fc5(x))\n","    x = f.softmax(self.fc6(x))\n","    return x"],"metadata":{"id":"NGgIoPmQ3FW8","executionInfo":{"status":"ok","timestamp":1668137134166,"user_tz":-330,"elapsed":19,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class Agent1:\n","  def __init__(self,state_size,action_size,lr1,lr2,gamma,lamda):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.lr1 = lr1\n","    self.lr2 = lr2\n","    self.gamma = gamma\n","    self.lamda = lamda\n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.actor = Actor(self.state_size,self.action).to(self.device)\n","    self.critic = Critic(self.state_size,self.action_size).to(self.device)\n","    self.actor_optim = optim.Adam(self.actor.parameters() ,lr = self.lr1)\n","    self.critic_optim = optim.Adam(self.critic.parameters() ,lr = self.lr2)\n","  def action(self,state,message):\n","    action = self.actor(state,message)\n","    return action\n","  def q_value(self,state,action,reward):\n","    value   = torch.cat([state,action,reward],0)\n","    q_value = self.critic(value)\n","    return q_value\n","  def ppo_iters(self,reward,done,value,next_value):\n","    gae = 0\n","    returns = []\n","    for i in range(5):\n","      delta = reward + next_value * (1-done)*self.gamma - value\n","      gae   = delta+ gae*self.lamda*(1-done)\n","      returns.insert(0,gae+value+next_value)\n","    return returns\n","  def learn(self,state,next_state,action,next_action,reward,done,message):\n","    log_prob = torch.log(action).to(self.device)\n","    next_log_prob = torch.log(next_action).to(self.device)\n","    ratio = next_log_prob/log_prob\n","    value = self.q_value(state,action,reward)\n","    next_value = self.q_value(next_state,next_action,reward)\n","    returns = self.ppo_iters(reward,done,value,next_value)\n","    advantage = torch.tensor(returns).float() - value\n","    s1        = ratio * advantage\n","    s2        = torch.clamp(ratio ,1- 0.2,0.2)* advantage\n","    actor_loss = torch.min(s1,s2).mean()\n","    critic_loss = advantage - value\n","    critic_loss = critic_loss*critic_loss\n","    loss        = actor_loss + 0.5*critic_loss \n","    self.actor_optim.zero_grad()\n","    self.critic_optim.zero_grad()\n","    loss.backward()\n","    self.actor_optim.step()\n","    self.critic_optim.step()\n"],"metadata":{"id":"gC1HxTm9zfWP","executionInfo":{"status":"ok","timestamp":1668137134167,"user_tz":-330,"elapsed":19,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class Agent2:\n","  def __init__(self,state_size,action_size,lr1,lr2,gamma,lamda):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.lr1 = lr1\n","    self.lr2 = lr2\n","    self.gamma = gamma\n","    self.lamda = lamda\n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.actor = Actor(self.state_size,self.action).to(self.device)\n","    self.critic = Critic(self.state_size,self.action_size).to(self.device)\n","    self.actor_optim = optim.Adam(self.actor.parameters() ,lr = self.lr1)\n","    self.critic_optim = optim.Adam(self.critic.parameters() ,lr = self.lr2)\n","  def action(self,state,message):\n","    action = self.actor(state,message)\n","    return action\n","  def q_value(self,state,action,reward):\n","    value   = torch.cat([state,action,reward],0)\n","    q_value = self.critic(value)\n","    return q_value\n","  def ppo_iters(self,reward,done,value,next_value):\n","    gae = 0\n","    returns = []\n","    for i in range(5):\n","      delta = reward + next_value * (1-done)*self.gamma - value\n","      gae   = delta+ gae*self.lamda*(1-done)\n","      returns.insert(0,gae+value+next_value)\n","    return returns\n","  def learn(self,state,next_state,action,next_action,reward,done,message):\n","    log_prob = torch.log(action).to(self.device)\n","    next_log_prob = torch.log(next_action).to(self.device)\n","    ratio = next_log_prob/log_prob\n","    value = self.q_value(state,action,reward)\n","    next_value = self.q_value(next_state,next_action,reward)\n","    returns = self.ppo_iters(reward,done,value,next_value)\n","    advantage = torch.tensor(returns).float() - value\n","    s1        = ratio * advantage\n","    s2        = torch.clamp(ratio ,1- 0.2,0.2)* advantage\n","    actor_loss = torch.min(s1,s2).mean()\n","    critic_loss = advantage - value\n","    critic_loss = critic_loss*critic_loss\n","    loss        = actor_loss + 0.5*critic_loss \n","    self.actor_optim.zero_grad()\n","    self.critic_optim.zero_grad()\n","    loss.backward()\n","    self.actor_optim.step()\n","    self.critic_optim.step()\n"],"metadata":{"id":"iD9PVPwT2_v-","executionInfo":{"status":"ok","timestamp":1668137134607,"user_tz":-330,"elapsed":458,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class Agent3:\n","  def __init__(self,state_size,action_size,lr1,lr2,gamma,lamda):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.lr1 = lr1\n","    self.lr2 = lr2\n","    self.gamma = gamma\n","    self.lamda = lamda\n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.actor = Actor(self.state_size,self.action).to(self.device)\n","    self.critic = Critic(self.state_size,self.action_size).to(self.device)\n","    self.actor_optim = optim.Adam(self.actor.parameters() ,lr = self.lr1)\n","    self.critic_optim = optim.Adam(self.critic.parameters() ,lr = self.lr2)\n","  def action(self,state,message):\n","    action = self.actor(state,message)\n","    return action\n","  def q_value(self,state,action,reward):\n","    value   = torch.cat([state,action,reward],0)\n","    q_value = self.critic(value)\n","    return q_value\n","  def ppo_iters(self,reward,done,value,next_value):\n","    gae = 0\n","    returns = []\n","    for i in range(5):\n","      delta = reward + next_value * (1-done)*self.gamma - value\n","      gae   = delta+ gae*self.lamda*(1-done)\n","      returns.insert(0,gae+value+next_value)\n","    return returns\n","  def learn(self,state,next_state,action,next_action,reward,done,message):\n","    log_prob = torch.log(action).to(self.device)\n","    next_log_prob = torch.log(next_action).to(self.device)\n","    ratio = next_log_prob/log_prob\n","    value = self.q_value(state,action,reward)\n","    next_value = self.q_value(next_state,next_action,reward)\n","    returns = self.ppo_iters(reward,done,value,next_value)\n","    advantage = torch.tensor(returns).float() - value\n","    s1        = ratio * advantage\n","    s2        = torch.clamp(ratio ,1- 0.2,0.2)* advantage\n","    actor_loss = torch.min(s1,s2).mean()\n","    critic_loss = advantage - value\n","    critic_loss = critic_loss*critic_loss\n","    loss        = actor_loss + 0.5*critic_loss \n","    self.actor_optim.zero_grad()\n","    self.critic_optim.zero_grad()\n","    loss.backward()\n","    self.actor_optim.step()\n","    self.critic_optim.step()\n"],"metadata":{"id":"v7Xk1gjf3BRn","executionInfo":{"status":"ok","timestamp":1668137134612,"user_tz":-330,"elapsed":20,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class Agent:\n","  def __init__(self,state_size,action_size,lr1,lr2,gamma,lamda):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.lr1 = lr1\n","    self.lr2 = lr2\n","    self.gamma = gamma\n","    self.lamda = lamda\n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.actor = Actor_good(self.state_size,self.action).to(self.device)\n","    self.critic = Critic(self.state_size,self.action_size).to(self.device)\n","    self.actor_optim = optim.Adam(self.actor.parameters() ,lr = self.lr1)\n","    self.critic_optim = optim.Adam(self.critic.parameters() ,lr = self.lr2)\n","  def action(self,state):\n","    action = self.actor(state)\n","    return action\n","  def q_value(self,state,action,reward):\n","    value   = torch.cat([state,action,reward],0)\n","    q_value = self.critic(value)\n","    return q_value\n","  def ppo_iters(self,reward,done,value,next_value):\n","    gae = 0\n","    returns = []\n","    for i in range(5):\n","      delta = reward + next_value * (1-done)*self.gamma - value\n","      gae   = delta+ gae*self.lamda*(1-done)\n","      returns.insert(0,gae+value+next_value)\n","    return returns\n","  def learn(self,state,next_state,action,next_action,reward,done,message):\n","    log_prob = torch.log(action).to(self.device)\n","    next_log_prob = torch.log(next_action).to(self.device)\n","    ratio = next_log_prob/log_prob\n","    value = self.q_value(state,action,reward)\n","    next_value = self.q_value(next_state,next_action,reward)\n","    returns = self.ppo_iters(reward,done,value,next_value)\n","    advantage = torch.tensor(returns).float() - value\n","    s1        = ratio * advantage\n","    s2        = torch.clamp(ratio ,1- 0.2,0.2)* advantage\n","    actor_loss = torch.min(s1,s2).mean()\n","    critic_loss = advantage - value\n","    critic_loss = critic_loss*critic_loss\n","    loss        = actor_loss + 0.5*critic_loss \n","    self.actor_optim.zero_grad()\n","    self.critic_optim.zero_grad()\n","    loss.backward()\n","    self.actor_optim.step()\n","    self.critic_optim.step()\n"],"metadata":{"id":"9nnbTtK-dmOR","executionInfo":{"status":"ok","timestamp":1668137134613,"user_tz":-330,"elapsed":19,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class Main:\n","  def __init__(self,state_sizes,actions_sizes,lr1s,lr2s,gamma,lamda):\n","    self.state_sizes = state_sizes\n","    self.action_sizes= actions_sizes\n","    self.lr1s        = lr1s\n","    self.lr2s        = lr2s\n","    self.gamma       = gamma\n","    self.lamda       = lamda\n","    self.device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.agent1 = Agent1(self.state_size[0],self.action_size[0],self.lr1[0],self.lr2[0],self.gamma,self.lamda)\n","    self.agent2 = Agent2(self.state_size[1],self.action_size[1],self.lr1[1],self.lr2[1],self.gamma,self.lamda)\n","    self.agent3 = Agent3(self.state_size[2],self.action_size[2],self.lr1[2],self.lr2[2],self.gamma,self.lamda)\n","    self.agent4 = Agent(self.state_size[2],self.action_size[2],self.lr1[2],self.lr2[2],self.gamma,self.lamda)\n","    self.msg_net= Msg_net().to(self.device())\n","    self.msg_optim = optim.Adam(self.msg_net.parameters() ,lr = self.lrs[0])\n","  def choose_actions(self,states,message):\n","    action1         = self.agent1(states[0],message)\n","    action2         = self.agent2(states[1],message)\n","    action3         = self.agent3(states[2],message)\n","    action4        = self.agent4(states[4])\n","    actions   = [action1,action2,action3,action4]\n","    return  actions\n","  def learn(self,states,next_state,actions,next_actions,rewards,dones,message):\n","    state1 = torch.tensor(states[0]).float().to(self.device)\n","    state2 = torch.tensor(states[1]).float().to(self.device)\n","    state3 = torch.tensor(states[2]).float().to(self.device)\n","    state4 = torch.tensor(states[3]).float().to(self.device)\n","    self.agent1.learn(actions[0],next_actions[0],rewards[0],dones[0],message)\n","    self.agent2.learn(actions[1],next_actions[1],rewards[1],dones[1],message)\n","    self.agent3.learn(actions[2],next_actions[2],rewards[2],dones[2],message)\n","    self.agent4.learn(actions[3],next_actions[3],rewards[3],dones[3],message)\n","  def messages(self,state,actions,values,messages):\n","    msg_list = self.msg_net(state,actions,values,messages)\n","    msg = msg_list[0]\n","    agent = torch.max(msg_list[1])\n","    if agent==0:\n","      msgs = [msg,torch.tensor([1]),torch.tensor(1)]\n","    elif agent==1:\n","       msgs = [torch.tensor([1]),msg,torch.tensor(1)]\n","    elif agent==2:\n","      msgs = [torch.tensor([1]),torch.tensor(1),msg]\n","    else:\n","      msgs = [torch.tensor([1]),torch.tensor([1]),torch.tensor(1)]\n","    return msgs\n","  def messages_learn(self,state,values,action,messages):\n","    msgn = self.msg_net(state,values,action,messages)\n","    idx  = msgn[1]\n","    \n","  def play(self,n_games,steps):\n","    for i in range(n_games,steps):\n","      state = env.reset()\n","      done  = [False * 4]\n","      score = [0,0,0,0]\n","      for step in range(steps):\n","        if i==0 and step ==0:\n","          messages= [torch.tensor([1]),torch.tensor([1]),torch.tensor([1]),torch.tensor([1])]\n","        else:\n","\n","        actions = self.choose_actions(state,messages)\n","        next_state,reward,done,info = env.step(actions)\n","        next_action  = self.choose_actions(next_state,)\n","      "],"metadata":{"id":"hFfP07gh3CiC","colab":{"base_uri":"https://localhost:8080/","height":131},"executionInfo":{"status":"error","timestamp":1668137134614,"user_tz":-330,"elapsed":19,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"147ff4fc-4822-4dff-a2c6-ba87b461b47d"},"execution_count":13,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-7f027c60b41d>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    def play(self,n_games,steps):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4h6TL1eLuAqv"},"execution_count":null,"outputs":[]}]}