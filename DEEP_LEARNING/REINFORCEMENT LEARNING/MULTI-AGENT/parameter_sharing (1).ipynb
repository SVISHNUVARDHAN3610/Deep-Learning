{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPysYDd5KB8glUowvPBASv4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!git clone https://github.com/openai/multiagent-particle-envs.git\n","!pip install gym==0.10.5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOXw6INqrMj3","executionInfo":{"status":"ok","timestamp":1663560685290,"user_tz":-330,"elapsed":4094,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"1a0fb73d-d09a-403b-8a02-544b1e3c4c3f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'multiagent-particle-envs' already exists and is not an empty directory.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym==0.10.5 in /usr/local/lib/python3.7/dist-packages (0.10.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.15.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.21.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (2.23.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.5.26)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2022.6.15)\n"]}]},{"cell_type":"code","source":["cd multiagent-particle-envs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OOSvM9LnrQvn","executionInfo":{"status":"ok","timestamp":1663560685292,"user_tz":-330,"elapsed":16,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"a214d7da-909e-4d71-a85c-16b1d97cd163"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/multiagent-particle-envs\n"]}]},{"cell_type":"code","source":["pip install -e."],"metadata":{"id":"rARMYtEIrUtE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663560690571,"user_tz":-330,"elapsed":5288,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"c33f1669-7a1c-4e64-8401-685e9712df89"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/multiagent-particle-envs\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from multiagent==0.0.1) (0.10.5)\n","Requirement already satisfied: numpy-stl in /usr/local/lib/python3.7/dist-packages (from multiagent==0.0.1) (2.17.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.15.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.21.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (2.23.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.5.26)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (1.24.3)\n","Requirement already satisfied: python-utils>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from numpy-stl->multiagent==0.0.1) (3.3.3)\n","Installing collected packages: multiagent\n","  Attempting uninstall: multiagent\n","    Found existing installation: multiagent 0.0.1\n","    Can't uninstall 'multiagent'. No files were found to uninstall.\n","  Running setup.py develop for multiagent\n","Successfully installed multiagent-0.0.1\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"9q7l-RswRwgq","executionInfo":{"status":"ok","timestamp":1663560690573,"user_tz":-330,"elapsed":54,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","from torch.autograd import Variable as v\n","from make_env import make_env\n","env = make_env(\"simple_reference\")"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","from torch.autograd import Variable as v\n","\n","class Actor(nn.Module):\n","  def __init__(self,state_size,action_size,message_size):\n","    super(Actor,self).__init__()\n","    self.fc3 = nn.Linear(64,128)\n","    self.fc4 = nn.Linear(128,256)\n","    self.fc5 = nn.Linear(256,512)\n","    self.fc6 = nn.Linear(512,512)\n","    self.fc7 = nn.Linear(512,256)\n","    self.fc8 = nn.Linear(256,128)\n","    self.fc9 = nn.Linear(128,64)\n","    self.fc10= nn.Linear(64,32)\n","    self.fc11= nn.Linear(32,5)\n","  def forward(self,state,message):\n","    message  = torch.tensor([message.item(),0],dtype = torch.float32)\n","    self.fc1 = nn.Linear(state.shape[0],32)\n","    self.fm1 = nn.Linear(message.shape[0],32)\n","    #state    = torch.tensor(state,dtype = torch.float64)\n","    state    =  torch.tensor(state).float()\n","    s        = self.fc1(state)\n","    m        = self.fm1(message)\n","    cat      = torch.cat([s,m],0)\n","    cat      = torch.reshape(cat,(-1,))\n","    self.fc2 = nn.Linear(cat.shape[0],64)\n","    x        = f.relu(self.fc8(f.relu(self.fc7(f.relu(self.fc6(f.relu(self.fc5(f.relu(self.fc4(f.relu(self.fc3(f.relu(self.fc2(cat))))))))))))))\n","    x        = f.relu(self.fc10(f.relu(self.fc9(x))))\n","    x        = f.softmax(self.fc11(x))\n","    return x"],"metadata":{"id":"xXddrePXR71c","executionInfo":{"status":"ok","timestamp":1663560690574,"user_tz":-330,"elapsed":53,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","from torch.autograd import Variable as v\n","\n","class Critic(nn.Module):\n","  def __init__(self,state_size,action_size,message_size):\n","    super(Critic,self).__init__()\n","    self.fc3 = nn.Linear(64,128)\n","    self.fc4 = nn.Linear(128,256)\n","    self.fc5 = nn.Linear(256,512)\n","    self.fc6 = nn.Linear(512,512)\n","    self.fc7 = nn.Linear(512,256)\n","    self.fc8 = nn.Linear(256,128)\n","    self.fc9 = nn.Linear(128,64)\n","    self.fc10= nn.Linear(64,32)\n","    self.fc11= nn.Linear(32,1)\n","  def forward(self,state,action,reward):\n","    reward   = torch.tensor([reward.item(),0],dtype = torch.float64)\n","    self.fc1 = nn.Linear(state.shape[0],32)\n","    self.fa1 = nn.Linear(action.shape[0],32)\n","    self.fr1 = nn.Linear(reward.shape[0],32)\n","    #state    = torch.tensor(state[0],dtype = torch.float64)\n","    state    = torch.tensor(state).float()\n","    reward   = torch.tensor(reward).float()\n","    s        = self.fc1(state)\n","    a        = self.fa1(action)\n","    r        = self.fr1(reward)\n","    cat      = torch.cat([s,a,r],0)\n","    cat      = torch.reshape(cat,(-1,))\n","    self.fc2 = nn.Linear(cat.shape[0],64)\n","    x        = f.relu(self.fc8(f.relu(self.fc7(f.relu(self.fc6(f.relu(self.fc5(f.relu(self.fc4(f.relu(self.fc3(f.relu(self.fc2(cat))))))))))))))\n","    x        = f.relu(self.fc10(f.relu(self.fc9(x))))\n","    x        = self.fc11(x)\n","    return x"],"metadata":{"id":"Dd1bZJyeu8Nl","executionInfo":{"status":"ok","timestamp":1663560690575,"user_tz":-330,"elapsed":53,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","from torch.autograd import Variable as v\n","\n","class Message(nn.Module):\n","  def __init__(self,state_size,action_size,message_size):\n","    super(Message,self).__init__()\n","    self.state_size  = state_size\n","    self.action_size = action_size\n","    self.q           = nn.Linear(5,128)\n","  def para(self,parameters):\n","    count  = len(parameters)\n","    vector = nn.Embedding(1024,128)\n","    vectors = []\n","    for i in range(count):\n","      out  = vector(parameters[i].long())\n","      vectors.append(torch.reshape(out,(-1,)))\n","    vectors = torch.cat(vectors,0)\n","    return vectors\n","  def message(self,action,value,vectors,message):\n","    message= torch.tensor(message).float()\n","    q = nn.Linear(5,128)\n","    k = nn.Linear(value.shape[0],128)\n","    v = nn.Linear(vectors.shape[1],128)\n","    m = nn.Linear(message.shape[0],128)\n","    wq = self.q(action)\n","    wk = k(value)\n","    wv = v(vectors)\n","    wm = m(message)\n","    mat1 = torch.matmul(wq,wk)\n","    mat2 = torch.matmul(wv,wm)\n","    mat1 = torch.tensor([mat1])\n","    mat2 = torch.reshape(mat2,(self.sha,1))\n","    msg  = torch.matmul(mat2,mat1)\n","    return msg\n","  def forward(self,action,value,message,para):\n","    para = self.para(para)\n","    self.sha  =int(para.shape[0]/32)\n","    para = torch.reshape(para , (self.sha,32))\n","    msg  = self.message(action,value,para,message)\n","    msg  = f.softmax(msg)\n","    msg  = msg.mean()\n","    return msg"],"metadata":{"id":"d984JlYATbNv","executionInfo":{"status":"ok","timestamp":1663560690576,"user_tz":-330,"elapsed":53,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["agent1_msg = []\n","agent2_msg = []\n","agent1_val = []\n","agent2_val = []\n","count      = []"],"metadata":{"id":"9XHj9lAD323C","executionInfo":{"status":"ok","timestamp":1663560690576,"user_tz":-330,"elapsed":52,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","from torch.autograd import Variable as v\n","import sys\n","sys.path.append('./')\n","from network.network import Actor,Critic,Message\n","\n","class Agent1:\n","  def __init__(self,state_size,action_size,reward_size,message_size,buffer):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.reward_size = reward_size\n","    self.buffer = buffer\n","    self.message_size = message_size \n","    self.gamma = 0.99\n","    self.lamda= 0.95\n","    self.lr1 = 0.000005\n","    self.lr2 = 0.00007\n","    self.device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.actor = Actor(self.state_size,self.action_size,self.message_size).to(self.device)\n","    self.critic= Critic(self.state_size,self.action_size,self.message_size).to(self.device)\n","    self.message = Message(self.state_size,self.action_size,self.reward_size).to(self.device)\n","    self.actor_optim = optim.Adam(self.actor.parameters(),lr = self.lr1)\n","    self.critic_optim = optim.Adam(self.critic.parameters(),lr = self.lr2)\n","    self.message_optim = optim.Adam(self.message.parameters() ,lr = self.lr1)\n","    self.value         = []\n","    self.policy        = []\n","  def message(self,action,value,message,para):\n","    #parameters = list(self.actor.parameters())\n","    msg     = self.message(action,value,message,para)\n","    return msg\n","  def choose_action(self,state,message):\n","    state   = state\n","    act     = self.actor(state,message).to(self.device)\n","    return act\n","  def values(self,state,action,reward,noise):\n","    state   = state + noise \n","    value   = self.critic(state,action,reward).to(self.device)\n","    return value\n","  def returns(self,reward,done,value,next_value):\n","    returns = []\n","    gae     = 0\n","    for i in range(5):\n","      delta = reward + self.gamma * (1-done) * next_value  - value\n","      gae   = delta + gae*self.lamda * (1-done)\n","      returns.insert(0,gae + value)\n","    return returns\n","  def msg_loss(self,action,p_action,value,reward,done,msg):\n","    l2    = torch.log(p_action) - torch.log(action)\n","    l2    = l2*value\n","    loss  = reward + l2*(1-done)*msg*value\n","    loss  = self.lamda*loss.mean()\n","    return loss\n","  def learn(self,state,next_state,reward,done,msg,next_value,noise):\n","    state        = torch.tensor(state).float().to(self.device)\n","    reward       = torch.tensor(reward,dtype = torch.float32).to(self.device)\n","    done         = torch.tensor(done ,dtype = torch.float32).to(self.device)\n","    action       = self.choose_action(state,msg)\n","    value        = self.values(state,action,reward,noise)\n","    para         = list(self.actor.parameters())\n","    msg          = self.message(action,value,msg,para)\n","    #agent1_msg.append(torch.tensor([msg]).float())\n","    self.policy.append(action)\n","    self.value.append(value)\n","    self.buffer.agent1_val.append(torch.tensor(value).float())\n","    self.buffer.agent1_mg.append(torch.tensor([msg]).float())\n","    advantage = self.returns(reward,done,value,next_value)\n","    advantage = torch.tensor([advantage[0][0],advantage[1][0],advantage[2][0],advantage[3][0],advantage[4][0]]) - value\n","    argmax      = torch.argmax(torch.tensor(self.value))\n","    p_action    = self.policy[argmax.item()]\n","    actor_loss  = p_action - action\n","    actor_loss  = actor_loss.mean()\n","    critic_loss = (advantage - value)**2\n","    critic_loss = critic_loss.mean()\n","    loss        = actor_loss + critic_loss*0.5\n","    loss        = v(loss,requires_grad = True)\n","    self.actor_optim.zero_grad()\n","    self.critic_optim.zero_grad()\n","    loss.backward()\n","    self.actor_optim.step()\n","    self.critic_optim.step()\n","    #message\n","    msg_loss   = self.msg_loss(action,p_action,value,reward,done,msg)\n","    self.buffer.appending_ag1(reward,value,msg,loss,actor_loss,critic_loss,msg_loss,advantage,next_value)\n","    msg_loss   = v(msg_loss.mean() ,requires_grad = True)\n","    self.message_optim.zero_grad()\n","    msg_loss.backward()\n","    self.message_optim.step()"],"metadata":{"id":"tbTR_FAOvBoH","executionInfo":{"status":"ok","timestamp":1663561817322,"user_tz":-330,"elapsed":858,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","from torch.autograd import Variable as v\n","import sys\n","sys.path.append('./')\n","from network.network import Actor,Critic,Message\n","\n","class Agent2:\n","  def __init__(self,state_size,action_size,reward_size,message_size,buffer):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.buffer = buffer\n","    self.reward_size = reward_size\n","    self.message_size = message_size \n","    self.gamma = 0.99\n","    self.lamda= 0.95\n","    self.lr1 = 0.0000009\n","    self.lr2 = 0.000003\n","    self.device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.actor = Actor(self.state_size,self.action_size,self.message_size).to(self.device)\n","    self.critic= Critic(self.state_size,self.action_size,self.message_size).to(self.device)\n","    self.message = Message(self.state_size,self.action_size,self.reward_size).to(self.device)\n","    self.actor_optim = optim.Adam(self.actor.parameters(),lr = self.lr1)\n","    self.critic_optim = optim.Adam(self.critic.parameters(),lr = self.lr2)\n","    self.message_optim = optim.Adam(self.message.parameters() ,lr = self.lr1)\n","    self.value         = []\n","    self.policy        = []\n","  def message(self,action,value,message,para):\n","    #parameters = list(self.actor.parameters())\n","    msg = self.message(action,value,message,para)\n","    return msg\n","  def choose_action(self,state,message):\n","    state = state\n","    act   = self.actor(state,message).to(self.device)\n","    return act\n","  def values(self,state,action,reward,noise):\n","    state = state + noise \n","    value = self.critic(state,action,reward).to(self.device)\n","    return value\n","  def returns(self,reward,done,value,next_value):\n","    returns = []\n","    gae     = 0\n","    for i in range(5):\n","      delta = reward + self.gamma * (1-done) * next_value  - value\n","      gae   = delta + gae*self.lamda * (1-done)\n","      returns.insert(0,gae + value)\n","    return returns\n","  def msg_loss(self,action,p_action,value,reward,done,msg):\n","    l2 = torch.log(p_action) - torch.log(action)\n","    l2 = l2*value\n","    loss = reward + l2*(1-done)*msg*value\n","    loss = self.lamda*loss.mean()\n","    return loss\n","\n","  def learn(self,state,next_state,reward,done,msg,next_value,noise):\n","    #converting to tensors\n","    state     = torch.tensor(state).float().to(self.device)\n","    reward    = torch.tensor(reward,dtype = torch.float32).to(self.device)\n","    done      = torch.tensor(done ,dtype = torch.float32).to(self.device)\n","    action    = self.choose_action(state,msg)\n","    value     = self.values(state,action,reward,noise)\n","    para      = list(self.actor.parameters())\n","    msg       = self.message(action,value,msg,para)\n","    self.policy.append(action)\n","    self.value.append(value)\n","    self.buffer.agent2_val.append(torch.tensor(value).float())\n","    self.buffer.agent2_mg.append(torch.tensor([msg]).float())\n","    returns = self.returns(reward,done,value,next_value)\n","    returns = torch.tensor([returns[0][0],returns[1][0],returns[2][0],returns[3][0],returns[4][0]]) \n","    advantage = returns-value\n","    argmax    = torch.argmax(torch.tensor(self.value))\n","    p_action  = self.policy[argmax.item()]\n","    actor_loss= p_action - action\n","    actor_loss= actor_loss.mean()\n","    critic_loss= (returns - value)**2\n","    critic_loss = critic_loss.mean()\n","    loss = actor_loss + critic_loss*0.5\n","    loss = v(loss,requires_grad = True)\n","    self.actor_optim.zero_grad()\n","    self.critic_optim.zero_grad()\n","    loss.backward()\n","    self.actor_optim.step()\n","    self.critic_optim.step()\n","    #message\n","    msg_loss = self.msg_loss(action,p_action,value,reward,done,msg)\n","    msg_loss = v(msg_loss.mean() ,requires_grad =True)\n","    self.buffer.appending_ag2(reward,value,msg,loss,actor_loss,critic_loss,msg_loss,returns,next_value)\n","    self.message_optim.zero_grad()\n","    msg_loss.backward()\n","    self.message_optim.step()"],"metadata":{"id":"zgCoT4fMLulh","executionInfo":{"status":"ok","timestamp":1663561818241,"user_tz":-330,"elapsed":5,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","from torch.autograd import Variable as v\n","from make_env import make_env\n","env = make_env(\"simple_reference\")\n","import sys\n","sys.path.append('./')\n","from network.network import Critic\n","from Agents.Agent1 import Agent1\n","from Agents.Agent2 import Agent2\n","class Centeral:\n","  def __init__(self,state_size,action_size,reward_size,message_size,n_games,step,buffer):\n","    self.state_size = state_size\n","    self.action_size  = action_size\n","    self.reward_size = reward_size\n","    self.message_size = message_size\n","    self.n_games = n_games\n","    self.step = step\n","    self.buffer = buffer\n","    self.device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.agent1       = Agent1(self.state_size,self.action_size,self.reward_size,self.message_size,self.buffer)\n","    self.agent2       = Agent2(self.state_size,self.action_size,self.reward_size,self.message_size,self.buffer)\n","    self.critic       = Critic(self.state_size,self.action_size,self.message_size).to(self.device)\n","    self.optim        = optim.Adam(self.critic.parameters() ,lr = 0.000009)\n","  def noise(self,state):\n","    obs1,obs2 = state[0],state[1]\n","    n1    = torch.rand(state[0].size)+ state[1] \n","    n1    = f.softmax(n1)\n","    n2    = torch.rand(state[1].size)+ state[0] \n","    n2    = f.softmax(n2)\n","    noise = []\n","    noise.append(n1)\n","    noise.append(n2)\n","    return noise\n","  def choose_action(self,state,noise,msg):\n","    actions = []\n","    obs1 = torch.from_numpy(state[0]).float().to(self.device)\n","    obs2 = torch.from_numpy(state[1]).float().to(self.device)\n","    obs1 = obs1+ noise[0]\n","    obs2  = obs2+ noise[1]\n","    obs1 = torch.tensor(obs1,dtype = torch.float32).to(self.device)\n","    obs2 = torch.tensor(obs2,dtype = torch.float32).to(self.device)\n","    m1 = torch.tensor(msg[0].item(),dtype = torch.long).to(self.device)\n","    m2 = torch.tensor(msg[1].item(),dtype = torch.long).to(self.device)\n","    act1 = self.agent1.choose_action(obs1,m1).to(self.device)\n","    act2 = self.agent2.choose_action(obs2,m2).to(self.device)\n","    actions.append(act1.detach().numpy())\n","    actions.append(act2.detach().numpy())\n","    #actions.append(act3)\n","    return actions\n","  def next_value(self,state,action,reward,noise):\n","    values = []\n","    action[0],action[1] = torch.tensor(action[0],dtype = torch.float32).to(self.device),torch.tensor(action[1],dtype = torch.float32).to(self.device)\n","    reward[0],reward[1] = torch.tensor(reward[0],dtype = torch.float32).to(self.device),torch.tensor(reward[1],dtype = torch.float32).to(self.device)\n","    state[0],state[1] = torch.from_numpy(state[0]).float().to(self.device),torch.from_numpy(state[1]).float().to(self.device)\n","    state[0],state[1] = state[0]+noise[0],state[1]+noise[1]\n","    state[0] = torch.DoubleTensor(state[0])\n","    va1  = self.critic(state[0],action[0],reward[0])\n","    va2  = self.critic(state[1],action[1],reward[1])\n","    values.append(va1)\n","    values.append(va2)\n","    return values\n","  def returns(self,reward,values,idx):\n","    gae = 0\n","    delta = sum(reward)*0.99*sum(values) + self.buffer.agent1_val[idx] +self.buffer.agent2_val[idx]\n","    gae = delta + 0.95*gae+sum(reward)\n","    return gae\n","  def update(self,state,next_state,done,reward,noise,msg,idx):\n","    actions = self.choose_action(state,noise,msg)\n","    values  = self.next_value(state,actions,reward,noise)\n","    self.agent1.learn(state[0],next_state[0],reward[0],done[0],msg[0],values[0],noise[0])\n","    self.agent2.learn(state[1],next_state[1],reward[0],done[1],msg[1],values[1],noise[1])\n","    loss = self.returns(reward,values,idx)\n","    loss = (loss - sum(values))**2\n","    loss = loss.mean()\n","    loss = v(loss,requires_grad = True)\n","    self.buffer.loss.append(loss)\n","    self.optim.zero_grad()\n","    loss.backward()\n","    self.optim.step()\n","  def clear(self):\n","    agent1_val = []\n","    agent2_val = []\n","  def message(self,episode,step,idx):\n","    if episode==0&step ==0:\n","      msg1 = torch.tensor([0])\n","      msg2 = torch.tensor([0])\n","    else:\n","      msg1 = self.buffer.agent1_mg[idx]\n","      msg2 = self.buffer.agent2_mg[idx]\n","    return msg1,msg2\n","  def run(self):\n","    sum =0\n","    #count.append(sum)\n","    for i in range(self.n_games):\n","      state = env.reset()\n","      score = [0,0]\n","      done = [False *2 ]\n","      print(\"episode\",i)\n","      for  e in range(self.step):\n","        msg = self.message(i,e,sum)\n","        noise = self.noise(state)\n","        #agent1_msg.append(msg[0])\n","        #agent2_msg.append(msg[1])\n","        actions = self.choose_action(state,noise,msg)\n","        next_state,reward,done,info = env.step(actions)\n","        if done:\n","          self.update(state,next_state,done,reward,noise,msg,sum)\n","          self.buffer.count.append(sum)\n","          Ploting(self.buffer)\n","          state = next_state\n","          score += reward\n","          sum = sum+1\n","        else:\n","          print(\"done\")"],"metadata":{"id":"rrsWsp7Vrrne","executionInfo":{"status":"ok","timestamp":1663561818242,"user_tz":-330,"elapsed":4,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["if __name__ ==\"__main__\":\n","  buffer = Buffer()\n","  c = Centeral(21,5,1,1,100,200,buffer)\n","  c.run()"],"metadata":{"id":"w6WQj2ZZtoQH","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6fe5868f-3e2d-4e31-93a2-2719d91963f3","executionInfo":{"status":"error","timestamp":1663562331675,"user_tz":-330,"elapsed":512471,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["episode 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"error","ename":"SystemError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)","\u001b[0;31mSystemError\u001b[0m: <built-in method write of _io.BufferedWriter object at 0x7fe4354d0950> returned a result with an error set","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)","\u001b[0;31mSystemError\u001b[0m: <built-in method write of _io.BufferedWriter object at 0x7fe4354d0950> returned a result with an error set","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)","\u001b[0;31mSystemError\u001b[0m: <built-in method write of _io.BufferedWriter object at 0x7fe4354d0950> returned a result with an error set","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)","\u001b[0;31mSystemError\u001b[0m: <built-in method write of _io.BufferedWriter object at 0x7fe4354d0950> returned a result with an error set","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)","\u001b[0;31mSystemError\u001b[0m: <built-in method write of _io.BufferedWriter object at 0x7fe4354d0950> returned a result with an error set","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-1c197c7dd36b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCenteral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-32-8ed36370f48a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m           \u001b[0mPloting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m           \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m           \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-49b46f7be2fc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, buffer)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent1_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent2_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent2_networks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent2_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-49b46f7be2fc>\u001b[0m in \u001b[0;36magent2_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"agent2_loss.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0magent2_networks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2126\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2127\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m                 _png.write_png(renderer._renderer, fh, self.figure.dpi,\n\u001b[0;32m--> 537\u001b[0;31m                                metadata={**default_metadata, **metadata})\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSystemError\u001b[0m: <built-in method write of _io.BufferedWriter object at 0x7fe4354d0950> returned a result with an error set"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU9fn+8fdD2CHsYd9kF5A1IGqxKlbFqmhtUYu7LdrWqq2/ti6tWrWtbbV2sypfFdQquCCKirtWXIoYFtmXsG8hCYEkkD15fn/MIY0YJEBmziS5X9c1V2bOmcncOTC5c7bPMXdHREQEoF7YAUREJH6oFEREpJxKQUREyqkURESknEpBRETK1Q87wNFo166d9+zZM+wYIiI1yoIFCzLdPamyeTW6FHr27ElKSkrYMUREahQz23Swedp8JCIi5VQKIiJSTqUgIiLlolYKZvaEmaWb2bIK054zs8XBbaOZLQ6m9zSz/ArzHolWLhERObho7mieBvwTeGr/BHe/aP99M3sAyK7w/HXuPiyKeURE5BCiVgruPtfMelY2z8wMmAicFq33FxGRwxfWPoWxwE53X1th2jFmtsjMPjSzsQd7oZlNNrMUM0vJyMiIflIRkTokrFK4BJhe4fEOoLu7Dwd+DjxrZi0qe6G7T3H3ZHdPTkqq9NwLEZFabdonG5i7Jjp/FMe8FMysPvAd4Ln909y90N13BfcXAOuAfrHOJiIS79buzOV3c1by8uJtUfn+YawpnA6scvet+yeYWZKZJQT3ewF9gfUhZBMRiVvuzu0vL6NZo/rcfvaxUXmPaB6SOh34L9DfzLaa2TXBrIv58qYjgJOBJcEhqi8C17l7VrSyiYjURC8s2Mr8DVncOn4AbZs3isp7RPPoo0sOMv3KSqbNBGZGK4uISE2Xta+IP8xZSXKP1nxvZLeovY/OaBYRqQH+MGcluQUl/O6C46hXz6L2PioFEZE499n6XbywYCs/GNuL/h0To/peKgURkThWWFLK7S8vo2vrJtw4rm/U369GX09BRKS2e+iDdaSm72XqVaNo0jAh6u+nNQURkTi1Ki2Hh/+TyvnDOnNq//YxeU+VgohIHCotc341cymJjRtwx7mDYva+KgURkTg09ZMNfLFlD3eeO5A2zRrG7H1VCiIicWZLVh4PvL2G0wa057yhnWP63ioFEZE44u7c+tJSEuoZ954/mMiVBmJHpSAiEkdeWLCVj1Mz+dVZ/encqknM31+lICISJ9JzC7j3tRWM6tmaScf3CCWDSkFEJE7cNXs5BSVl3HfhkKgOZfF1VAoiInHgreVpzFmaxo3j+tI7qXloOVQKIiIhy84v5jcvL+PYTi2YfHKvULOoFEREQvaHOSvJ3FvIHy88jgYJ4f5aVimIiITo47WZzPh8Cz8Y24shXVuFHUelICISltyCYn754hf0ateMn38rPi5Lr1FSRURC8rvXV5KWU8CLPzqRxg2iPwJqVWhNQUQkBP9Znc6Mz7cw+eTejOjeOuw45VQKIiIxlp1fzC0zl9K3fXNuOj36F845HNp8JCISY3e/uoKMvYVMuXxk3Gw22i9qawpm9oSZpZvZsgrT7jKzbWa2OLidXWHerWaWamarzezMaOUSEQnTuyt2MnPhVn58Su+4ONroQNHcfDQNOKuS6Q+6+7DgNgfAzAYCFwODgtf8y8ziqz5FRI7Snrwibp21lAEdE/npafG12Wi/qJWCu88Fsqr49AnADHcvdPcNQCowOlrZRETCcNfs5ezeV8QDE4fSsH587tINI9X1ZrYk2Ly0f5d7F2BLhedsDaZ9hZlNNrMUM0vJyMiIdlYRkWrxxtIdvLx4Oz89rS+DOrcMO85BxboUHgZ6A8OAHcADh/sN3H2Kuye7e3JSUlJ15xMRqXZp2QXcOmspQ7u25Men9g47zteKaSm4+053L3X3MuD/+N8mom1AtwpP7RpMExGp0crKnP/3whcUFpfx4EXDQh/b6FBims7MOlV4eAGw/8ik2cDFZtbIzI4B+gLzY5lNRCQapn66kY9TM7nj3IH0CnFI7KqK2nkKZjYdOAVoZ2ZbgTuBU8xsGODARuBaAHdfbmbPAyuAEuAn7l4arWwiIrGwKi2HP765itOP7cDFo7od+gVxIGql4O6XVDL58a95/u+A30Urj4hILBUUl3LTjMW0aNyAP154HGbhXEntcOmMZhGRKLj/rdWsSstl6pWjaNu8Udhxqiy+93iIiNRAn6Rm8tjHG7hsTA9OHdA+7DiHRaUgIlKN9uQVcfPzX9A7qRm3nX1s2HEOm0pBRKSauDu3zVpK5t5C/nbxcJo0rHmj9agURESqyUsLtzFnaRo/P6Mfg7vE71nLX0elICJSDbZk5XHn7OWM7tmGa0+O77OWv45KQUTkKBWVlHH99EUY8MDEoSTUqxmHn1ZGh6SKiByl+99ezRdb9vCvSSPo1qZp2HGOitYURESOwvurdjJl7nouHdOds4/rdOgXxDmVgojIEUrLLuDm579gQMdEfv3tgWHHqRYqBRGRI1BSWsYNMxZRWFLGP78/Iu6utXyktE9BROQI/P39VOZvyOKB7w2lT/v4H/20qrSmICJymD5dl8k/3l/Ld0Z04cKRXcOOU61UCiIihyFzbyE3zVjMMe2acc+EwWHHqXYqBRGRKiotc3723GL25Bfz0PdH0KxR7dsCr1IQEamif7y/lo/WZnL3eYM4tlOLsONEhUpBRKQK5q7J4G/vRfYjXFRDrqJ2JFQKIiKHsH1PPjfOWES/9once/7gGnMVtSOhUhAR+RpFJWVc/+xCikrK+NelI2jasPbtR6iodv90IiJH6Q9vrGTh5j388/vD6Z1Ue85HOBitKYiIHMSsRVuZ+slGrjqpJ+cM6Rx2nJiIWimY2RNmlm5myypM+7OZrTKzJWY2y8xaBdN7mlm+mS0Obo9EK5eISFUs25bNLTOXcvwxbWrkZTWPVDTXFKYBZx0w7R1gsLsPAdYAt1aYt87dhwW366KYS0Tka2XtK+LapxfQpllDHpo0ggYJdWejStR+UnefC2QdMO1tdy8JHs4Datf54SJS45WUlnHD9EVk7C3kkUtH0q55o7AjxVSY9Xc18EaFx8eY2SIz+9DMxh7sRWY22cxSzCwlIyMj+ilFpE7589ur+Tg1k3snDGZot1Zhx4m5UErBzG4HSoBngkk7gO7uPhz4OfCsmVV6uqC7T3H3ZHdPTkpKik1gEakTXluynUc/jFwwZ2ItPkHt68S8FMzsSuAcYJK7O4C7F7r7ruD+AmAd0C/W2USk7lqVlsMvXljCyB6tueOcQWHHCU1MS8HMzgJ+CZzn7nkVpieZWUJwvxfQF1gfy2wiUndl5xVz7dMLSGxcn4cnjaBh/bqzY/lAUTt5zcymA6cA7cxsK3AnkaONGgHvBKeJzwuONDoZuNvMioEy4Dp3z6r0G4uIVKPSMufG5xaxfU8+MyaPoX2LxmFHClXUSsHdL6lk8uMHee5MYGa0soiIHMyD76zhP6szuPf8wYzs0SbsOKGru+tIIlLnvbJ4G//8IJWLkrsx6fjuYceJCyoFEamTFm7ezS9eXMLoY9pwTy0f+fRwqBREpM7ZtiefyU8toGOLxjxy6cg6vWP5QBolVUTqlL2FJVwz7XMKS0qZMfl42jRrGHakuKJSEJE6o7TMuWnGItam72XqlaPo0z4x7EhxR+tMIlJn/PHNVby7Mp07zx3Iyf00IkJlVAoiUic8//kWpsxdz2VjenD5CT3DjhO3VAoiUut9kprJbbOWMrZvO+48d2DYceKaSkFEarXVablc9/QCeic156FJI6hfh66NcCS0dESk1tqZU8BVU+fTpGECU68aRYvGDcKOFPd09JGI1Er7Cku4etrn7Mkv5vlrT6BzqyZhR6oRVAoiUuuUlJZx/bMLWZWWy2OXJzO4S8uwI9UY2nwkIrWKu3Pn7OV8sDqDuycM4tQB7cOOVKOoFESkVpkydz3PfLaZ677Zm0nH9wg7To2jUhCRWmPWoq384Y1VnDOkE788s3/YcWoklYKI1AofrErnFy8s4cTebXlg4lDq1dOop0dCpSAiNd6CTVn86JkFHNupBVMuT6ZR/YSwI9VYKgURqdFWp+Vy1dTP6dyyCVOvGkXzRjqo8mioFESkxtqSlcflT3xGk4YJPHn1aNo1bxR2pBpPpSAiNVLm3kIuf2I++UWlPHX18XRr0zTsSLWC1rNEpMbJLSjmyqnz2ZGdzzM/OJ7+HXVdhOoS1TUFM3vCzNLNbFmFaW3M7B0zWxt8bR1MNzP7u5mlmtkSMxsRzWwiUjMVFJdy7dMLWLkjl4cnjWRkjzZhR6pVor35aBpw1gHTbgHec/e+wHvBY4DxQN/gNhl4OMrZRKSGKS1zfvbcYj5dt4v7vzdEZytHQVRLwd3nAlkHTJ4APBncfxI4v8L0pzxiHtDKzDpFM5+I1BxlZc4vX1zCG8vS+PW3j+WC4V3DjlQrhbGjuYO77wjupwEdgvtdgC0Vnrc1mPYlZjbZzFLMLCUjIyO6SUUkLrg7v3llGTMXbuVnp/fjB2N7hR2p1gr16CN3d8AP8zVT3D3Z3ZOTknSNVZHazt2557WVPPPZZn50Sm9uGNcn7Ei1WhilsHP/ZqHga3owfRvQrcLzugbTRKSOcnf+/NZqnvhkA1ed1JNfntkfMw1fEU1hlMJs4Irg/hXAKxWmXx4chTQGyK6wmUlE6qB/vJ/Kv/6zju8f3507zhmoQoiBqJ6nYGbTgVOAdma2FbgTuA943syuATYBE4OnzwHOBlKBPOCqaGYTkfg2Ze46/vLOGi4c0ZV7JwxWIcRIlUrBzG4EpgK5wGPAcOAWd3/7617n7pccZNa4Sp7rwE+qkkdEarepn2zg93MiQ2D/6btDNOJpDFV189HV7p4DnAG0Bi4j8he/iEi1mvrJBn776grOHNSBBy8aRoIKIaaqWgr7/1XOBp529+UVpomIVIuKhfDP74+gQYKGZ4u1qi7xBWb2NpFSeMvMEoGy6MUSkbpGhRAfqrqj+RpgGLDe3fPMrA3aESwi1eTxjzdwz2sqhHhQ1SV/ArDa3feY2aXAr4Hs6MUSkbrioQ9Suee1FYwf3FGFEAequvQfBvLMbChwM7AOeCpqqUSk1nN3/vL2av781mrOH9aZf1wyXIUQB6r6L1ASHDI6Afinuz8EaABzETki7s4f3ljF399P5aLkbjwwcRj1VQhxoar7FHLN7FYih6KONbN6QIPoxRKR2qqszLnr1eU89d9NXH5CD+46d5DOQ4gjVa3mi4BCIucrpBEZl+jPUUslIrVSaZlz60tLeeq/m5h8ci9+e54KId5UqRSCIngGaGlm5wAF7q59CiJSZUUlZdz03GKeS9nCDaf14dbxAzR0RRyqUimY2URgPvA9ImMVfWZm341mMBGpPfYVlnDNk5/z6hfbuWX8AH5+hkY7jVdV3adwOzDK3dMBzCwJeBd4MVrBRKR22L2viKumfc6SrXv404VDmDiq26FfJKGpainU218IgV2EfIEeEYl/2/fkc/kT89mclcfDl47kzEEdw44kh1DVUnjTzN4CpgePLyIy1LWISKVS0/dy+eOfkVNQwpNXjeaE3m3DjiRVUKVScPdfmNmFwEnBpCnuPit6sUSkJvtiyx6unDqfhHrGjMljGNylZdiRpIqqfJEdd58JzIxiFhGpBT5em8nkp1No27whT199PD3bNQs7khyGry0FM8sFvLJZRK6L0yIqqUSkRnptyXZ+9txieic156mrR9O+ReOwI8lh+tpScHcNZSEih+TuPDp3Pfe9sYrkHq15/IpRtGyqQQ9qoqheo1lEar/i0jLueGU50+dv5pwhnbj/e0Np3CAh7FhyhFQKInLEcguK+cmzi5i7JoMfn9Kb/3dGfw1bUcOpFETkiGzfk8/V0z5nbfpe7vvOcVw8unvYkaQaxLwUzKw/8FyFSb2AO4BWwA+BjGD6be6ucyFE4tCybdlcPe1z8otKmXbVKMb2TQo7klSTmJeCu68mcmlPzCwB2AbMInJ5zwfd/f5YZxKRqntv5U5+On0RrZo04MUfnUj/jjoepTYJe/PROGCdu2/S4Fgi8c3deeq/m/jtq8sZ1Lklj1+RrENOa6Gwxy+6mP8NnQFwvZktMbMnzKx1ZS8ws8lmlmJmKRkZGZU9RUSqWVFJGbfNWsads5dz2oAOPHftGBVCLWWRq2yG8MZmDYHtwCB332lmHYBMIifL3QN0cverv+57JCcne0pKSvTDitRhGbmF/OjfC0jZtJsfn9Kbm8/oT4KOMKrRzGyBuydXNi/MzUfjgYXuvhNg/1cAM/s/4LWwgolIxLJt2Ux+KoWsvCL+cclwzh3aOexIEmVhbj66hAqbjsysU4V5FwDLYp5IRMrN/mI7333kUwBevO5EFUIdEcqagpk1A74FXFth8p/MbBiRzUcbD5gnIjFSWubc//ZqHv7POkb1bM2/Jo0kKbFR2LEkRkIpBXffB7Q9YNplYWQRkf/J2lfEjTMW8dHaTC4Z3Z3fnjeIhvXDPh5FYinsQ1JFJE4s3rKHH/97AZn7inSGch2mUhCp49ydZz7bzN2vriApsREzrzuR47rqojh1lUpBpA7LLyrl1y8vY+bCrXyzXxJ/vWgYrZs1DDuWhEilIFJHrc/Yy0+eXcSqtBxuHNeXG8b11fkHolIQqYtmLdrK7bOW0bB+PZ64YhSnDmgfdiSJEyoFkTpkX2EJd7yynJkLtzKqZ2v+dvFwOrdqEnYsiSMqBZE6YsX2HK6fvpANmfv46Wl9uHFcX+on6HBT+TKVgkgt5+78e94m7nl9JS2bNOCZa47nxD7two4lcUqlIFKLZecXc8vMJbyxLI2T+yXxl4lDaddcZyfLwakURGqplI1Z3DhjMTtzCrh1/AB+OLaXrp8sh6RSEKllikrK+Ou7a3jkw3V0btWE5687gRHdK708ichXqBREapG1O3O56bnFLN+ew8TkrvzmnIEkNm4QdiypQVQKIrVAWZnz5H83ct8bq2jWqD6PXjaSMwd1DDuW1EAqBZEabkd2Pr98cQkfrc3ktAHt+eOFQzTUtRwxlYJIDeXuPJ+yhXtfW0lJmfP7C47jktHdMNPOZDlyKgWRGmjbnnxumRlZOxjTqw1/unAo3ds2DTuW1AIqBZEaxN15dv5mfv/6Shy45/zBTBrdXYeaSrVRKYjUEFuy8rjlpSV8krqLk/q05b7vDKFbG60dSPVSKYjEueLSMh7/eAN/fXcN9evV074DiSqVgkgcW7h5N7e9tJRVabmcMbADd503SKOaSlSpFETiUE5BMX9+czX//mwTHVs01nkHEjOhlYKZbQRygVKgxN2TzawN8BzQE9gITHT33WFlFIk1d+eNZWncNXs5mXsLufLEntx8Rn+aN9LfbxIbYf9PO9XdMys8vgV4z93vM7Nbgse/CieaSGxtycrjztnLeX9VOoO7tOCxK5IZ0rVV2LGkjgm7FA40ATgluP8k8B9UClLLFRSX8uiH63n4w1TqmfGbcwZyxQk9dAEcCUWYpeDA22bmwKPuPgXo4O47gvlpQIcDX2Rmk4HJAN27d49VVpFq5+68tXwn976+gq278/n2kE7cfvax2pEsoQqzFL7h7tvMrD3wjpmtqjjT3T0oDA6YPgWYApCcnPyV+SI1QWp6Lr99dQUfrc2kf4dEpv9wDCf0bht2LJHwSsHdtwVf081sFjAa2Glmndx9h5l1AtLDyicSDdl5xfzj/bVM+3QjTRsmcNe5A7l0jDYVSfwIpRTMrBlQz91zg/tnAHcDs4ErgPuCr6+EkU+kuhWVlPH0vE38/b215BQUc1FyN35xZn/a6tKYEmfCWlPoAMwKzsisDzzr7m+a2efA82Z2DbAJmBhSPpFq4e68uSyN+95cxaZdeYzt247bzj6WYzu1CDuaSKVCKQV3Xw8MrWT6LmBc7BOJVL9Fm3fzu9dXkrJpN/06NGfaVaM4pX/7sGOJfK14OyRVpMbbkpXHH99cxWtLdpCU2Ij7vnMc3x3ZVfsNpEZQKYhUk505BTz0QSrT528moZ5xw7i+XHtyL5rpbGSpQfS/VeQoZe0r4pEP1/HkpxspLXMmjurGDaf1pWPLxmFHEzlsKgWRI5SdX8xjH63niY83kF9cyvnDu3DTuH66AprUaCoFkcO0r7CEaZ9u5NEP15FTUMK3h3TiZ6f3pU/7xLCjiRw1lYJIFeUWFPP0vE08/tEGdu0r4vRj2/Pzb/VnYGcdXiq1h0pB5BB27yti6qcbmfbJBnIKSvhmvyRuOr0vw7u3DjuaSLVTKYgcRHpuAY9/tIGn520ir6iUMwd14PpT+3Jc15ZhRxOJGpWCyAG27clnyofrmPH5FopLyzh3aGd+fEof+nfUPgOp/VQKIoEV23N47OP1vPrFdgC+M7wrPzqlNz3bNQs5mUjsqBSkTisrcz5cm8FjH63nk9RdNG2YwKTje/DDk3vRRdc1kDpIpSB1UkFxKS8v2sZjH28gNX0vHVs05pbxA7hkVHdaNm0QdjyR0KgUpE7J3FvIM/M28/S8jWTuLWJgpxY8eNFQvn1cZxrW19hEIioFqfXcnYWbd/P0fzcxZ2kaRaVlnDagPT8Yewwn9GpLMIS7iKBSkFosr6iEVxZv5+n/bmLFjhwSG9Xn+8d359IxPejTvnnY8UTikkpBap31GXv597zNvLBgC7kFJQzomMjvLziOCcM6a8RSkUPQJ0RqhYLiUt5ansbzKVv4JHUXDRKM8YM7cfkJPRjZo7U2EYlUkUpBarRl27J5PmULLy/aRk5BCV1bN+Hmb/XjotHdaJ+ooatFDpdKQWqc7LxiXl68jedTtrB8ew4N69dj/OCOTEzuxgm92lKvntYKRI6USkFqhOLSMuauyeDlxdt5a3kaRSVlDOrcgrsnDGLC0C46t0CkmqgUJG65Ows27eblxdt4fckOducV07ppAy4e1Y2Jyd0Y3EUD04lUt5iXgpl1A54COgAOTHH3v5nZXcAPgYzgqbe5+5xY55Pwpabn8vKi7bzyxTa2ZOXTuEE9Tj+2AxcM78LYvkk6yUwkisJYUygBbnb3hWaWCCwws3eCeQ+6+/0hZJKQbczcx+tLdzBn6Q6Wb8+hnsFJfdpx07h+nDm4I811KKlITMT8k+buO4Adwf1cM1sJdIl1Dgnf+oy9zFm6g9eXprFyRw4Aw7q14o5zBnLO0E46ekgkBKH++WVmPYHhwGfAScD1ZnY5kEJkbWJ3Ja+ZDEwG6N69e8yySvVITY8UwZylO1iVlgvAyB6t+fW3j2X8cZ00MqlIyMzdw3ljs+bAh8Dv3P0lM+sAZBLZz3AP0Mndr/6675GcnOwpKSnRDytHrKS0jIWb9/Duyp28u2In6zP3YQbJPVozfnAnxh/XkU4tVQQisWRmC9w9ubJ5oawpmFkDYCbwjLu/BODuOyvM/z/gtTCyydHbW1jCR2syeGflTj5Ylc7uvGIaJBhjerXlypN6cuagjnRooU1DIvEojKOPDHgcWOnuf6kwvVOwvwHgAmBZrLPJkXF31mfuY+6aDD5YncG8dbsoKi2jVdMGnNa/PacP7MDYvu1IbKxzCUTiXRhrCicBlwFLzWxxMO024BIzG0Zk89FG4NoQskkVZecX82lqJnPXZjB3TSbb9uQD0KtdM648qSfjBrRnZI/W1E/Q4aMiNUkYRx99DFQ2DoHOSYhjpWXO0m3ZzF2Twdw1GSzasofSMiexUX1O7NOWH5/am5P7JtGtTdOwo4rIUdDB31Ipd2dt+l7mrd/FvPW7+HTdLvbkFWMGx3VpyY++2Ztv9k9iWLdWNNDagEitoVIQIHIB+4olMH9DFrv2FQHQpVUTxg3owMn92jG2bxJtmjUMOa2IRItKoY4qLXPW7Mxl/oYs5q3fxWcbssiqUAKn9G/PmF5tGNOrrTYJidQhKoU6IjuvmIVbdrNo024WbN7NF1uy2VtYAkRK4FSVgIigUqiVysqcdRl7Wbh5Nws27Wbh5j2kpu8FoJ7BgI4tOH94Z0b2aE1yjzYqAREpp1Ko4crKnE1ZeSzdls2ybdks3ZrNsu3Z5BZE1gJaNW3AiO6tOX9YZ0Z0b83Qbq10nWIROSj9dqhBDlUADRPqMaBTIucO7cywbq0Y2aM1vdo10/WJRaTKVApxKregmDU7c1m5I5fVabmsSsth1Y5ccgu/WgBDurRkcJeW9OuQqGsNiMhRUSmErKS0jI278liVlsPqtEgJrErLYevu/PLnJDaqT/+OiUwY3pnBnVUAIhI9KoUYyS0oZn3GPtZl7I3c0veRmrGXTbv2UVwaGak2oZ5xTLtmDOvWiktGd6d/h0QGdEqkS6sm2gQkIjGhUqhGBcWlbN2dx6ZdeWzOymND5r7yAkjLKSh/XkI9o0fbpvROas63Bnagd1JzBnRMpE/75jRukBDiTyAidZ1K4TCUlTmZ+wrZujufzcEv/k278tiSlcemrH3szCn80vMTG9WnV/vmnNinLb2TmtOnfXN6JzWne5um2vQjInGpzpeCu1NQXEZ2fjF78ovYk1dMem4hadn5pGUXsjOngB3Z+ezMidwvKfvyRYk6tGhEjzbN+EafJHq0bUr3Nk3pHnxt26yhNvuISI1SJ0thxfYcbpixiOz8YrLziikqLav0eU0aJNCpZWM6tGjM8ce0oUPLxnRs0ZiurZvQvU1TurVpqs09IlKr1MlSSGxcn77tm9OqaQNaNGlAy+DWqklDWjZpQPsWjejQojEtGtfXX/oiUqfUyVLo1qYpD186MuwYIiJxR3s7RUSknEpBRETKqRRERKScSkFERMqpFEREpJxKQUREyqkURESknEpBRETKmbsf+llxyswygE1H8S3aAZnVFCca4j0fKGN1UcbqoYxV08PdkyqbUaNL4WiZWYq7J4ed42DiPR8oY3VRxuqhjEdPm49ERKScSkFERMrV9VKYEnaAQ4j3fKCM1UUZq4cyHqU6vU9BRES+rK6vKYiISAUqBRERKVcnS8HMzjKz1WaWama3hJ0HwMy6mdkHZrbCzJab2Y3B9DZm9o6ZrQ2+tg45Z4KZLTKz14LHx5jZZ8GyfM7MGoaZL8jUykg5VYgAAAYXSURBVMxeNLNVZrbSzE6Ip+VoZj8L/o2Xmdl0M2scD8vRzJ4ws3QzW1ZhWqXLzSL+HuRdYmYjQsr35+DfeYmZzTKzVhXm3RrkW21mZ0Y738EyVph3s5m5mbULHsd8GVZFnSsFM0sAHgLGAwOBS8xsYLipACgBbnb3gcAY4CdBrluA99y9L/Be8DhMNwIrKzz+I/Cgu/cBdgPXhJLqy/4GvOnuA4ChRPLGxXI0sy7ADUCyuw8GEoCLiY/lOA0464BpB1tu44G+wW0y8HBI+d4BBrv7EGANcCtA8Nm5GBgUvOZfwWc/jIyYWTfgDGBzhclhLMNDqnOlAIwGUt19vbsXATOACSFnwt13uPvC4H4ukV9kXYhkezJ42pPA+eEkBDPrCnwbeCx4bMBpwIvBU0LNB2BmLYGTgccB3L3I3fcQR8uRyGVwm5hZfaApsIM4WI7uPhfIOmDywZbbBOApj5gHtDKzTrHO5+5vu3tJ8HAe0LVCvhnuXujuG4BUIp/9qDrIMgR4EPglUPHInpgvw6qoi6XQBdhS4fHWYFrcMLOewHDgM6CDu+8IZqUBHUKKBfBXIv+xy4LHbYE9FT6U8bAsjwEygKnBZq7HzKwZcbIc3X0bcD+Rvxh3ANnAAuJvOe53sOUWj5+jq4E3gvtxk8/MJgDb3P2LA2bFTcaK6mIpxDUzaw7MBG5y95yK8zxy/HAoxxCb2TlAursvCOP9D0N9YATwsLsPB/ZxwKaikJdjayJ/IR4DdAaaUcnmhngU5nI7FDO7ncgm2GfCzlKRmTUFbgPuCDtLVdXFUtgGdKvwuGswLXRm1oBIITzj7i8Fk3fuX6UMvqaHFO8k4Dwz20hkk9tpRLbdtwo2g0B8LMutwFZ3/yx4/CKRkoiX5Xg6sMHdM9y9GHiJyLKNt+W438GWW9x8jszsSuAcYJL/78SreMnXm8gfAF8En52uwEIz60j8ZPySulgKnwN9g6M9GhLZGTU75Ez7t88/Dqx0979UmDUbuCK4fwXwSqyzAbj7re7e1d17Ellm77v7JOAD4Lth59vP3dOALWbWP5g0DlhBnCxHIpuNxphZ0+DffH++uFqOFRxsuc0GLg+OoBkDZFfYzBQzZnYWkU2a57l7XoVZs4GLzayRmR1DZGfu/Fjnc/el7t7e3XsGn52twIjg/2lcLMOvcPc6dwPOJnKkwjrg9rDzBJm+QWTVfAmwOLidTWS7/XvAWuBdoE0cZD0FeC2434vIhy0VeAFoFAf5hgEpwbJ8GWgdT8sR+C2wClgGPA00ioflCEwnsp+jmMgvr2sOttwAI3IU3zpgKZGjqcLIl0pku/z+z8wjFZ5/e5BvNTA+rGV4wPyNQLuwlmFVbhrmQkREytXFzUciInIQKgURESmnUhARkXIqBRERKadSEBGRcioFkRgys1MsGGFWJB6pFEREpJxKQaQSZnapmc03s8Vm9qhFriOx18weDK6F8J6ZJQXPHWZm8yqM6b//mgN9zOxdM/vCzBaaWe/g2ze3/13v4ZngzGbM7D6LXE9jiZndH9KPLnWcSkHkAGZ2LHARcJK7DwNKgUlEBq9LcfdBwIfAncFLngJ+5ZEx/ZdWmP4M8JC7DwVOJHKmK0RGwL2JyPU8egEnmVlb4AJgUPB97o3uTylSOZWCyFeNA0YCn5vZ4uBxLyJDhj8XPOffwDeC6ze0cvcPg+lPAiebWSLQxd1nAbh7gf9vbJ757r7V3cuIDM3Qk8gQ2gXA42b2HaDiOD4iMaNSEPkqA55092HBrb+731XJ8450jJjCCvdLgfoeuZbCaCKjup4DvHmE31vkqKgURL7qPeC7ZtYeyq9T3IPI52X/SKbfBz5292xgt5mNDaZfBnzokavnbTWz84Pv0SgYW79SwXU0Wrr7HOBnRC4jKhJz9Q/9FJG6xd1XmNmvgbfNrB6RES9/QuSCPaODeelE9jtAZEjpR4Jf+uuBq4LplwGPmtndwff43te8bSLwipk1JrKm8vNq/rFEqkSjpIpUkZntdffmYecQiSZtPhIRkXJaUxARkXJaUxARkXIqBRERKadSEBGRcioFEREpp1IQEZFy/x9MoXS9Srpw8AAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["class Buffer:\n","  def __init__(self):\n","    self.agent1_reward = []\n","    self.agent1_value  = []\n","    self.agent1_msg    = []\n","    self.agent1_loss   = []\n","    self.agent1_actor  = []\n","    self.agent1_critic = []\n","    self.agent1msg_loss= []\n","    self.agent1_return = []\n","    self.agent1_nextv  = []\n","    self.agent2_reward = []\n","    self.agent2_value  = []\n","    self.agent2_msg    = []\n","    self.agent2_loss   = []\n","    self.agent2_actor  = []\n","    self.agent2_critic = []\n","    self.agent2msg_loss= []\n","    self.agent2_return = []\n","    self.agent2_nextv  = []\n","    self.count         = []\n","    self.loss          = []\n","    self.agent1_val    = []\n","    self.agent2_val    = []\n","    self.agent1_mg    = []\n","    self.agent2_mg    = []\n","  def appending_ag1(self,reward,value,msg,loss,actor,critic,msgl,returns,next_value):\n","    self.agent1_reward.append(torch.tensor(reward).detach().numpy())\n","    self.agent1_value.append(torch.tensor(value).detach().numpy())\n","    self.agent1_msg.append(torch.tensor(msg).detach().numpy())\n","    self.agent1_loss.append(torch.tensor(loss).detach().numpy())\n","    self.agent1_actor.append(torch.tensor(actor).detach().numpy())\n","    self.agent1_critic.append(torch.tensor(critic).detach().numpy())\n","    self.agent1msg_loss.append(torch.tensor(msgl).detach().numpy())\n","    self.agent1_return.append(torch.tensor(returns).detach().numpy())\n","    self.agent1_nextv.append(torch.tensor(next_value).detach().numpy())\n","  def appending_ag2(self,reward,value,msg,loss,actor,critic,msgl,returns,next_value):\n","    self.agent2_reward.append(torch.tensor(reward).detach().numpy())\n","    self.agent2_value.append(torch.tensor(value).detach().numpy())\n","    self.agent2_msg.append(torch.tensor(msg).detach().numpy())\n","    self.agent2_loss.append(torch.tensor(loss).detach().numpy())\n","    self.agent2_actor.append(torch.tensor(actor).detach().numpy())\n","    self.agent2_critic.append(torch.tensor(critic).detach().numpy())\n","    self.agent2msg_loss.append(torch.tensor(msgl).detach().numpy())\n","    self.agent2_return.append(torch.tensor(returns).detach().numpy())\n","    self.agent2_nextv.append(torch.tensor(next_value).detach().numpy())\n"],"metadata":{"id":"wALYDPQ35gvv","executionInfo":{"status":"ok","timestamp":1663561535361,"user_tz":-330,"elapsed":1100,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_3cgpUL24VE3","executionInfo":{"status":"aborted","timestamp":1663560693485,"user_tz":-330,"elapsed":16,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","class Ploting:\n","  def __init__(self,buffer):\n","    self.buffer = buffer\n","    self.agent1_reward()\n","    self.agent1_loss()\n","    self.agent1_networks()\n","    self.agent1_values()\n","    self.agent1_returns()\n","    self.agent2_reward()\n","    self.agent2_loss()\n","    self.agent2_networks()\n","    self.agent2_values()\n","    self.agent2_returns()\n","    self.agent1_msg_loss()\n","    self.agent2_msg_loss()\n","    self.message()\n","  def agent1_reward(self):\n","    plt.plot(self.buffer.count,self.buffer.agent1_reward)\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"reward\")\n","    plt.savefig(\"agent1_reward.png\")\n","    plt.close()\n","  def agent1_loss(self):\n","    plt.plot(self.buffer.count,self.buffer.agent1_loss)\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"loss\")\n","    plt.savefig(\"agent1_loss.png\")\n","    plt.close()\n","  def agent1_networks(self):\n","    plt.plot(self.buffer.count,self.buffer.agent1_actor,label = \"actor\")\n","    plt.plot(self.buffer.count,self.buffer.agent1_critic,label = \"critic\")\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"loss\")\n","    plt.legend()\n","    plt.savefig(\"agent1_network.png\")\n","    plt.close()\n","  def agent1_values(self):\n","    plt.plot(self.buffer.count,self.buffer.agent1_value,label = \"agent1_values\")\n","    plt.plot(self.buffer.count,self.buffer.agent1_nextv,label = \"agent1_next_values\")\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"values\")\n","    plt.legend()\n","    plt.savefig(\"agent1_values.png\")\n","    plt.close()    \n","  def agent1_returns(self):\n","    plt.plot(self.buffer.count,self.buffer.agent1_return,label = \"returns\")\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"returns\")\n","    plt.legend()\n","    plt.savefig(\"agent1_return.png\")\n","    plt.close()\n","  def agent1_msg_loss(self):\n","    plt.plot(self.buffer.count,self.buffer.agent1msg_loss)\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"loss\")\n","    plt.savefig(\"agent1_msg_loss.png\")\n","    plt.close()\n","  def agent2_reward(self):\n","    plt.plot(self.buffer.count,self.buffer.agent2_reward)\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"reward\")\n","    plt.savefig(\"agent2_reward.png\")\n","    plt.close()\n","  def agent2_loss(self):\n","    plt.plot(self.buffer.count,self.buffer.agent2_loss)\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"loss\")\n","    plt.savefig(\"agent2_loss.png\")\n","    plt.close()\n","  def agent2_networks(self):\n","    plt.plot(self.buffer.count,self.buffer.agent2_actor,label = \"actor\")\n","    plt.plot(self.buffer.count,self.buffer.agent2_critic,label = \"critic\")\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"loss\")\n","    plt.legend()\n","    plt.savefig(\"agent2_network.png\")\n","    plt.close()\n","  def agent2_values(self):\n","    plt.plot(self.buffer.count,self.buffer.agent2_value,label = \"agent2_values\")\n","    plt.plot(self.buffer.count,self.buffer.agent2_nextv,label = \"agent2_next_values\")\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"values\")\n","    plt.legend()\n","    plt.savefig(\"agent2_values.png\")\n","    plt.close()    \n","  def agent2_returns(self):\n","    plt.plot(self.buffer.count,self.buffer.agent2_return,label = \"returns\")\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"returns\")\n","    plt.savefig(\"agent2_return.png\")\n","    plt.legend()\n","    plt.close()\n","  def agent2_msg_loss(self):\n","    plt.plot(self.buffer.count,self.buffer.agent2msg_loss)\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"loss\")\n","    plt.savefig(\"agent2_msg_loss.png\")\n","    plt.close()\n","  def message(self):\n","    plt.plot(self.buffer.count,self.buffer.agent1_msg,label =\"agent1_msg\")\n","    plt.plot(self.buffer.count,self.buffer.agent2_msg,label = \"agent2_msg\")\n","    plt.xlabel(\"epochs\")\n","    plt.ylabel(\"msg\")\n","    plt.legend()\n","    plt.savefig(\"agents_msg.png\")\n","    plt.close()"],"metadata":{"id":"HeVOwBUq69-U","executionInfo":{"status":"aborted","timestamp":1663560693487,"user_tz":-330,"elapsed":18,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mq09uhRhp395","executionInfo":{"status":"aborted","timestamp":1663560693488,"user_tz":-330,"elapsed":18,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}}},"execution_count":null,"outputs":[]}]}