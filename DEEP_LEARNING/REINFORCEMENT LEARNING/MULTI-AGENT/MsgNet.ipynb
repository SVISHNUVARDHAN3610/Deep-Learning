{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOdUMXY3YP6LMoB4xY1SZKY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!git clone https://github.com/openai/multiagent-particle-envs.git\n","!pip install gym==0.10.5"],"metadata":{"id":"hrLLB0_gD5kA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664207809545,"user_tz":-330,"elapsed":11785,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"928ea794-9235-4a36-8de4-e7e2ee6e534e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'multiagent-particle-envs'...\n","remote: Enumerating objects: 242, done.\u001b[K\n","remote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Compressing objects: 100% (5/5), done.\u001b[K\n","remote: Total 242 (delta 0), reused 3 (delta 0), pack-reused 237\u001b[K\n","Receiving objects: 100% (242/242), 107.24 KiB | 892.00 KiB/s, done.\n","Resolving deltas: 100% (127/127), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gym==0.10.5\n","  Downloading gym-0.10.5.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.21.6)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.15.0)\n","Collecting pyglet>=1.2.0\n","  Downloading pyglet-1.5.27-py3-none-any.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 41.3 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2022.6.15)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.10.5-py3-none-any.whl size=1581307 sha256=b176d05be43b1156ea7a5bec04b1937e70c98b0f576c6110a4e3431b77dd93c9\n","  Stored in directory: /root/.cache/pip/wheels/7a/2c/df/a05b548a40fae16ca400ecbeda0067e1a296499c1fbd7e0c9a\n","Successfully built gym\n","Installing collected packages: pyglet, gym\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","Successfully installed gym-0.10.5 pyglet-1.5.27\n"]}]},{"cell_type":"code","source":["cd multiagent-particle-envs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QL47MZWdD5WI","executionInfo":{"status":"ok","timestamp":1664207809546,"user_tz":-330,"elapsed":45,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"6e6bbe16-bfae-4284-ef8c-4cfb6065ac11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/multiagent-particle-envs\n"]}]},{"cell_type":"code","source":["!pip install -e."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0lGjw_ZENsD","executionInfo":{"status":"ok","timestamp":1664207814764,"user_tz":-330,"elapsed":5253,"user":{"displayName":"vishnu vardhan","userId":"07969204109325658924"}},"outputId":"5a4548fe-9740-4cc4-a081-9a820a246893"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/multiagent-particle-envs\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from multiagent==0.0.1) (0.10.5)\n","Collecting numpy-stl\n","  Downloading numpy_stl-2.17.1-py3-none-any.whl (18 kB)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.5.27)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.21.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (1.15.0)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym->multiagent==0.0.1) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym->multiagent==0.0.1) (1.24.3)\n","Requirement already satisfied: python-utils>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from numpy-stl->multiagent==0.0.1) (3.3.3)\n","Installing collected packages: numpy-stl, multiagent\n","  Running setup.py develop for multiagent\n","Successfully installed multiagent-0.0.1 numpy-stl-2.17.1\n"]}]},{"cell_type":"code","source":["class Buffer:\n","  def __init__(self):\n","    self.agent1_reward = []\n","    self.agent1_value  = []\n","    self.agent1_loss   = []\n","    self.agent1_actor  = []\n","    self.agent1_critic = []\n","    self.agent1_nextval= []\n","    self.agent1_message= []\n","    self.agent1_returns= []\n","    self.agent2_reward = []\n","    self.agent2_value  = []\n","    self.agent2_loss   = []\n","    self.agent2_actor  = []\n","    self.agent2_critic = []\n","    self.agent2_nextval= []\n","    self.agent2_message= []\n","    self.agent2_returns= []\n","    self.agent3_reward = []\n","    self.agent3_value  = []\n","    self.agent3_loss   = []\n","    self.agent3_actor  = []\n","    self.agent3_critic = []\n","    self.agent3_nextval= []\n","    self.agent3_message= []\n","    self.agent3_returns= []\n","    self.main_loss     = []\n","    self.episodes      = []\n","    self.cout          = []\n","    self.messages_list = [[],[],[]]\n","  def agent1_appendning(self,reward,value,loss,actor,critic,next_value,message,returns):\n","    self.agent1_reward.append(torch.tensor(reward).cpu())\n","    self.agent1_value.append(torch.tensor(value).cpu())\n","    self.agent1_loss.append(torch.tensor(loss).cpu())\n","    self.agent1_actor.append(torch.tensor(actor).cpu())\n","    self.agent1_critic.append(torch.tensor(critic).cpu())\n","    self.agent1_nextval.append(torch.tensor(next_value).cpu())\n","    self.agent1_message.append(torch.tensor(message).cpu())\n","    self.agent1_returns.append(torch.tensor(returns).cpu())\n","  def agent2_appendning(self,reward,value,loss,actor,critic,next_value,message,returns):\n","    self.agent2_reward.append(torch.tensor(reward).cpu())\n","    self.agent2_value.append(torch.tensor(value).cpu())\n","    self.agent2_loss.append(torch.tensor(loss).cpu())\n","    self.agent2_actor.append(torch.tensor(actor).cpu())\n","    self.agent2_critic.append(torch.tensor(critic).cpu())\n","    self.agent2_nextval.append(torch.tensor(next_value).cpu())\n","    self.agent2_message.append(torch.tensor(message).cpu())\n","    self.agent2_returns.append(torch.tensor(returns).cpu())\n","  def agent3_appendning(self,reward,value,loss,actor,critic,next_value,message,returns):\n","    self.agent3_reward.append(torch.tensor(reward).cpu())\n","    self.agent3_value.append(torch.tensor(value).cpu())\n","    self.agent3_loss.append(torch.tensor(loss).cpu())\n","    self.agent3_actor.append(torch.tensor(actor).cpu())\n","    self.agent3_critic.append(torch.tensor(critic).cpu())\n","    self.agent3_nextval.append(torch.tensor(next_value).cpu())\n","    self.agent3_message.append(torch.tensor(message).cpu())\n","    self.agent3_returns.append(torch.tensor(returns).cpu())\n"],"metadata":{"id":"WjpqCvsNCiNl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpDvromgvGv6"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from torch.autograd import variable as v\n","\n","class Actor(nn.Module):\n","  def __init__(self,state_size,action_size,message_size):\n","    super(Actor,self).__init__()\n","    self.state_size   = state_size\n","    self.action_size  = action_size\n","    self.message_size = message_size\n","    self.device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.fc3          = nn.Linear(64,128).to(self.device)\n","    self.fc4          = nn.Linear(128,256).to(self.device)\n","    self.fc7          = nn.Linear(256,128).to(self.device)\n","    self.fc8          = nn.Linear(128,32).to(self.device)\n","    self.fc9          = nn.Linear(32,self.action_size).to(self.device)\n","  def forward(self,state,message):\n","    message           = torch.tensor(message).float().to(self.device)\n","    self.fc1          = nn.Linear(state.shape[0],32).to(self.device)\n","    self.fc2          = nn.Linear(message.shape[0],32).to(self.device)\n","    a1                = self.fc1(state)\n","    m1                = self.fc2(message)\n","    cat               = torch.cat([a1,m1],0)\n","    self.cat          = nn.Linear(cat.shape[0],64).to(self.device)\n","    cat               = self.cat(cat)\n","    x                 = self.fc8(f.relu(self.fc7(f.relu(self.fc4(f.relu(self.fc3(cat)))))))\n","    x                 = self.fc9(x)\n","    x                 = f.softmax(x)\n","    return x"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","\n","class Critic(nn.Module):\n","  def __init__(self,state_size,action_size,reward_size):\n","    super(Critic,self).__init__()\n","    self.device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.fc4 = nn.Linear(64,128).to(self.device)\n","    self.fc5 = nn.Linear(128,128).to(self.device)\n","    self.fc6 = nn.Linear(128,128).to(self.device)\n","    self.fc7 = nn.Linear(128,64).to(self.device)\n","    self.fc8 = nn.Linear(64,1).to(self.device)\n","  def forward(self,state,action,reward):\n","    self.fc1 = nn.Linear(state.shape[0],32).to(self.device)\n","    self.fc2 = nn.Linear(action.shape[0],32).to(self.device)\n","    self.fc3 = nn.Linear(reward.shape[0],32).to(self.device)\n","    s        = self.fc1(state)\n","    a        = self.fc2(action)\n","    r        = self.fc3(reward)\n","    cat      = torch.cat([s,a,r],0)\n","    self.cat = nn.Linear(cat.shape[0],64).to(self.device)\n","    cat      = self.cat(cat)\n","    x        = f.relu(self.fc8(f.relu(self.fc7(f.relu(self.fc6(f.relu(self.fc5(f.relu(self.fc4(cat))))))))))\n","    return x"],"metadata":{"id":"z4-Pjc2BCOaF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Msgnet(nn.Module):\n","  def __init__(self):\n","    super(Msgnet,self).__init__()\n","    self.pa1    = nn.Linear(64,128)\n","    self.pa2    = nn.Linear(128,128)\n","    self.pa3    = nn.Linear(128,128)\n","    self.pa4    = nn.Linear(256,128)\n","    self.pa5    = nn.Linear(256,128)\n","    self.pa6    = nn.Linear(128,128)\n","    self.pa7    = nn.Linear(128,128)\n","    self.pa8    = nn.Linear(32,128)\n","    self.pa9    = nn.Linear(32,128)\n","    self.pa10   = nn.Linear(5,128)\n","    self.value  = nn.Linear(1,4)\n","    self.lstm   = nn.Linear(4,32)\n","    self.l1     = nn.Linear(32,128)\n","    self.l2     = nn.Linear(128,64)\n","    self.l3     = nn.Linear(64,32)\n","    self.l4     = nn.Linear(32,1)\n","  def forward(self,parameter,value,messages):\n","    #prsent\n","    x0          = self.pa1(parameter[0]).reshape((-1,))\n","    x1          = self.pa2(parameter[1]).reshape((-1,))\n","    x2          = self.pa3(parameter[2]).reshape((-1,))\n","    x3          = self.pa4(parameter[3]).reshape((-1,))\n","    x4          = self.pa5(parameter[4]).reshape((-1,))\n","    x5          = self.pa6(parameter[5]).reshape((-1,))\n","    x6          = self.pa7(parameter[6]).reshape((-1,))\n","    x7          = self.pa8(parameter[7]).reshape((-1,))\n","    x8          = self.pa9(parameter[8]).reshape((-1,))\n","    x9          = self.pa10(parameter[9]).reshape((-1,))\n","    k           = torch.cat([x0,x1,x2,x3,x4,x5,x6,x7,x8,x9])\n","    k           = torch.reshape(k,(17728,4))\n","    q           = self.value(value)\n","    mat1        = torch.matmul(k,q)\n","    mat1        = torch.reshape(mat1,(4432,4)).float()\n","    mat2        = torch.matmul(mat1,messages.float())\n","    mat2        = torch.reshape(mat2,(1108,4))\n","    w           = f.relu(self.l4(f.relu(self.l3(f.relu(self.l2(f.relu(self.l1(f.relu(self.lstm(mat2))))))))))\n","    w           = f.softmax(w,dim = 0)\n","    w           = w.mean()\n","    return w"],"metadata":{"id":"14BGXqnfzrGo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Agent1:\n","  def __init__(self,state_size,action_size,reward_size,buffer):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.reward_size = reward_size \n","    self.buffer      = buffer\n","    self.device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.lr1         = 0.00000001\n","    self.lr2         = 0.00000009\n","    self.lr3         = 0.0000007\n","    self.gamma       = 0.99\n","    self.lamda       = 0.95\n","    self.actor       = Actor(self.state_size,self.action_size,self.reward_size).to(self.device)\n","    self.critic      = Critic(self.state_size,self.action_size,self.reward_size).to(self.device)\n","    self.msgnet      = Msgnet().to(self.device)\n","    self.actor_optim = optim.Adam(self.actor.parameters() ,lr = self.lr1)\n","    self.critic_optim= optim.Adam(self.critic.parameters(),lr = self.lr2)\n","    self.msg_optim   = optim.Adam(self.msgnet.parameters(),lr = self.lr3)\n","    self.policy      = []\n","    self.values      = []\n","  def choose_action(self,state,message):\n","    state = torch.tensor(state).float().to(self.device)\n","    act   = self.actor(state,message)\n","    return act\n","  def q_value(self,state,action,reward):\n","    reward = torch.tensor([reward]).float().to(self.device)\n","    state  = torch.tensor(state).float().to(self.device)\n","    value  = self.critic(state,action,reward)\n","    return value\n","  def ppo_iter(self,reward,value,next_value,done):\n","    gae    = 0\n","    returns= []\n","    for i in range(5):\n","      delta = reward + self.gamma*(1-done)*next_value - value\n","      gae   = delta + self.lamda*(1-done)*gae\n","      returns.insert(0,gae + value+next_value)\n","    return returns\n","  def concate(self,messages,i):\n","    m1  = torch.cat([messages[0],self.buffer.agent2_value[i]])\n","    m2  = torch.cat([messages[1],self.buffer.agent3_value[i]])\n","    msg = torch.cat([m1,m2])\n","    return msg\n","  def message(self,value,messages,i):\n","    oth_messages = self.concate(messages,i)\n","    value        = value\n","    parameter    = list(self.actor.parameters())\n","    msg          = self.msgnet(parameter,value,oth_messages)\n","    return msg\n","  def learn(self,state,next_state,reward,done,next_value,i,messages):\n","    action        = self.choose_action(state,self.concate(messages,i))\n","    value         = self.q_value(state,action,reward)\n","    msg           = self.message(value,messages,i)\n","    returns       = torch.tensor(self.ppo_iter(reward,value,next_value,done))\n","    advantage     = returns - value\n","    self.policy.append(action)\n","    self.values.append(value)\n","    idx           = torch.argmax(torch.tensor(self.values))\n","    idx           = idx.item()\n","    prev_policy   = self.policy[idx]\n","    loss          = (prev_policy-action)**2*value*advantage\n","    loss          = torch.tensor(loss).to(self.device)\n","    loss.requires_grad = True\n","    critic_loss   = (returns - value)**2\n","    critic_loss   = torch.tensor(critic_loss.mean()).to(self.device)\n","    critic_loss.requires_grad = True\n","    msg_loss      = loss+0.5*critic_loss\n","    msg_loss      = torch.tensor([msg_loss.mean()])\n","    #msg_loss.requires_grad = True\n","    #==========================================================================================================================================================\n","    self.actor_optim.zero_grad()\n","    loss.backward()\n","    self.actor_optim.step()\n","    self.critic_optim.zero_grad()\n","    critic_loss.backward()\n","    self.critic_optim.step()\n","    self.msg_optim.zero_grad()\n","    msg_loss.backward()\n","    self.msg_optim.step()\n","    #==========================================================================================================================================================\n","    return msg"],"metadata":{"id":"hgFDkew7A4jK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.set_grad_enabled(True)\n","class Agent2:\n","  def __init__(self,state_size,action_size,reward_size,buffer):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.reward_size = reward_size \n","    self.buffer      = buffer\n","    self.device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.lr1         = 0.00000001\n","    self.lr2         = 0.00000009\n","    self.lr3         = 0.0000007\n","    self.gamma       = 0.99\n","    self.lamda       = 0.95\n","    self.actor       = Actor(self.state_size,self.action_size,self.reward_size).to(self.device)\n","    self.critic      = Critic(self.state_size,self.action_size,self.reward_size).to(self.device)\n","    self.msgnet      = Msgnet().to(self.device)\n","    self.actor_optim = optim.Adam(self.actor.parameters() ,lr = self.lr1)\n","    self.critic_optim= optim.Adam(self.critic.parameters(),lr = self.lr2)\n","    self.msg_optim   = optim.Adam(self.msgnet.parameters(),lr = self.lr3)\n","    self.policy      = []\n","    self.values       = []\n","  def choose_action(self,state,message):\n","    state = torch.tensor(state).float().to(self.device)\n","    act   = self.actor(state,message)\n","    return act\n","  def q_value(self,state,action,reward):\n","    reward = torch.tensor([reward]).float().to(self.device)\n","    state  = torch.tensor(state).float().to(self.device)\n","    value  = self.critic(state,action,reward)\n","    return value\n","  def ppo_iter(self,reward,value,next_value,done):\n","    gae    = 0\n","    returns= []\n","    for i in range(5):\n","      delta = reward + self.gamma*(1-done)*next_value - value\n","      gae   = delta + self.lamda*(1-done)*gae\n","      returns.insert(0,gae + value+next_value)\n","    return returns\n","  def concate(self,messages,i):\n","    m1  = torch.tensor([messages[0]])\n","    m2  = torch.tensor([messages[1]])\n","    v1  = torch.tensor(self.buffer.agent3_value[i])\n","    v2  = torch.tensor(self.buffer.agent1_value[i])\n","    m1  = torch.cat([m1,v1])\n","    m2  = torch.cat([m2,v2])\n","    msg = torch.cat([m1,m2])\n","    return msg\n","  def message(self,value,messages,i):\n","    oth_messages = self.concate(messages,i)\n","    value        = value\n","    parameter    = list(self.actor.parameters())\n","    msg          = self.msgnet(parameter,value,oth_messages)\n","    return msg\n","  def learn(self,state,next_state,reward,done,next_value,i,messages):\n","    action        = self.choose_action(state,self.concate(messages,i))\n","    value         = self.q_value(state,action,reward)\n","    msg           = self.message(value,messages,i)\n","    returns       = torch.tensor(self.ppo_iter(reward,value,next_value,done))\n","    advantage     = returns - value\n","    self.policy.append(action)\n","    self.values.append(value)\n","    self.buffer.agent2_value.append([value])\n","    idx           = torch.argmax(torch.tensor(self.values))\n","    idx           = idx.item()\n","    prev_policy   = self.policy[idx]\n","    loss          = (prev_policy-action)**2*value*advantage\n","    loss          = torch.tensor([loss.mean()]).to(self.device)\n","    loss.requires_grad = True\n","    critic_loss   = (returns - value)**2\n","    critic_loss   = torch.tensor(critic_loss.mean()).to(self.device)\n","    critic_loss.requires_grad = True\n","    msg_loss      = loss+0.5*critic_loss\n","    msg_loss      = torch.tensor(msg_loss.mean())\n","    msg_loss.requires_grad = True\n","    msg_loss.retain_grad()\n","    #==========================================================================================================================================================\n","    self.actor_optim.zero_grad()\n","    loss.backward()\n","    self.actor_optim.step()\n","    self.critic_optim.zero_grad()\n","    critic_loss.backward()\n","    self.critic_optim.step()\n","    self.msg_optim.zero_grad()\n","    msg_loss.backward()\n","    self.msg_optim.step()\n","    #==========================================================================================================================================================\n","    return msg"],"metadata":{"id":"EJ1iDv_CNTnq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Agent3:\n","  def __init__(self,state_size,action_size,reward_size,buffer):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.reward_size = reward_size \n","    self.buffer      = buffer\n","    self.device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.lr1         = 0.00000001\n","    self.lr2         = 0.00000009\n","    self.lr3         = 0.0000007\n","    self.gamma       = 0.99\n","    self.lamda       = 0.95\n","    self.actor       = Actor(self.state_size,self.action_size,self.reward_size).to(self.device)\n","    self.critic      = Critic(self.state_size,self.action_size,self.reward_size).to(self.device)\n","    self.msgnet      = Msgnet().to(self.device)\n","    self.actor_optim = optim.Adam(self.actor.parameters() ,lr = self.lr1)\n","    self.critic_optim= optim.Adam(self.critic.parameters(),lr = self.lr2)\n","    self.msg_optim   = optim.Adam(self.msgnet.parameters(),lr = self.lr3)\n","    self.policy      = []\n","    self.values       = []\n","  def choose_action(self,state,message):\n","    state = torch.tensor(state).float().to(self.device)\n","    act   = self.actor(state,message)\n","    return act\n","  def q_value(self,state,action,reward):\n","    reward = torch.tensor([reward]).float().to(self.device)\n","    state  = torch.tensor(state).float().to(self.device)\n","    value  = self.critic(state,action,reward)\n","    return value\n","  def ppo_iter(self,reward,value,next_value,done):\n","    gae    = 0\n","    returns= []\n","    for i in range(5):\n","      delta = reward + self.gamma*(1-done)*next_value - value\n","      gae   = delta + self.lamda*(1-done)*gae\n","      returns.insert(0,gae + value+next_value)\n","    return returns\n","  def concate(self,messages,i):\n","    m1  = torch.tensor([messages[0]])\n","    m2  = torch.tensor([messages[1]])\n","    v1  = torch.tensor(self.buffer.agent1_value[i])\n","    v2  = torch.tensor(self.buffer.agent2_value[i])\n","    m1  = torch.cat([m1,v1])\n","    m2  = torch.cat([m2,v2])\n","    msg = torch.cat([m1,m2])\n","    return msg\n","  def message(self,value,messages,i):\n","    oth_messages = self.concate(messages,i)\n","    value        = value\n","    parameter    = list(self.actor.parameters())\n","    msg          = self.msgnet(parameter,value,oth_messages)\n","    return msg\n","  def learn(self,state,next_state,reward,done,next_value,i,messages):\n","    action        = self.choose_action(state,self.concate(messages,i))\n","    value         = self.q_value(state,action,reward)\n","    msg           = self.message(value,messages,i)\n","    returns       = torch.tensor(self.ppo_iter(reward,value,next_value,done))\n","    advantage     = returns - value\n","    self.policy.append(action)\n","    self.values.append(value)\n","    self.buffer.agent3_value.append([value])\n","    idx           = torch.argmax(torch.tensor(self.values))\n","    idx           = idx.item()\n","    prev_policy   = self.policy[idx]\n","    loss          = (prev_policy-action)**2*value*advantage\n","    loss          = torch.tensor([loss.mean()]).to(self.device)\n","    loss.requires_grad = True\n","    critic_loss   = (returns - value)**2\n","    critic_loss   = torch.tensor(critic_loss.mean()).to(self.device)\n","    critic_loss.requires_grad = True\n","    msg_loss      = loss+0.5*critic_loss\n","    msg_loss      = torch.tensor(msg_loss.mean())\n","    msg_loss.requires_grad = True\n","    msg_loss.retain_grad()\n","    #==========================================================================================================================================================\n","    self.actor_optim.zero_grad()\n","    loss.backward()\n","    self.actor_optim.step()\n","    self.critic_optim.zero_grad()\n","    critic_loss.backward()\n","    self.critic_optim.step()\n","    self.msg_optim.zero_grad()\n","    msg_loss.backward()\n","    self.msg_optim.step()\n","    #==========================================================================================================================================================\n","    return msg"],"metadata":{"id":"WsiizLaf1xuc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.set_grad_enabled(True)\n","class Main:\n","  def __init__(self,state_size,action_size,reward_size,buffer,n_games,steps):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.reward_size = reward_size\n","    self.n_games     = n_games\n","    self.steps       = steps\n","    self.gamma       = 0.99\n","    self.buffer      = buffer\n","    self.device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.critic      = Critic(self.state_size,self.action_size,self.reward_size).to(self.device)\n","    self.optim       = optim.Adam(self.critic.parameters(),lr = 0.000005)\n","    self.agent1      = Agent1(self.state_size,self.action_size,self.reward_size,self.buffer)\n","    self.agent2      = Agent2(self.state_size,self.action_size,self.reward_size,self.buffer)\n","    self.agent3      = Agent3(self.state_size,self.action_size,self.reward_size,self.buffer)\n","  def concate(self,messages,i):\n","    value1  = torch.tensor(self.buffer.agent1_value[i])\n","    value2  = torch.tensor(self.buffer.agent2_value[i])\n","    value3  = torch.tensor(self.buffer.agent3_value[i])\n","    msg1,msg2,msg3    = torch.tensor([messages[0]]),torch.tensor([messages[1]]),torch.tensor([messages[2]])\n","    msg1    = torch.cat([msg1,value1])\n","    msg2    = torch.cat([msg2,value2])\n","    msg3    = torch.cat([msg3,value3])\n","    amsg1 = torch.cat([msg2,msg3])\n","    amsg2 = torch.cat([msg3,msg1])\n","    amsg3 = torch.cat([msg1,msg2])\n","    return amsg1,amsg2,amsg3\n","  def choose_actions(self,state,messages,i):\n","    msg1,msg2,msg3 = self.concate(messages,i)\n","    actions = []\n","    act1    = self.agent1.choose_action(state[0],msg1)\n","    act2    = self.agent2.choose_action(state[1],msg2)\n","    act3    = self.agent3.choose_action(state[2],msg3)\n","    actions.append(act1)\n","    actions.append(act2)\n","    actions.append(act3)\n","    return actions\n","  def loss(self,reward,done,next_values,values):\n","    gae     = 0\n","    returns = []\n","    for i in range(1):\n","      delta = sum(reward) + (1-sum(done))*0.99*sum(next_values)- sum(values)\n","      gae   = delta + 0.95*gae*(1-sum(done))\n","      returns.insert(0,gae+sum(next_values))\n","    returns = torch.tensor([returns]).float().to(self.device)\n","    return returns\n","  def next_values(self,next_state,action,reward):\n","    val  = []\n","    for i in range(3):\n","      rewards  = torch.tensor([reward[i]]).float().to(self.device)\n","      obs = torch.tensor(next_state[i]).float().to(self.device)\n","      value = self.critic(obs,action[i],rewards)\n","      val.append(value)\n","    return val\n","  def update(self,state,next_state,rewards,messages,done,i):\n","    actions  = self.choose_actions(state,messages,i)\n","    values   = self.next_values(state,actions,rewards)\n","    next_act = self.choose_actions(next_state,messages,i)\n","    nvalues  = self.next_values(next_state,next_act,rewards)\n","    loss     = self.loss(rewards,done,nvalues,values).mean()\n","    loss.requires_grad = True\n","    msg1     = self.agent1.learn(state[0],next_state[0],rewards[0],done[0],nvalues[0],i,[messages[1],messages[2]])\n","    msg2     = self.agent2.learn(state[1],next_state[1],rewards[1],done[1],nvalues[1],i,[messages[2],messages[0]])\n","    msg3     = self.agent3.learn(state[2],next_state[2],rewards[2],done[2],nvalues[2],i,[messages[0],messages[1]])\n","    self.buffer.messages_list[0].append(msg1)\n","    self.buffer.messages_list[1].append(msg2)\n","    self.buffer.messages_list[2].append(msg3)\n","    print(len(self.buffer.messages_list[0]))\n","    self.optim.zero_grad()\n","    loss.backward()\n","    self.optim.step()\n","  def run(self,env,idx):\n","    for i in range(self.n_games):\n","      state = env.reset()\n","      score = [0,0,0]\n","      done  = [False*3]\n","      for step in range(self.steps):  \n","        if i==0 and step==0:\n","          messages = [torch.tensor([1]),torch.tensor([1]),torch.tensor([1])]\n","        else:\n","          print(self.buffer.messages_list[0])\n","          msg1 = self.buffer.messages_list[0][idx]\n","          msg2 = self.buffer.messages_list[1][idx]\n","          msg3 = self.buffer.messages_list[2][idx]\n","          messages=[msg1,msg2,msg3]\n","        actions = self.choose_actions(state,messages,idx)\n","        next_state,reward,done,infor = env.step(actions)\n","        if done:\n","          self.update(state,next_state,reward,messages,done,idx)\n","          state = next_state\n","          idx  = idx+1\n","        else:\n","          print(\"completed\")"],"metadata":{"id":"lvHFkV6V2I0F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ ==\"__main__\":\n","  buffer = Buffer()\n","  from make_env import make_env\n","  buffer.agent1_value.append(torch.tensor([1]))\n","  buffer.agent2_value.append(torch.tensor([1]))\n","  buffer.agent3_value.append(torch.tensor([1]))\n","  env = make_env(\"simple_adversary\")\n","  m = Main(21,5,5,buffer,10,10)\n","  cout = -1\n","  m.run(env,cout)"],"metadata":{"id":"BmP4iFXoCmfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pCi7iLHjDbwS"},"execution_count":null,"outputs":[]}]}